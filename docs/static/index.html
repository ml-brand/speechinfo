<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Speech Info — статическая версия (стр. 1/2)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-02-10T21%3A28%3A54Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-02-10T21%3A28%3A54Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-02-10T21%3A28%3A54Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">Speech Info</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+0VCG1pUgLahlZGNi" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link disabled" href="#">←</a>
        <a class="page-link current" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a>
        <a class="nav-link" href="page-2.html">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="118" data-search="fireredtts-2: towards long conversational speech generation for podcast and chatbot сегодня разберём статью, авторы которой пытаются решить задачу multi-speaker-генерации длинных диалогов, например для подкастов и чат-ботов. во-первых, в работе предлагают новый стриминговый speech tokenizer с частотой 12,5 hz (12,5 токена/сек), чтобы тянуть длинные последовательности. обычно используют токенизаторы с частотой около 25 hz, а здесь её снижают — как раз чтобы упростить работу с длинными диалогами. во-вторых, для моделирования multi-layer speech-токенов используют подход dual-transformer: большой decoder-only-трансформер предсказывает токены первого уровня, а маленький трансформер быстро достраивает остальные. архитектура в speech tokenizer объединяют два источника информации: акустику и семантику из whisper (его энкодер заморожен). их приводят к одному пространству и кодируют в rvq-токены, чтобы в каждом была и семантическая, и акустическая информация. поверх этого работает tts-модель: на вход подаются speaker + text + speech tokens. трансформер сначала предсказывает токен первого уровня, а затем маленький декодер достраивает остальные уровни. после этого полный набор rvq-токенов превращается обратно в финальный speech. как обучают speech tokenizer обучение проходит в две стадии. на претрейне используют reconstruction loss и дополнительные лоссы для rvq и семантики, а также perceptual loss через wavlm для сравнения фичей реконструкции и оригинала. на этапе посттрейна семантический декодер убирают, акустический заменяют на стриминговую версию (24 khz), и дообучают уже с reconstruction + gan loss на более чистых данных. по wer токенизатор показывает лучший результат среди моделей с таким низким frame rate, хотя по mos уступает некоторым решениям вроде xcodec2. сценарии использования 1) voice cloning модель может воспроизводить голос по промпту. разборчивость речи получается хорошей, но вот похожесть на оригинальный голос — хуже, чем у лидеров. авторы говорят, что voice cloning — не главный фокус работы. 2) диалоговый чат с эмоциями для этого сценария собрали 15 часов эмоциональной речи, записанных одной женщиной (6 эмоций), и дообучили модель так, чтобы она могла отвечать с нужной интонацией. эмоции затем проверяли вручную — точность получилась высокой. правда, остаётся вопрос, насколько хорошо такая модель умеет говорить нейтрально. 3) генерация подкастов авторы сделали набор английских и китайских подкастовых разговоров, которые показывают, что модель может генерировать диалоги длиной до трёх минут и поддерживать несколько говорящих. сравнивают по mos и другим метрикам, и отдельно делают side-by-side с реальными записями. говорят, что примерно в 28% случаев их результат можно перепутать с настоящими диалогами. максим борисов ❣ специально для speech info fireredtts-2: towards long conversational speech generation for podcast and chatbot сегодня разберём статью , авторы которой пытаются решить задачу multi-speaker-генерации длинных диалогов, например для подкастов и чат-ботов. во-первых, в работе предлагают новый стриминговый speech tokenizer с частотой 12,5 hz (12,5 токена/сек), чтобы тянуть длинные последовательности. обычно используют токенизаторы с частотой около 25 hz, а здесь её снижают — как раз чтобы упростить работу с длинными диалогами. во-вторых, для моделирования multi-layer speech-токенов используют подход dual-transformer: большой decoder-only-трансформер предсказывает токены первого уровня, а маленький трансформер быстро достраивает остальные. архитектура в speech tokenizer объединяют два источника информации: акустику и семантику из whisper (его энкодер заморожен). их приводят к одному пространству и кодируют в rvq-токены, чтобы в каждом была и семантическая, и акустическая информация. поверх этого работает tts-модель: на вход подаются speaker + text + speech tokens. трансформер сначала предсказывает токен первого уровня, а затем маленький декодер достраивает остальные уровни. после этого полный набор rvq-токенов превращается обратно в финальный speech. как обучают speech tokenizer обучение проходит в две стадии. на претрейне используют reconstruction loss и дополнительные лоссы для rvq и семантики, а также perceptual loss через wavlm для сравнения фичей реконструкции и оригинала. на этапе посттрейна семантический декодер убирают, акустический заменяют на стриминговую версию (24 khz), и дообучают уже с reconstruction + gan loss на более чистых данных. по wer токенизатор показывает лучший результат среди моделей с таким низким frame rate, хотя по mos уступает некоторым решениям вроде xcodec2. сценарии использования 1) voice cloning модель может воспроизводить голос по промпту. разборчивость речи получается хорошей, но вот похожесть на оригинальный голос — хуже, чем у лидеров. авторы говорят, что voice cloning — не главный фокус работы. 2) диалоговый чат с эмоциями для этого сценария собрали 15 часов эмоциональной речи, записанных одной женщиной (6 эмоций), и дообучили модель так, чтобы она могла отвечать с нужной интонацией. эмоции затем проверяли вручную — точность получилась высокой. правда, остаётся вопрос, насколько хорошо такая модель умеет говорить нейтрально. 3) генерация подкастов авторы сделали набор английских и китайских подкастовых разговоров, которые показывают, что модель может генерировать диалоги длиной до трёх минут и поддерживать несколько говорящих. сравнивают по mos и другим метрикам, и отдельно делают side-by-side с реальными записями. говорят, что примерно в 28% случаев их результат можно перепутать с настоящими диалогами. максим борисов ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-02-06T07:53:21+00:00" href="./posts/118.html">2026-02-06 07:53 UTC</a></div>
      </div>
      <div class="post-body"><strong>FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot </strong><br><br>Сегодня разберём <a href="https://arxiv.org/abs/2509.02020v2" rel="nofollow noopener noreferrer">статью</a>, авторы которой пытаются решить задачу multi-speaker-генерации длинных диалогов, например для подкастов и чат-ботов. <br><br>Во-первых, в работе предлагают новый стриминговый speech tokenizer с частотой 12,5 Hz (12,5 токена/сек), чтобы тянуть длинные последовательности. Обычно используют токенизаторы с частотой около 25 Hz, а здесь её снижают — как раз чтобы упростить работу с длинными диалогами.<br><br>Во-вторых, для моделирования multi-layer speech-токенов используют подход dual-transformer: большой decoder-only-трансформер предсказывает токены первого уровня, а маленький трансформер быстро достраивает остальные.<br><br><strong>Архитектура </strong><br><br>В speech tokenizer объединяют два источника информации: акустику и семантику из Whisper (его энкодер заморожен). Их приводят к одному пространству и кодируют в RVQ-токены, чтобы в каждом была и семантическая, и акустическая информация.<br><br>Поверх этого работает TTS-модель: на вход подаются speaker + text + speech tokens. Трансформер сначала предсказывает токен первого уровня, а затем маленький декодер достраивает остальные уровни. После этого полный набор RVQ-токенов превращается обратно в финальный speech.<br><br><strong>Как обучают speech tokenizer<br></strong><br>Обучение проходит в две стадии. На претрейне используют reconstruction loss и дополнительные лоссы для RVQ и семантики, а также perceptual loss через WavLM для сравнения фичей реконструкции и оригинала.<br><br>На этапе посттрейна семантический декодер убирают, акустический заменяют на  стриминговую версию (24 kHz), и дообучают уже с reconstruction + GAN loss на более чистых данных.<br><br>По WER токенизатор показывает лучший результат среди моделей с таким низким frame rate, хотя по MOS уступает некоторым решениям вроде XCodec2.<br><br><strong>Сценарии использования<br></strong><br>1) Voice cloning<br><br>Модель может воспроизводить голос по промпту. Разборчивость речи получается хорошей, но вот похожесть на оригинальный голос — хуже, чем у лидеров. Авторы говорят, что voice cloning — не главный фокус работы.<br><br>2) Диалоговый чат с эмоциями<br><br>Для этого сценария собрали 15 часов эмоциональной речи, записанных одной женщиной (6 эмоций), и дообучили модель так, чтобы она могла отвечать с нужной интонацией. Эмоции затем проверяли вручную — точность получилась высокой. Правда, остаётся вопрос, насколько хорошо такая модель умеет говорить нейтрально.<br><br>3) Генерация подкастов<br><br>Авторы сделали набор английских и китайских подкастовых разговоров, которые показывают, что модель может генерировать диалоги длиной до трёх минут и поддерживать несколько говорящих. Сравнивают по MOS и другим метрикам, и отдельно делают side-by-side с реальными записями. Говорят, что примерно в 28% случаев их результат можно перепутать с настоящими диалогами.<br><br><em>Максим Борисов</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/118_480.webp" srcset="../assets/media/thumbs/118_480.webp 480w, ../assets/media/118.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="118" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/119_480.webp" srcset="../assets/media/thumbs/119_480.webp 480w, ../assets/media/119.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="118" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/120_480.webp" srcset="../assets/media/thumbs/120_480.webp 480w, ../assets/media/120.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="118" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/121_480.webp" srcset="../assets/media/thumbs/121_480.webp 480w, ../assets/media/121.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="118" data-image-index="3" /></div></div>
      <div class="actions">
        <span>265 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/118" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/118.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="115" data-search="tts-1 technical report. 2/2 продолжаем рассказ о техрепотре свежего tts-движка американского стартапа inworld. sft: что сработало, а что нет после pretrain-стадии авторы переходят к sft и алайнменту. на sft используют около 200 тысяч часов транскрибированных данных. для фильтрации отбрасывают 20% худших сэмплов по dns-mos, 5% самых быстрых и 5% самых медленных по символам в секунду, плюс применяются текстовые эвристики для удаления плохих транскрипций. авторы утверждают, что для качества синтеза было важно инициализировать learning rate для sft финальным значением после стадии pretrain. попытка подмешать text-based instruction-following данные, чтобы лучше понимать сложные промпты, привела к ухудшению стабильности синтеза, несмотря на отсутствие деградации лосса на аудиоданных. ещё в работе есть аблейшн, который показал, что стартовать sft с speech-pretrained lm заметно лучше, чем с llama-3.2-1b-instruct — и лоссу, и по метрикам wer и sim. rl-алайнмент и разметка стилей для алайнмента используют rl с grpo, так как даже после sft остаются клики, артефакты и ошибки произношения. grpo позволяет оценивать несколько ответов на один и тот же запрос относительно среднего по группе, что даёт более стабильное обучение. используется композитный реворд, включающий wer, similarity и dns-mos, а также отдельные награды для аудиотегов. wer считают с помощью whisper-large-v3, similarity — через wavlm-large. утверждают, что единая модель с композитным ревордом работает лучше, чем модели, обученные под каждую метрику отдельно. в качестве аргумента приводят только графики grpo. отдельный блок отведён стилям и невербальным эффектам. попытка просто конкатенировать style-tag и текст не сработала — авторы объясняют это тем, что кодек смешивает семантическое и акустическое пространства, и стиль сложно изолировать от голосовых характеристик. решением стал парный датасет: нейтральные и стилизованные высказывания одного и того же спикера, склеенные паузой 0,5–1,5 секунды, с использованием тега как разделителя. на один нейтральный сэмпл приходится от одного до пяти стилизованных, около 20% данных содержат невербальные вокализации, а примерно 30% — непарные нейтральные примеры для сохранения базового синтеза. в оценке качества приходят к тому, что увеличение размера модели улучшает similarity и стабилизирует wer, а rl-алайнмент даёт прирост на коротких, средних и длинных сэмплах. что интересно, на внутренней tts-арене побеждают всех конкурентов, например, tts-1-max имеет win-rate 59,1% против 11labs. инференс модели имеют два режима: мгновенный voice cloning по референсу и транскрипту и профессиональный voice cloning с lora-дообучением speechlm. для стриминга сделана аккуратная склейка сегментов по участкам тишины, чтобы избежать щелчков, а также стабилизация громкости за счёт дополнительного контекста в аудиодекодере. inworld вместе с inference платформой modular ускорил api за счёт асинхронного планировщика, батчинга в декодере, sparse-формата для penalty sampling и кастомных gpu-ядер на mojo в составе max pipeline. это даёт первые две секунды синтезированного аудио в среднем на 70% быстрее, чем через vllm. какие есть проблемы в конце авторы честно говорят и об ограничениях. кэширование референса помогает снизить задержки, но может подтягивать стиль и эмоции из референсного аудио. длинные последовательности хуже генерируются при коротких промптах, а параметры декодинга постоянно приходится балансировать между сходством с голосом и выразительностью. в целом, получилась довольно инженерная работа о том, как стартап оптимизирует качество, задержки и стоимость — без архитектурных откровений, но с массой практических деталей. владимир гогорян ❣ специально для speech info tts-1 technical report. 2/2 продолжаем рассказ о техрепотре свежего tts-движка американского стартапа inworld. sft: что сработало, а что нет после pretrain-стадии авторы переходят к sft и алайнменту. на sft используют около 200 тысяч часов транскрибированных данных. для фильтрации отбрасывают 20% худших сэмплов по dns-mos, 5% самых быстрых и 5% самых медленных по символам в секунду, плюс применяются текстовые эвристики для удаления плохих транскрипций. авторы утверждают, что для качества синтеза было важно инициализировать learning rate для sft финальным значением после стадии pretrain. попытка подмешать text-based instruction-following данные, чтобы лучше понимать сложные промпты, привела к ухудшению стабильности синтеза, несмотря на отсутствие деградации лосса на аудиоданных. ещё в работе есть аблейшн, который показал, что стартовать sft с speech-pretrained lm заметно лучше, чем с llama-3.2-1b-instruct — и лоссу, и по метрикам wer и sim. rl-алайнмент и разметка стилей для алайнмента используют rl с grpo, так как даже после sft остаются клики, артефакты и ошибки произношения. grpo позволяет оценивать несколько ответов на один и тот же запрос относительно среднего по группе, что даёт более стабильное обучение. используется композитный реворд, включающий wer, similarity и dns-mos, а также отдельные награды для аудиотегов. wer считают с помощью whisper-large-v3, similarity — через wavlm-large. утверждают, что единая модель с композитным ревордом работает лучше, чем модели, обученные под каждую метрику отдельно. в качестве аргумента приводят только графики grpo. отдельный блок отведён стилям и невербальным эффектам. попытка просто конкатенировать style-tag и текст не сработала — авторы объясняют это тем, что кодек смешивает семантическое и акустическое пространства, и стиль сложно изолировать от голосовых характеристик. решением стал парный датасет: нейтральные и стилизованные высказывания одного и того же спикера, склеенные паузой 0,5–1,5 секунды, с использованием тега как разделителя. на один нейтральный сэмпл приходится от одного до пяти стилизованных, около 20% данных содержат невербальные вокализации, а примерно 30% — непарные нейтральные примеры для сохранения базового синтеза. в оценке качества приходят к тому, что увеличение размера модели улучшает similarity и стабилизирует wer, а rl-алайнмент даёт прирост на коротких, средних и длинных сэмплах. что интересно, на внутренней tts-арене побеждают всех конкурентов, например, tts-1-max имеет win-rate 59,1% против 11labs. инференс модели имеют два режима: мгновенный voice cloning по референсу и транскрипту и профессиональный voice cloning с lora-дообучением speechlm. для стриминга сделана аккуратная склейка сегментов по участкам тишины, чтобы избежать щелчков, а также стабилизация громкости за счёт дополнительного контекста в аудиодекодере. inworld вместе с inference платформой modular ускорил api за счёт асинхронного планировщика, батчинга в декодере, sparse-формата для penalty sampling и кастомных gpu-ядер на mojo в составе max pipeline. это даёт первые две секунды синтезированного аудио в среднем на 70% быстрее, чем через vllm. какие есть проблемы в конце авторы честно говорят и об ограничениях. кэширование референса помогает снизить задержки, но может подтягивать стиль и эмоции из референсного аудио. длинные последовательности хуже генерируются при коротких промптах, а параметры декодинга постоянно приходится балансировать между сходством с голосом и выразительностью. в целом, получилась довольно инженерная работа о том, как стартап оптимизирует качество, задержки и стоимость — без архитектурных откровений, но с массой практических деталей. владимир гогорян ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-01-28T10:02:21+00:00" href="./posts/115.html">2026-01-28 10:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>TTS-1 Technical Report. 2/2<br></strong><br>Продолжаем рассказ о <a href="https://arxiv.org/abs/2507.21138v1" rel="nofollow noopener noreferrer">техрепотре</a> свежего TTS-движка американского стартапа Inworld. <br><br><strong>SFT: что сработало, а что нет<br></strong><br>После pretrain-стадии авторы переходят к SFT и алайнменту. На SFT используют около 200 тысяч часов транскрибированных данных. Для фильтрации отбрасывают 20% худших сэмплов по DNS-MOS, 5% самых быстрых и 5% самых медленных по символам в секунду, плюс применяются текстовые эвристики для удаления плохих транскрипций.<br><br>Авторы утверждают, что для качества синтеза было важно инициализировать learning rate для SFT финальным значением после стадии pretrain. Попытка подмешать text-based instruction-following данные, чтобы лучше понимать сложные промпты, привела к ухудшению стабильности синтеза, несмотря на отсутствие деградации лосса на аудиоданных. Ещё в работе есть аблейшн, который показал, что стартовать SFT с speech-pretrained LM заметно лучше, чем с LLaMA-3.2-1B-Instruct — и лоссу, и по метрикам WER и SIM.<br><br><strong>RL-алайнмент и разметка стилей<br></strong><br>Для алайнмента используют RL с GRPO, так как даже после SFT остаются клики, артефакты и ошибки произношения. GRPO позволяет оценивать несколько ответов на один и тот же запрос относительно среднего по группе, что даёт более стабильное обучение. Используется композитный реворд, включающий WER, similarity и DNS-MOS, а также отдельные награды для аудиотегов. WER считают с помощью Whisper-large-v3, similarity — через WavLM-Large. Утверждают, что единая модель с композитным ревордом работает лучше, чем модели, обученные под каждую метрику отдельно. В качестве аргумента приводят только графики GRPO. <br><br>Отдельный блок отведён стилям и невербальным эффектам. Попытка просто конкатенировать style-tag и текст не сработала — авторы объясняют это тем, что кодек смешивает семантическое и акустическое пространства, и стиль сложно изолировать от голосовых характеристик. Решением стал парный датасет: нейтральные и стилизованные высказывания одного и того же спикера, склеенные паузой 0,5–1,5 секунды, с использованием тега как разделителя. На один нейтральный сэмпл приходится от одного до пяти стилизованных, около 20% данных содержат невербальные вокализации, а примерно 30% — непарные нейтральные примеры для сохранения базового синтеза.<br><br>В оценке качества приходят к тому, что увеличение размера модели улучшает similarity и стабилизирует WER, а RL-алайнмент даёт прирост на коротких, средних и длинных сэмплах. Что интересно, на внутренней TTS-арене побеждают всех конкурентов, например, TTS-1-Max имеет win-rate 59,1% против 11Labs.<br><br><strong>Инференс<br></strong><br>Модели имеют два режима: мгновенный voice cloning по референсу и транскрипту и профессиональный voice cloning с LoRA-дообучением SpeechLM. Для стриминга сделана аккуратная склейка сегментов по участкам тишины, чтобы избежать щелчков, а также стабилизация громкости за счёт дополнительного контекста в аудиодекодере. Inworld вместе с Inference платформой Modular ускорил API за счёт асинхронного планировщика, батчинга в декодере, sparse-формата для penalty sampling и кастомных GPU-ядер на Mojo в составе MAX pipeline. Это даёт первые две секунды синтезированного аудио в среднем на 70% быстрее, чем через vLLM.<br><br><strong>Какие есть проблемы<br></strong><br>В конце авторы честно говорят и об ограничениях. Кэширование референса помогает снизить задержки, но может подтягивать стиль и эмоции из референсного аудио. Длинные последовательности хуже генерируются при коротких промптах, а параметры декодинга постоянно приходится балансировать между сходством с голосом и выразительностью. В целом, получилась довольно инженерная работа о том, как стартап оптимизирует качество, задержки и стоимость — без архитектурных откровений, но с массой практических деталей.<br><br><em>Владимир Гогорян</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/115_480.webp" srcset="../assets/media/thumbs/115_480.webp 480w, ../assets/media/115.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="115" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/116_480.webp" srcset="../assets/media/thumbs/116_480.webp 480w, ../assets/media/116.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="115" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/117_480.webp" srcset="../assets/media/thumbs/117_480.webp 480w, ../assets/media/117.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="115" data-image-index="2" /></div></div>
      <div class="actions">
        <span>466 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/115" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/115.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="112" data-search="tts-1 technical report. 1/2 сегодня начинаем разбирать техрепорт tts-1 от стартапа inworld, представившего собственный движок синтеза и довольно подробный отчёт о нём. с архитектурной точки зрения решение можно описать как «yet another speechlm», но с большим количеством инженерии. есть аудиокодек, есть языковая модель, которая генерирует токены, и есть декодер, который восстанавливает аудио. но, как обычно, дьявол скрыт в деталях — и ими авторы довольно открыто делятся. что под капотом в качестве кодека используется x-codec 2 с super-resolution-декодером до 48 кгц. причины выбора простые: кодек опенсорсный, его удобно адаптировать под стриминг, он сильно экономит хранение и обработку данных. например, один час моноаудио 48 кгц в сыром виде занимает около 365 мб, тогда как токенизированное представление с кодбуком на 65 536 токенов — всего около 0,19 мб при хранении в uint16. для стартапа это большой плюс. кодек переобучали полностью на 110 тысячах часов собственных данных. помимо модифицированного декодера архитектура осталась стандартной: энкодер на базе wav2vec-bert с resnet-блоками, декодер vocos, квантизация fsq, multi-period и multi-stft дискриминаторы. из необычного — дополнительный rms-лосс, который ввели для борьбы с неконсистентной громкостью на склейках и в high-pitch-сегментах, что особенно проявлялось в стриминговом режиме. апсемплинг до 48 кгц повышение разрешения аудио из 16 кгц в 48 кгц сделано через двухэтапный uptraining: сначала на данных с native sample rate ≥32 кгц, затем дополнительный fine-tuning на аудио ≥44.1 кгц. за счёт подбора страйдов и hop-length в декодере такой апсемплинг почти не влияет на скорость и сложность обучения и, по словам авторов, даже даёт выигрыш по dns-mos. speechlm поверх кодека — speechlm. в tts-1 используется llama-3.2 на 1,6b параметров, а в версии tts-1 max — llama-3.1 на 8,8b. общий словарь объединяет текстовые токены, 65 тысяч аудиотокенов и специальные токены, включая теги эмоций и невербальных вокализаций. на этапе претрена к аудиоданным подмешивается около 20 миллиардов текстовых токенов из redpajama-v2 и instruction-данных laion oig, чтобы сохранить текстовое понимание. обучались модели с bfloat16, flash attention 2 и fused adamw. для распределенного обучения младшая версия использовала ddp, а старшая fsdp и torch.compile. один полный прогон претрена занял около двух дней для маленькой модели и около 10 дней для большой. во второй части поста расскажем про sft, rl и инженерию деплоя в tts-1. владимир гогорян ❣ специально для speech info tts-1 technical report. 1/2 сегодня начинаем разбирать техрепорт tts-1 от стартапа inworld, представившего собственный движок синтеза и довольно подробный отчёт о нём. с архитектурной точки зрения решение можно описать как «yet another speechlm», но с большим количеством инженерии. есть аудиокодек, есть языковая модель, которая генерирует токены, и есть декодер, который восстанавливает аудио. но, как обычно, дьявол скрыт в деталях — и ими авторы довольно открыто делятся. что под капотом в качестве кодека используется x-codec 2 с super-resolution-декодером до 48 кгц. причины выбора простые: кодек опенсорсный, его удобно адаптировать под стриминг, он сильно экономит хранение и обработку данных. например, один час моноаудио 48 кгц в сыром виде занимает около 365 мб, тогда как токенизированное представление с кодбуком на 65 536 токенов — всего около 0,19 мб при хранении в uint16. для стартапа это большой плюс. кодек переобучали полностью на 110 тысячах часов собственных данных. помимо модифицированного декодера архитектура осталась стандартной: энкодер на базе wav2vec-bert с resnet-блоками, декодер vocos, квантизация fsq, multi-period и multi-stft дискриминаторы. из необычного — дополнительный rms-лосс, который ввели для борьбы с неконсистентной громкостью на склейках и в high-pitch-сегментах, что особенно проявлялось в стриминговом режиме. апсемплинг до 48 кгц повышение разрешения аудио из 16 кгц в 48 кгц сделано через двухэтапный uptraining: сначала на данных с native sample rate ≥32 кгц, затем дополнительный fine-tuning на аудио ≥44.1 кгц. за счёт подбора страйдов и hop-length в декодере такой апсемплинг почти не влияет на скорость и сложность обучения и, по словам авторов, даже даёт выигрыш по dns-mos. speechlm поверх кодека — speechlm. в tts-1 используется llama-3.2 на 1,6b параметров, а в версии tts-1 max — llama-3.1 на 8,8b. общий словарь объединяет текстовые токены, 65 тысяч аудиотокенов и специальные токены, включая теги эмоций и невербальных вокализаций. на этапе претрена к аудиоданным подмешивается около 20 миллиардов текстовых токенов из redpajama-v2 и instruction-данных laion oig, чтобы сохранить текстовое понимание. обучались модели с bfloat16, flash attention 2 и fused adamw. для распределенного обучения младшая версия использовала ddp, а старшая fsdp и torch.compile. один полный прогон претрена занял около двух дней для маленькой модели и около 10 дней для большой. во второй части поста расскажем про sft, rl и инженерию деплоя в tts-1. владимир гогорян ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-01-22T09:39:42+00:00" href="./posts/112.html">2026-01-22 09:39 UTC</a></div>
      </div>
      <div class="post-body"><strong>TTS-1 Technical Report. 1/2<br></strong><br>Сегодня начинаем разбирать <a href="https://arxiv.org/abs/2507.21138v1" rel="nofollow noopener noreferrer">техрепорт</a> TTS-1 от стартапа Inworld, представившего собственный движок синтеза и довольно подробный отчёт о нём.<br><br>С архитектурной точки зрения решение можно описать как «yet another SpeechLM», но с большим количеством инженерии. Есть аудиокодек, есть языковая модель, которая генерирует токены, и есть декодер, который восстанавливает аудио. Но, как обычно, дьявол скрыт в деталях — и ими авторы довольно открыто делятся.<br><br><strong>Что под капотом<br></strong><br>В качестве кодека используется X-Codec 2 с super-resolution-декодером до 48 кГц. Причины выбора простые: кодек опенсорсный, его удобно адаптировать под стриминг, он сильно экономит хранение и обработку данных. Например, один час моноаудио 48 кГц в сыром виде занимает около 365 МБ, тогда как токенизированное представление с кодбуком на 65 536 токенов — всего около 0,19 МБ при хранении в uint16. Для стартапа это большой плюс.<br><br>Кодек переобучали полностью на 110 тысячах часов собственных данных. Помимо модифицированного декодера архитектура осталась стандартной: энкодер на базе Wav2Vec-BERT с ResNet-блоками, декодер Vocos, квантизация FSQ, Multi-Period и Multi-STFT дискриминаторы. Из необычного — дополнительный RMS-лосс, который ввели для борьбы с неконсистентной громкостью на склейках и в high-pitch-сегментах, что особенно проявлялось в стриминговом режиме.<br><br><strong>Апсемплинг до 48 кГц</strong><br><br>Повышение разрешения аудио из 16 кГц в 48 кГц сделано через двухэтапный uptraining: сначала на данных с native sample rate ≥32 кГц, затем дополнительный fine-tuning на аудио ≥44.1 кГц. За счёт подбора страйдов и hop-length в декодере такой апсемплинг почти не влияет на скорость и сложность обучения и, по словам авторов, даже даёт выигрыш по DNS-MOS.<br><br><strong>SpeechLM</strong><br><br>Поверх кодека — SpeechLM. В TTS-1 используется LLaMA-3.2 на 1,6B параметров, а в версии TTS-1 Max — LLaMA-3.1 на 8,8B. Общий словарь объединяет текстовые токены, 65 тысяч аудиотокенов и специальные токены, включая теги эмоций и невербальных вокализаций.<br><br>На этапе претрена к аудиоданным подмешивается около 20 миллиардов текстовых токенов из RedPajama-v2 и instruction-данных LAION OIG, чтобы сохранить текстовое понимание. Обучались модели с bfloat16, flash attention 2 и fused AdamW. Для распределенного обучения младшая версия использовала DDP, а старшая FSDP и torch.compile. Один полный прогон претрена занял около двух дней для маленькой модели и около 10 дней для большой.<br><br>Во второй части поста расскажем про SFT, RL и инженерию деплоя в TTS-1.<br><br><em>Владимир Гогорян</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/112_480.webp" srcset="../assets/media/thumbs/112_480.webp 480w, ../assets/media/112.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="112" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/113_480.webp" srcset="../assets/media/thumbs/113_480.webp 480w, ../assets/media/113.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="112" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/114_480.webp" srcset="../assets/media/thumbs/114_480.webp 480w, ../assets/media/114.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="112" data-image-index="2" /></div></div>
      <div class="actions">
        <span>670 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/112" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/112.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="111" data-search="лучшие статьи 2025 года: выбор авторов speech info. часть 2 настраиваемся на конец рабочей недели и вспоминаем ещё несколько полезных статей прошедшего года. выбрали и прокомментировали их авторы нашего канала. cosyvoice 3: towards in-the-wild speech generation via scaling-up and post-training в работе представлена новая версия модели cosyvoice для zero-shot-синтеза речи. ключевые улучшения: 1) новый речевой токенизатор — использует fsq (25 ток./с) и обучается на основе lm minmo с помощью многозадачного обучения (asr, ser, aed, lid, sid); 2) дифференцируемая оптимизация награды (diffro) — новый подход для дообучения моделей синтеза речи на основе llm, который позволяет напрямую оптимизировать речевые токены; 3) масштабирование данных (до 1 млн часов, 9 языков, 18 китайских диалектов) и модели (с 0,5b до 1,5b параметров). cosyvoice 3 показывает существенное улучшение по сравнению с предыдущей версией, а также покрывает больше языков. недавно авторы выложили в открытый доступ модель cosyvoice3-0.5b. indextts2: a breakthrough in emotionally expressive and duration-controlled auto-regressive zero-shot text-to-speech indextts2 — авторегрессионная zero-shot tts-модель, которая решает две ключевые задачи: контроль длительности и разделение управления между идентичностью спикера и эмоцией. длительность можно задавать явно, подавая в lm число токенов, которые нужно сгенерировать. а использование grl при обучении для отделения эмоциональных признаков от идентичности спикера позволяет применять два промпта: один для стиля, второй — для тембра. также предложен способ управления эмоцией по текстовому промпту: знания дистиллируют из deepseek-r1, чтобы по тексту предсказывать распределение по семи базовым эмоциям в меньшую lm-модель. на инференсе эмбеддинг эмоции вычисляется как взвешенная сумма фиксированных эмбеддингов, полученных из аудиопримеров для каждой базовой эмоции. adaptive knowledge distillation for device-directed speech detection в apple предлагают детектить обращение к устройству без триггерной фразы — по одной интонации, но с ограничениями (например, режим включается только вскоре после взаимодействия с девайсом), чтобы не ловить лишние срабатывания. обучают небольшой on-device-энкодер сразу на три задачи: hey siri, siri и интонационную активацию, а качество подтягивают через дистилляцию из замороженного asr-энкодера на всех уровнях модели. вывод простой: такая дистилляция заметно улучшает качество, а общий энкодер на несколько триггеров помогает всем задачам. по словам авторов, в телефонах это уже работает, а на колонках пока сложнее из-за данных и краевых случаев. статьи отобрали ❣ дмитрий попов, борис шелудько speech info лучшие статьи 2025 года: выбор авторов speech info. часть 2 настраиваемся на конец рабочей недели и вспоминаем ещё несколько полезных статей прошедшего года. выбрали и прокомментировали их авторы нашего канала. cosyvoice 3: towards in-the-wild speech generation via scaling-up and post-training в работе представлена новая версия модели cosyvoice для zero-shot-синтеза речи. ключевые улучшения: 1) новый речевой токенизатор — использует fsq (25 ток./с) и обучается на основе lm minmo с помощью многозадачного обучения (asr, ser, aed, lid, sid); 2) дифференцируемая оптимизация награды (diffro) — новый подход для дообучения моделей синтеза речи на основе llm, который позволяет напрямую оптимизировать речевые токены; 3) масштабирование данных (до 1 млн часов, 9 языков, 18 китайских диалектов) и модели (с 0,5b до 1,5b параметров). cosyvoice 3 показывает существенное улучшение по сравнению с предыдущей версией, а также покрывает больше языков. недавно авторы выложили в открытый доступ модель cosyvoice3-0.5b . indextts2: a breakthrough in emotionally expressive and duration-controlled auto-regressive zero-shot text-to-speech indextts2 — авторегрессионная zero-shot tts-модель, которая решает две ключевые задачи: контроль длительности и разделение управления между идентичностью спикера и эмоцией. длительность можно задавать явно, подавая в lm число токенов, которые нужно сгенерировать. а использование grl при обучении для отделения эмоциональных признаков от идентичности спикера позволяет применять два промпта: один для стиля, второй — для тембра. также предложен способ управления эмоцией по текстовому промпту: знания дистиллируют из deepseek-r1, чтобы по тексту предсказывать распределение по семи базовым эмоциям в меньшую lm-модель. на инференсе эмбеддинг эмоции вычисляется как взвешенная сумма фиксированных эмбеддингов, полученных из аудиопримеров для каждой базовой эмоции. adaptive knowledge distillation for device-directed speech detection в apple предлагают детектить обращение к устройству без триггерной фразы — по одной интонации, но с ограничениями (например, режим включается только вскоре после взаимодействия с девайсом), чтобы не ловить лишние срабатывания. обучают небольшой on-device-энкодер сразу на три задачи: hey siri, siri и интонационную активацию, а качество подтягивают через дистилляцию из замороженного asr-энкодера на всех уровнях модели. вывод простой: такая дистилляция заметно улучшает качество, а общий энкодер на несколько триггеров помогает всем задачам. по словам авторов, в телефонах это уже работает, а на колонках пока сложнее из-за данных и краевых случаев. статьи отобрали ❣ дмитрий попов, борис шелудько speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-01-15T10:43:01+00:00" href="./posts/111.html">2026-01-15 10:43 UTC</a></div>
      </div>
      <div class="post-body"><strong>Лучшие статьи 2025 года: выбор авторов Speech Info. Часть 2</strong><br><br>Настраиваемся на конец рабочей недели и вспоминаем ещё несколько полезных статей прошедшего года. Выбрали и прокомментировали их авторы нашего канала. <br><br><a href="https://arxiv.org/abs/2505.17589v2" rel="nofollow noopener noreferrer"><strong>CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training</strong></a><br><br>В работе представлена новая версия модели CosyVoice для zero-shot-синтеза речи. Ключевые улучшения:<br>1) новый речевой токенизатор — использует FSQ (25 ток./с) и обучается на основе LM MinMo с помощью многозадачного обучения (ASR, SER, AED, LID, SID);<br>2) дифференцируемая оптимизация награды (DiffRO) — новый подход для дообучения моделей синтеза речи на основе LLM, который позволяет напрямую оптимизировать речевые токены;<br>3) масштабирование данных (до 1 млн часов, 9 языков, 18 китайских диалектов) и модели (с 0,5B до 1,5B параметров).<br>CosyVoice 3 показывает существенное улучшение по сравнению с предыдущей версией, а также покрывает больше языков. Недавно авторы выложили в открытый доступ модель <a href="https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512" rel="nofollow noopener noreferrer">CosyVoice3-0.5B</a>.<br><br><a href="https://arxiv.org/abs/2506.21619v2" rel="nofollow noopener noreferrer"><strong>IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</strong></a><br><br>IndexTTS2 — авторегрессионная zero-shot TTS-модель, которая решает две ключевые задачи: контроль длительности и разделение управления между идентичностью спикера и эмоцией. Длительность можно задавать явно, подавая в LM число токенов, которые нужно сгенерировать. А использование GRL при обучении для отделения эмоциональных признаков от идентичности спикера позволяет применять два промпта: один для стиля, второй — для тембра. Также предложен способ управления эмоцией по текстовому промпту: знания дистиллируют из DeepSeek-R1, чтобы по тексту предсказывать распределение по семи базовым эмоциям в меньшую LM-модель. На инференсе эмбеддинг эмоции вычисляется как взвешенная сумма фиксированных эмбеддингов, полученных из аудиопримеров для каждой базовой эмоции.<br><br><a href="https://arxiv.org/abs/2508.02801v1" rel="nofollow noopener noreferrer"><strong>Adaptive Knowledge Distillation for Device-Directed Speech Detection<br></strong></a><br>В Apple предлагают детектить обращение к устройству без триггерной фразы — по одной интонации, но с ограничениями (например, режим включается только вскоре после взаимодействия с девайсом), чтобы не ловить лишние срабатывания. Обучают небольшой on-device-энкодер сразу на три задачи: Hey Siri, Siri и интонационную активацию, а качество подтягивают через дистилляцию из замороженного ASR-энкодера на всех уровнях модели. Вывод простой: такая дистилляция заметно улучшает качество, а общий энкодер на несколько триггеров помогает всем задачам. По словам авторов, в телефонах это уже работает, а на колонках пока сложнее из-за данных и краевых случаев.<br><br><em>Статьи отобрали </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Дмитрий Попов, Борис Шелудько</em><br><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a></div>
      <div class="actions">
        <span>768 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/111" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/111.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="110" data-search="лучшие статьи 2025 года: выбор авторов speech info. часть 1 за прошедший год накопилось много интересных работ на тему голосовых технологий. статьями, которые стоит перечитать и сохранить, поделились эксперты нашего канала. продолжать список можно бесконечно в комментариях. beyond transcription: mechanistic interpretability in asr статья переносит mechinterp-инструментарий из nlp в asr, делая это системно и на крупных моделях (whisper-large-v3 и qwen2-audio). авторы адаптируют logit lens, линейные пробы и activation patching под asr и вводят новый метод encoder lens для «развертывания» промежуточных представлений энкодера в текст. благодаря этому получается показать ряд не описанных ранее явлений (наличие неявной информации в энкодере; возможность предсказывать галлюцинации по residual декодера; механизм повторения токенов в self-attention). delayed fusion: integrating large language models into first-pass decoding in end-to-end speech recognition авторы предлагают метод delayed fusion для интеграции llm в первый проход декодирования asr, который принципиально отличается от классических shallow fusion и n-best rescoring тем, что: 1) применяет llm‑оценки к гипотезам с задержкой и после прунинга в ходе поиска, тем самым резко сокращая число оцениваемых гипотез и llm-вызовов при сохранении влияния llm уже на первом проходе; 2) позволяет на лету ретокенизировать гипотезы по словесным границам и тем самым без переобучения использовать llm с другой лексикой/токенизацией, тогда как стандартный shallow fusion требует совпадения словаря asr и lm или дорогостоящего дообучения; 3) вводит настраиваемый механизм управления моментом вызова llm (стратегии shortest-hypothesis fusion и fixed-interval fusion). music flamingo: scaling music understanding in audio language models статья о новой alm, специально заточенной под глубокое понимание музыки, включая вокальные композиции. основные вклады авторов — создание масштабных датасетов mf-skills и mf-think с многоуровневыми описаниями и цепочками рассуждений, основанными на теории музыки, а также предложение поэтапного подхода к обучению, сочетающего дообучение на мультиязычных asr-данных, тонкую настройку на музыкальных задачах и rl-стадию с grpo. streaming sortformer: speaker cache-based online speaker diarization with arrival-time ordering работа о стриминговой диаризации: модель в реальном времени получает аудио и сразу выдаёт вероятности по спикерам, без классического каскада «сегментация → эмбеддинги → кластеризация». ключевая идея — держать кэш эмбеддингов уже встреченных спикеров и подавать его вместе с текущим аудиофрагментом, постоянно обновляя (спикеры в кэше упорядочены по времени появления). ограничение простое: число спикеров фиксировано архитектурно — модель нельзя безболезненно перенести на сильно большее количество, чем было на обучении. зато на нескольких датасетах она обгоняет бейзлайны и прошлую офлайн-версию, оставаясь пригодной для реалтайм-сценариев. продолжение следует. статьи отобрали ❣ екатерина козлова, борис шелудько speech info лучшие статьи 2025 года: выбор авторов speech info. часть 1 за прошедший год накопилось много интересных работ на тему голосовых технологий. статьями, которые стоит перечитать и сохранить, поделились эксперты нашего канала. продолжать список можно бесконечно в комментариях. beyond transcription: mechanistic interpretability in asr статья переносит mechinterp-инструментарий из nlp в asr, делая это системно и на крупных моделях (whisper-large-v3 и qwen2-audio). авторы адаптируют logit lens, линейные пробы и activation patching под asr и вводят новый метод encoder lens для «развертывания» промежуточных представлений энкодера в текст. благодаря этому получается показать ряд не описанных ранее явлений (наличие неявной информации в энкодере; возможность предсказывать галлюцинации по residual декодера; механизм повторения токенов в self-attention). delayed fusion: integrating large language models into first-pass decoding in end-to-end speech recognition авторы предлагают метод delayed fusion для интеграции llm в первый проход декодирования asr, который принципиально отличается от классических shallow fusion и n-best rescoring тем, что: 1) применяет llm‑оценки к гипотезам с задержкой и после прунинга в ходе поиска, тем самым резко сокращая число оцениваемых гипотез и llm-вызовов при сохранении влияния llm уже на первом проходе; 2) позволяет на лету ретокенизировать гипотезы по словесным границам и тем самым без переобучения использовать llm с другой лексикой/токенизацией, тогда как стандартный shallow fusion требует совпадения словаря asr и lm или дорогостоящего дообучения; 3) вводит настраиваемый механизм управления моментом вызова llm (стратегии shortest-hypothesis fusion и fixed-interval fusion). music flamingo: scaling music understanding in audio language models статья о новой alm, специально заточенной под глубокое понимание музыки, включая вокальные композиции. основные вклады авторов — создание масштабных датасетов mf-skills и mf-think с многоуровневыми описаниями и цепочками рассуждений, основанными на теории музыки, а также предложение поэтапного подхода к обучению, сочетающего дообучение на мультиязычных asr-данных, тонкую настройку на музыкальных задачах и rl-стадию с grpo. streaming sortformer: speaker cache-based online speaker diarization with arrival-time ordering работа о стриминговой диаризации: модель в реальном времени получает аудио и сразу выдаёт вероятности по спикерам, без классического каскада «сегментация → эмбеддинги → кластеризация». ключевая идея — держать кэш эмбеддингов уже встреченных спикеров и подавать его вместе с текущим аудиофрагментом, постоянно обновляя (спикеры в кэше упорядочены по времени появления). ограничение простое: число спикеров фиксировано архитектурно — модель нельзя безболезненно перенести на сильно большее количество, чем было на обучении. зато на нескольких датасетах она обгоняет бейзлайны и прошлую офлайн-версию, оставаясь пригодной для реалтайм-сценариев. продолжение следует. статьи отобрали ❣ екатерина козлова, борис шелудько speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2026-01-13T10:39:01+00:00" href="./posts/110.html">2026-01-13 10:39 UTC</a></div>
      </div>
      <div class="post-body"><strong>Лучшие статьи 2025 года: выбор авторов Speech Info. Часть 1<br></strong><br>За прошедший год накопилось много интересных работ на тему голосовых технологий. Статьями, которые стоит перечитать и сохранить, поделились эксперты нашего канала. Продолжать список можно <del>бесконечно</del> в комментариях.<br><br><a href="https://arxiv.org/abs/2508.15882v1" rel="nofollow noopener noreferrer"><strong>Beyond Transcription: Mechanistic Interpretability in ASR<br></strong></a><br>Статья переносит mechinterp-инструментарий из NLP в ASR, делая это системно и на крупных моделях (Whisper-large-v3 и Qwen2-Audio). Авторы адаптируют logit lens, линейные пробы и activation patching под ASR и вводят новый метод Encoder Lens для «развертывания» промежуточных представлений энкодера в текст. Благодаря этому получается показать ряд не описанных ранее явлений (наличие неявной информации в энкодере; возможность предсказывать галлюцинации по residual декодера; механизм повторения токенов в self-attention).<br><br><a href="https://arxiv.org/abs/2501.09258" rel="nofollow noopener noreferrer"><strong>Delayed Fusion: Integrating Large Language Models into First-Pass Decoding in End-to-end Speech Recognition<br></strong></a><br>Авторы предлагают метод delayed fusion для интеграции LLM в первый проход декодирования ASR, который принципиально отличается от классических shallow fusion и N-best rescoring тем, что:<br>1) применяет LLM‑оценки к гипотезам с задержкой и после прунинга в ходе поиска, тем самым резко сокращая число оцениваемых гипотез и LLM-вызовов при сохранении влияния LLM уже на первом проходе;<br>2) позволяет на лету ретокенизировать гипотезы по словесным границам и тем самым без переобучения использовать LLM с другой лексикой/токенизацией, тогда как стандартный shallow fusion требует совпадения словаря ASR и LM или дорогостоящего дообучения;<br>3) вводит настраиваемый механизм управления моментом вызова LLM (стратегии shortest-hypothesis fusion и fixed-interval fusion).<br><br><a href="https://arxiv.org/abs/2511.10289v1" rel="nofollow noopener noreferrer"><strong>Music Flamingo: Scaling Music Understanding in Audio Language Models</strong></a><br><br>Статья о новой ALM, специально заточенной под глубокое понимание музыки, включая вокальные композиции. Основные вклады авторов — создание масштабных датасетов MF-Skills и MF-Think с многоуровневыми описаниями и цепочками рассуждений, основанными на теории музыки, а также предложение поэтапного подхода к обучению, сочетающего дообучение на мультиязычных ASR-данных, тонкую настройку на музыкальных задачах и RL-стадию с GRPO.<br><br><a href="https://arxiv.org/abs/2507.18446" rel="nofollow noopener noreferrer"><strong>Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization with Arrival-Time Ordering<br></strong></a><br>Работа о стриминговой диаризации: модель в реальном времени получает аудио и сразу выдаёт вероятности по спикерам, без классического каскада «сегментация → эмбеддинги → кластеризация». Ключевая идея — держать кэш эмбеддингов уже встреченных спикеров и подавать его вместе с текущим аудиофрагментом, постоянно обновляя (спикеры в кэше упорядочены по времени появления). Ограничение простое: число спикеров фиксировано архитектурно — модель нельзя безболезненно перенести на сильно большее количество, чем было на обучении. Зато на нескольких датасетах она обгоняет бейзлайны и прошлую офлайн-версию, оставаясь пригодной для реалтайм-сценариев.<br><br>Продолжение следует.<br><br><em>Статьи отобрали </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Екатерина Козлова, Борис Шелудько<br></em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a></div>
      <div class="actions">
        <span>663 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/110" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/110.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="109" data-search="🎉итоги года: посты, которые были на слуху хотя speech info нет ещё и года, некоторые итоги у нас уже имеются. например, мы успели написать вместе с экспертами сотню с лишним разборов, осветить несколько крупных конференций (включая interspeech и icassp) и начать собирать сообщество людей, которым интересна тема голосовых технологий. в предновогодней публикации хотим вспомнить посты, которые больше всего читали в 2025-м. если какой-то из них запомнился вам или, по вашему мнению, в топе чего-то не хватает, приходите делиться в комментарии! билингвальный asr — уже в станциях и чате с алисой важный релиз прошедшего года. евгений ганкович рассказал, с какими сложностями столкнулись инженеры группы asr, пока делали алису двуязычной. ещё он поделился тем, как команде удалось не только не просадить, но и улучшить распознавание русского. все подводные камни процесса — в нашем разборе. архитектура kws от яндекса: как колонка с алисой выбирает, куда слушать рассказ о статье multichannel keyword spotting for noisy conditions, которую наши исследователи представили на конференции interspeech 2025 в роттердаме. разбираемся, как устроена архитектура kws, объединяющая мультиканальный вход и attention-механизм для более точного распознавания голосовых команд в шумных помещениях. как яндекс браузер переводит видео с сохранением оригинальных голосов в посте рассказали детали обновлённой версии перевода видео в яндекс браузере. в частности, разобрались за счёт чего технология умеет сохранять тембр и интонации оригинального голоса, а сам перевод стал точнее. приглашаем освежить в памяти. обзор статей с icassp 25. часть 1: шумоподавление в наушниках в апреле в индии прошла конференция icassp 2025, на которой побывал руководитель группы встроенного голосового ввода алексей рак. интересного хватило на серию постов, самым востребованным из которых стал этот — о двух работах на тему шумоподавлении в наушниках. wavchat: a survey of spoken dialogue models. часть 1/4 никита рыжиков превзошёл толкина написал четырёхчастный пост по следам масштабного обзора разговорных ии. первая часть квадрологии оказалась самой популярной. как заметил эксперт, несмотря на некоторые самоповторы, эта статья — пока лучшая попытка систематизировать происходящее в мире alm. так что приглашаем к чтению. остальные части: вторая, третья и четвёртая. mamba-модели в задачах speech enhancement екатерина кузина разобрала архитектуру mamba в контексте speech enhancement. в посте описан пайплайн модели для таких задач, а также есть наглядное сравнение mamba-блоков с transformer- и conformer-блоками. если пропустили пост, зовём наверстывать упущенное. как tortoisetts изменил правила игры в синтезе речи роман кайль рассказал историю появления tortoisetts и то, почему он стал важной вехой для современных tts-моделей. в посте разбираемся, как комбинация трансформера и диффузии позволила одновременно моделировать интонацию и голос. а ещё — как инженер-одиночка смог собрать рабочую схему на восьми gpu и почему этот подход подхватили большие команды. желаем отличных праздников и чтобы интересного чтения хватило на все 12 предстоящих месяцев! speech info 🎉 итоги года: посты, которые были на слуху хотя speech info нет ещё и года, некоторые итоги у нас уже имеются. например, мы успели написать вместе с экспертами сотню с лишним разборов, осветить несколько крупных конференций (включая interspeech и icassp) и начать собирать сообщество людей, которым интересна тема голосовых технологий. в предновогодней публикации хотим вспомнить посты, которые больше всего читали в 2025-м. если какой-то из них запомнился вам или, по вашему мнению, в топе чего-то не хватает, приходите делиться в комментарии! билингвальный asr — уже в станциях и чате с алисой важный релиз прошедшего года. евгений ганкович рассказал, с какими сложностями столкнулись инженеры группы asr, пока делали алису двуязычной. ещё он поделился тем, как команде удалось не только не просадить, но и улучшить распознавание русского. все подводные камни процесса — в нашем разборе. архитектура kws от яндекса: как колонка с алисой выбирает, куда слушать рассказ о статье multichannel keyword spotting for noisy conditions , которую наши исследователи представили на конференции interspeech 2025 в роттердаме. разбираемся, как устроена архитектура kws, объединяющая мультиканальный вход и attention-механизм для более точного распознавания голосовых команд в шумных помещениях. как яндекс браузер переводит видео с сохранением оригинальных голосов в посте рассказали детали обновлённой версии перевода видео в яндекс браузере. в частности, разобрались за счёт чего технология умеет сохранять тембр и интонации оригинального голоса, а сам перевод стал точнее. приглашаем освежить в памяти. обзор статей с icassp 25. часть 1: шумоподавление в наушниках в апреле в индии прошла конференция icassp 2025, на которой побывал руководитель группы встроенного голосового ввода алексей рак. интересного хватило на серию постов, самым востребованным из которых стал этот — о двух работах на тему шумоподавлении в наушниках. wavchat: a survey of spoken dialogue models. часть 1/4 никита рыжиков превзошёл толкина написал четырёхчастный пост по следам масштабного обзора разговорных ии. первая часть квадрологии оказалась самой популярной. как заметил эксперт, несмотря на некоторые самоповторы, эта статья — пока лучшая попытка систематизировать происходящее в мире alm. так что приглашаем к чтению. остальные части: вторая , третья и четвёртая . mamba-модели в задачах speech enhancement екатерина кузина разобрала архитектуру mamba в контексте speech enhancement. в посте описан пайплайн модели для таких задач, а также есть наглядное сравнение mamba-блоков с transformer- и conformer-блоками. если пропустили пост, зовём наверстывать упущенное. как tortoisetts изменил правила игры в синтезе речи роман кайль рассказал историю появления tortoisetts и то, почему он стал важной вехой для современных tts-моделей. в посте разбираемся, как комбинация трансформера и диффузии позволила одновременно моделировать интонацию и голос. а ещё — как инженер-одиночка смог собрать рабочую схему на восьми gpu и почему этот подход подхватили большие команды. желаем отличных праздников и чтобы интересного чтения хватило на все 12 предстоящих месяцев! speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-30T09:33:44+00:00" href="./posts/109.html">2025-12-30 09:33 UTC</a></div>
      </div>
      <div class="post-body"><strong><tg-emoji emoji-id="5341780233399848004">🎉</tg-emoji></strong><strong>Итоги года: посты, которые были на слуху</strong><br><br>Хотя Speech Info нет ещё и года, некоторые итоги у нас уже имеются. Например, мы успели написать вместе с экспертами сотню с лишним разборов, осветить несколько крупных конференций (включая Interspeech и ICASSP) и начать собирать сообщество людей, которым интересна тема голосовых технологий. <br><br>В предновогодней публикации хотим вспомнить посты, которые больше всего читали в 2025-м. Если какой-то из них запомнился вам или, по вашему мнению, в топе чего-то не хватает, приходите делиться в комментарии!<br><br><a href="https://t.me/speechinfo/9" rel="nofollow noopener noreferrer"><strong>Билингвальный ASR — уже в станциях и чате с Алисой</strong></a><br><br>Важный релиз прошедшего года. Евгений Ганкович рассказал, с какими сложностями столкнулись инженеры группы ASR, пока делали Алису двуязычной. Ещё он поделился тем, как команде удалось не только не просадить, но и улучшить распознавание русского. Все подводные камни процесса — в нашем разборе. <br><br><a href="https://t.me/speechinfo/64" rel="nofollow noopener noreferrer"><strong>Архитектура KWS от Яндекса: как колонка с Алисой выбирает, куда слушать</strong></a><br><br>Рассказ о статье <a href="https://arxiv.org/abs/2507.15558" rel="nofollow noopener noreferrer">Multichannel Keyword Spotting for Noisy Conditions</a>, которую наши исследователи представили на конференции Interspeech 2025 в Роттердаме. Разбираемся, как устроена архитектура KWS, объединяющая мультиканальный вход и attention-механизм для более точного распознавания голосовых команд в шумных помещениях.<br><br><a href="https://t.me/speechinfo/26" rel="nofollow noopener noreferrer"><strong>Как Яндекс Браузер переводит видео с сохранением оригинальных голосов</strong></a><br><br>В посте рассказали детали обновлённой версии перевода видео в Яндекс Браузере. В частности, разобрались за счёт чего технология умеет сохранять тембр и интонации оригинального голоса, а сам перевод стал точнее. Приглашаем освежить в памяти. <br><br><a href="https://t.me/speechinfo/38" rel="nofollow noopener noreferrer"><strong>Обзор статей с ICASSP 25. Часть 1: шумоподавление в наушниках</strong></a><br><br>В апреле в Индии прошла конференция ICASSP 2025, на которой побывал руководитель группы встроенного голосового ввода Алексей Рак. Интересного хватило на серию постов, самым востребованным из которых стал этот — о двух работах на тему шумоподавлении в наушниках.<br><br><a href="https://t.me/speechinfo/10" rel="nofollow noopener noreferrer"><strong>WavChat: A Survey of Spoken Dialogue Models. Часть 1/4 </strong></a><br><br>Никита Рыжиков <del>превзошёл Толкина</del> написал четырёхчастный пост по следам масштабного обзора разговорных ИИ. Первая часть квадрологии оказалась самой популярной. Как заметил эксперт, несмотря на некоторые самоповторы, эта статья — пока лучшая попытка систематизировать происходящее в мире ALM. Так что приглашаем к чтению. Остальные части: <a href="https://t.me/speechinfo/11" rel="nofollow noopener noreferrer">вторая</a>, <a href="https://t.me/speechinfo/33" rel="nofollow noopener noreferrer">третья</a> и <a href="https://t.me/speechinfo/34" rel="nofollow noopener noreferrer">четвёртая</a>. <br><br><a href="https://t.me/speechinfo/6" rel="nofollow noopener noreferrer"><strong>Mamba-модели в задачах Speech Enhancement</strong></a><br><br>Екатерина Кузина разобрала архитектуру Mamba в контексте Speech Enhancement. В посте описан пайплайн модели для таких задач, а также есть наглядное сравнение Mamba-блоков с transformer- и conformer-блоками. Если пропустили пост, зовём наверстывать упущенное.<br><br><a href="https://t.me/speechinfo/50" rel="nofollow noopener noreferrer"><strong>Как TortoiseTTS изменил правила игры в синтезе речи</strong></a><br><br>Роман Кайль рассказал историю появления TortoiseTTS и то, почему он стал важной вехой для современных TTS-моделей. В посте разбираемся, как комбинация трансформера и диффузии позволила одновременно моделировать интонацию и голос. А ещё — как инженер-одиночка смог собрать рабочую схему на восьми GPU и почему этот подход подхватили большие команды. <br><br>Желаем отличных праздников и чтобы интересного чтения хватило на все 12 предстоящих месяцев!<br><br><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a></div>
      <div class="actions">
        <span>730 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/109" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/109.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="106" data-search="три идеи на тему обучения speech-моделей сегодня делимся подборкой трёх концептуально интересных работ про обучение speech-моделей. первая — о контроле генерации на этапе декодирования, две остальные — о том, как аккуратнее стыковать речь и текст и обучать мультимодальные системы. length aware speech translation for video dubbing авторы решают понятную боль: как управлять длиной выходной последовательности (перевода), а не полагаться на эвристики поверх beam search (например, штрафы/нормализации за длину). нюанс таких эвристик в том, что они часто смещают ранжирование в сторону более коротких или более длинных гипотез. в статье предлагают разбить генерацию на несколько режимов длины: short, normal, long. вместо стандартного стартового токена (bos/sos) декодирование начинается со специального length-тега, и при обучении модель видит такие же теги — в итоге можно явно попросить «короткий» или «длинный» перевод. отдельно авторы модифицируют beam search: обычно на шаге прунинга оставляют top-k гипотез по скору. а тут при каждом прунинге стараются сохранять минимум по одной гипотезе каждого типа. это важно для случаев, когда «длинная» ветка обычно не доживает до конца: модель быстро завершает декодирование на коротких вариантах, а потом может выясниться, что более длинный — был бы лучше. очевидный минус подхода: поддержка длинных гипотез — это дополнительные затраты по производительности, потому что генерация идёт дольше. но сама идея «контролируем длину явно и держим разные длины в beam search» выглядит практичной. scheduled interleaved speech-text training for speech-to-speech translation with llms предположим, у нас есть текстовая llm, и мы хотим научить систему работать и со звуком. лобовой вариант — сразу добавить аудио в обучение и перейти в speech-режим. но такой переход получается слишком резким: до этого модель обучалась только на тексте, а теперь получает аудиопредставления, и на этом стыке всё легко может развалиться. чтобы этого избежать, текст обычно не убирают сразу, а продолжают подавать его вместе с аудио, постепенно меняя пропорции: сначала почти один текст и немного аудио, потом аудио становится больше, текста меньше — и так далее, вплоть до режима «почти только аудио». здесь авторы пошли ещё дальше и делают это не на уровне целых примеров, а внутри одного сэмпла: часть токенов — текстовые, часть — аудио. за счёт этого переход получается ещё мягче: сначала в сэмпле почти один текст и немного аудио, потом аудио всё больше. в конце для таких смешанных примеров остаётся только аудио, а также чисто текстовые примеры. text-enhanced audio encoder for large language model based speech recognition via cross-modality pre-training with unpaired audio-text data можно отдельно обучать аудиоэнкодер и отдельно — языковую модель, но дальше аудиочасть и llm всё равно нужно «поженить». авторы хотят сделать этот стык более гладким: чтобы при совмещении ничего не развалилось и текстовая часть llm не деградировала. логика такая: выход аудиоветки дальше подаётся на вход llm. авторам важно, чтобы этот вход по форме и свойствам был ближе к тому, к чему llm привыкла в текстовом режиме. поэтому они добавляют отдельную текстовую ветку и общую часть — shared transformer blocks. эти общие блоки обучаются на текстовом сигнале, за счёт этого выходы аудио- и текстовой веток становятся ближе по представлению, так что llm проще работать с аудиовыходом. новизна тут скорее в подходе к обучению: вместо полностью раздельной тренировки (когда батчи идут либо аудио-, либо текстовые) в работе допускают совместное использование аудио и текста в одном батче — и за счёт этого обучение получается более стабильным. евгений ганкович ❣ специально для speech info три идеи на тему обучения speech-моделей сегодня делимся подборкой трёх концептуально интересных работ про обучение speech-моделей. первая — о контроле генерации на этапе декодирования, две остальные — о том, как аккуратнее стыковать речь и текст и обучать мультимодальные системы. length aware speech translation for video dubbing авторы решают понятную боль: как управлять длиной выходной последовательности (перевода), а не полагаться на эвристики поверх beam search (например, штрафы/нормализации за длину). нюанс таких эвристик в том, что они часто смещают ранжирование в сторону более коротких или более длинных гипотез. в статье предлагают разбить генерацию на несколько режимов длины: short, normal, long. вместо стандартного стартового токена (bos/sos) декодирование начинается со специального length-тега, и при обучении модель видит такие же теги — в итоге можно явно попросить «короткий» или «длинный» перевод. отдельно авторы модифицируют beam search: обычно на шаге прунинга оставляют top-k гипотез по скору. а тут при каждом прунинге стараются сохранять минимум по одной гипотезе каждого типа. это важно для случаев, когда «длинная» ветка обычно не доживает до конца: модель быстро завершает декодирование на коротких вариантах, а потом может выясниться, что более длинный — был бы лучше. очевидный минус подхода: поддержка длинных гипотез — это дополнительные затраты по производительности, потому что генерация идёт дольше. но сама идея «контролируем длину явно и держим разные длины в beam search» выглядит практичной. scheduled interleaved speech-text training for speech-to-speech translation with llms предположим, у нас есть текстовая llm, и мы хотим научить систему работать и со звуком. лобовой вариант — сразу добавить аудио в обучение и перейти в speech-режим. но такой переход получается слишком резким: до этого модель обучалась только на тексте, а теперь получает аудиопредставления, и на этом стыке всё легко может развалиться. чтобы этого избежать, текст обычно не убирают сразу, а продолжают подавать его вместе с аудио, постепенно меняя пропорции: сначала почти один текст и немного аудио, потом аудио становится больше, текста меньше — и так далее, вплоть до режима «почти только аудио». здесь авторы пошли ещё дальше и делают это не на уровне целых примеров, а внутри одного сэмпла: часть токенов — текстовые, часть — аудио. за счёт этого переход получается ещё мягче: сначала в сэмпле почти один текст и немного аудио, потом аудио всё больше. в конце для таких смешанных примеров остаётся только аудио, а также чисто текстовые примеры. text-enhanced audio encoder for large language model based speech recognition via cross-modality pre-training with unpaired audio-text data можно отдельно обучать аудиоэнкодер и отдельно — языковую модель, но дальше аудиочасть и llm всё равно нужно «поженить». авторы хотят сделать этот стык более гладким: чтобы при совмещении ничего не развалилось и текстовая часть llm не деградировала. логика такая: выход аудиоветки дальше подаётся на вход llm. авторам важно, чтобы этот вход по форме и свойствам был ближе к тому, к чему llm привыкла в текстовом режиме. поэтому они добавляют отдельную текстовую ветку и общую часть — shared transformer blocks. эти общие блоки обучаются на текстовом сигнале, за счёт этого выходы аудио- и текстовой веток становятся ближе по представлению, так что llm проще работать с аудиовыходом. новизна тут скорее в подходе к обучению: вместо полностью раздельной тренировки (когда батчи идут либо аудио-, либо текстовые) в работе допускают совместное использование аудио и текста в одном батче — и за счёт этого обучение получается более стабильным. евгений ганкович ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-23T09:41:29+00:00" href="./posts/106.html">2025-12-23 09:41 UTC</a></div>
      </div>
      <div class="post-body"><strong>Три идеи на тему обучения speech-моделей</strong><br><strong><br></strong>Сегодня делимся подборкой трёх концептуально интересных работ про обучение speech-моделей. Первая — о контроле генерации на этапе декодирования, две остальные — о том, как аккуратнее стыковать речь и текст и обучать мультимодальные системы. <br><br><a href="https://arxiv.org/abs/2506.00740v1" rel="nofollow noopener noreferrer"><strong>Length Aware Speech Translation for Video Dubbing</strong></a> <br><br>Авторы решают понятную боль: как управлять длиной выходной последовательности (перевода), а не полагаться на эвристики поверх beam search (например, штрафы/нормализации за длину). Нюанс таких эвристик в том, что они часто смещают ранжирование в сторону более коротких или более длинных гипотез.<br><br>В статье предлагают разбить генерацию на несколько режимов длины: short, normal, long. Вместо стандартного стартового токена (BOS/SOS) декодирование начинается со специального length-тега, и при обучении модель видит такие же теги — в итоге можно явно попросить «короткий» или «длинный» перевод.<br><br>Отдельно авторы модифицируют beam search: обычно на шаге прунинга оставляют top-k гипотез по скору. А тут при каждом прунинге стараются сохранять минимум по одной гипотезе каждого типа. Это важно для случаев, когда «длинная» ветка обычно не доживает до конца: модель быстро завершает декодирование на коротких вариантах, а потом может выясниться, что более длинный — был бы лучше.<br><br>Очевидный минус подхода: поддержка длинных гипотез — это дополнительные затраты по производительности, потому что генерация идёт дольше. Но сама идея «контролируем длину явно и держим разные длины в beam search» выглядит практичной.<br><br><a href="https://arxiv.org/abs/2506.10299v1" rel="nofollow noopener noreferrer"><strong>Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs</strong></a><br><br>Предположим, у нас есть текстовая LLM, и мы хотим научить систему работать и со звуком. Лобовой вариант — сразу добавить аудио в обучение и перейти в speech-режим. Но такой переход получается слишком резким: до этого модель обучалась только на тексте, а теперь получает аудиопредставления, и на этом стыке всё легко может развалиться.<br><br>Чтобы этого избежать, текст обычно не убирают сразу, а продолжают подавать его вместе с аудио, постепенно меняя пропорции: сначала почти один текст и немного аудио, потом аудио становится больше, текста меньше — и так далее, вплоть до режима «почти только аудио».<br><br>Здесь авторы пошли ещё дальше и делают это не на уровне целых примеров, а внутри одного сэмпла: часть токенов — текстовые, часть — аудио. За счёт этого переход получается ещё мягче: сначала в сэмпле почти один текст и немного аудио, потом аудио всё больше. В конце для таких смешанных примеров остаётся только аудио, а также чисто текстовые примеры.<br><br><a href="https://www.isca-archive.org/interspeech_2025/su25b_interspeech.html#" rel="nofollow noopener noreferrer"><strong>Text-Enhanced Audio Encoder for Large Language Model based Speech Recognition via Cross-Modality Pre-training with Unpaired Audio-Text Data</strong></a><br><br>Можно отдельно обучать аудиоэнкодер и отдельно — языковую модель, но дальше аудиочасть и LLM всё равно нужно «поженить». Авторы хотят сделать этот стык более гладким: чтобы при совмещении ничего не развалилось и текстовая часть LLM не деградировала.<br><br>Логика такая: выход аудиоветки дальше подаётся на вход LLM. Авторам важно, чтобы этот вход по форме и свойствам был ближе к тому, к чему LLM привыкла в текстовом режиме. Поэтому они добавляют отдельную текстовую ветку и общую часть — shared transformer blocks. Эти общие блоки обучаются на текстовом сигнале, за счёт этого выходы аудио- и текстовой веток становятся ближе по представлению, так что LLM проще работать с аудиовыходом.<br><br>Новизна тут скорее в подходе к обучению: вместо полностью раздельной тренировки (когда батчи идут либо аудио-, либо текстовые) в работе допускают совместное использование аудио и текста в одном батче — и за счёт этого обучение получается более стабильным.<br><br><em>Евгений Ганкович</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/106_480.webp" srcset="../assets/media/thumbs/106_480.webp 480w, ../assets/media/106.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/107_480.webp" srcset="../assets/media/thumbs/107_480.webp 480w, ../assets/media/107.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/108_480.webp" srcset="../assets/media/thumbs/108_480.webp 480w, ../assets/media/108.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="2" /></div></div>
      <div class="actions">
        <span>655 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/106" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/106.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="105" data-search="vibevoice technical report сегодня разберём статью о новой модели vibevoice, которая с помощью next-token-диффузии синтезирует длинную речь от лица нескольких спикеров. авторы во многом ссылаются на свою предыдущую работу multimodal latent language modeling with next-token diffusion, но там речь идёт совсем не о natural speech. два главных преимущества их новой разработки: — трансформер, который используется в модели, предсказывает не дискретные токены, а латенты. — vibevoice может генерировать аудио длительностью до полутора часов. модель принимает на вход голосовые промпты и текстовые описания. для того чтобы она лучше понимала контекст, авторы применяют два вида токенизации: — для дискретных токенов — look-up-table (кодбук, который из токена делает представление). лосс кросс-энтропийный, получают сэмплированием. — а для непрерывных данных берут 𝜎-vae-энкодер, который предсказывает что-то похожее на векторные представления. лосс — l2-диффузионный. диффузионная голова обучается end2end вместе с трансформером — предсказывает вход для vae по последнему латенту трансформера. новая система токенизации сохраняет точность воспроизведения звука и значительно повышает эффективность вычислений при обработке длинных последовательностей. непрерывность токенов позволяет уменьшить их количество до 7,5 на секунду. сжатие данных, по сравнению с популярной моделью encodec, улучшается в 80 раз. посмотреть код и послушать демо можно на github команды. евгений шабалин ❣ специально для speech info vibevoice technical report сегодня разберём статью о новой модели vibevoice, которая с помощью next-token-диффузии синтезирует длинную речь от лица нескольких спикеров. авторы во многом ссылаются на свою предыдущую работу multimodal latent language modeling with next-token diffusion , но там речь идёт совсем не о natural speech. два главных преимущества их новой разработки: — трансформер, который используется в модели, предсказывает не дискретные токены, а латенты. — vibevoice может генерировать аудио длительностью до полутора часов. модель принимает на вход голосовые промпты и текстовые описания. для того чтобы она лучше понимала контекст, авторы применяют два вида токенизации: — для дискретных токенов — look-up-table (кодбук, который из токена делает представление). лосс кросс-энтропийный, получают сэмплированием. — а для непрерывных данных берут 𝜎-vae-энкодер, который предсказывает что-то похожее на векторные представления. лосс — l2-диффузионный. диффузионная голова обучается end2end вместе с трансформером — предсказывает вход для vae по последнему латенту трансформера. новая система токенизации сохраняет точность воспроизведения звука и значительно повышает эффективность вычислений при обработке длинных последовательностей. непрерывность токенов позволяет уменьшить их количество до 7,5 на секунду. сжатие данных, по сравнению с популярной моделью encodec, улучшается в 80 раз. посмотреть код и послушать демо можно на github команды. евгений шабалин ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-17T10:46:32+00:00" href="./posts/105.html">2025-12-17 10:46 UTC</a></div>
      </div>
      <div class="post-body"><strong>VibeVoice Technical Report</strong><br><strong><br></strong>Сегодня разберём <a href="https://arxiv.org/abs/2508.19205" rel="nofollow noopener noreferrer">статью</a> о новой модели VibeVoice, которая с помощью next-token-диффузии синтезирует длинную речь от лица нескольких спикеров. <br><br>Авторы во многом ссылаются на свою предыдущую работу <a href="https://arxiv.org/abs/2412.08635" rel="nofollow noopener noreferrer">Multimodal Latent Language Modeling with Next-Token Diffusion</a>, но там речь идёт совсем не о natural speech. Два главных преимущества их новой разработки: <br><br>— Трансформер, который используется в модели, предсказывает не дискретные токены, а латенты. <br>— VibeVoice может генерировать аудио длительностью до полутора часов.<br><br>Модель принимает на вход голосовые промпты и текстовые описания. Для того чтобы она лучше понимала контекст, авторы применяют два вида токенизации:<br><br>— Для дискретных токенов — look-up-table (кодбук, который из токена делает представление). Лосс кросс-энтропийный, получают сэмплированием. <br>— А для непрерывных данных берут 𝜎-VAE-энкодер, который предсказывает что-то похожее на векторные представления. Лосс — L2-диффузионный. <br><br>Диффузионная голова обучается end2end вместе с трансформером — предсказывает вход для VAE по последнему латенту трансформера.<br><br>Новая система токенизации сохраняет точность воспроизведения звука и значительно повышает эффективность вычислений при обработке длинных последовательностей. Непрерывность токенов позволяет уменьшить их количество до 7,5 на секунду. Сжатие данных, по сравнению с популярной моделью EnCodec, улучшается в 80 раз. <br><br>Посмотреть код и послушать демо можно на <a href="https://github.com/microsoft/VibeVoice" rel="nofollow noopener noreferrer">GitHub</a> команды. <br><br><em>Евгений Шабалин</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/105_480.webp" srcset="../assets/media/thumbs/105_480.webp 480w, ../assets/media/105.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="105" data-image-index="0" /></div></div>
      <div class="actions">
        <span>792 просмотров · 28 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/105" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/105.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="104" data-search="три статьи о новых подходах к обработке речи активация устройства без специального слова, новая архитектура для верификации спикера и необычный подход к оптимизации памяти — сегодня разберём несколько идей о том, как работать с речью. adaptive knowledge distillation for device-directed speech detection интонационный споттер от apple — модель на 5 млн параметров, которая способна по интонации понимать, когда человек обращается к колонке. авторы утверждают, что она уже используется на некоторых смартфонах. сейчас в работе версия для колонок, но пока они столкнулись с проблемой в данных, которую не описывают подробно. можно предположить, что проблема в более сложной акустике. модель обучена на нескольких сотнях тысяч размеченных сэмплов и дополнительных псевдолейблах. авторы одновременно учат и инферят общую тушку для трёх споттеров: hey siri, siri и интонационного. по их словам, это позволяет существенно увеличить качество модели на всех трёх задачах. ещё очень помогает трёхуровневая дистилляция с asr. ученик — конформер. сверху три адаптера для споттеров, а учитель — asr на 80 млн параметров, 12 conformer-слоёв и энкодер. masv: speaker verification with global and local context mamba технология верификации голоса для смарт-очков от meta* — стриминговая модель masv, новая архитектура, в которой блоки ecapa-tdnn дополняются mamba-модулем. такая комбинация позволяет учитывать и локальные, и глобальные зависимости, но остаётся достаточно лёгкой для того, чтобы работать на устройствах с ограниченными ресурсами и с длинными аудиовходами. модель оценивали на внутренних данных: в студийных условиях записали около 5 млн высказываний от 30 тысяч человек. датасет получился действительно большим, качество предложенной модели на нём выглядит высоким. но без оценки на реальных пользовательских сценариях для смарт-очков результаты могут оказаться нерепрезентативными: студийная запись на один микрофон не отражает типичные режимы работы устройства. unfolding a few structures for the many: memory-efficient compression of conformer and speech foundation models необычный подход к оптимизации памяти, которую модель потребляет во время инференса. вместо того чтобы хранить десятки отдельных слоёв, авторы обучают небольшой трансформер с несколькими блоками так, чтобы одни и те же слои можно было последовательно использовать несколько раз. на инференсе это даёт логически более глубокую сеть без добавления новых параметров. большую модель и её варианты с разной логической глубиной тренируют совместно в одном цикле. чтобы выровнять их поведение друг относительно друга, авторы добавляют самодистилляцию: минимизируют kl-дивергенцию между самой глубокой и самой короткой конфигурациями. качество компактной модели заметно уступает исходной глубокой архитектуре. но при логическом дублировании слоёв (многократном прохождении через одни и те же блоки), сжатая модель практически догоняет большую, при этом потребляя меньше памяти. борис шелудько ❣ специально для speech info * компания meta признана экстремистской; её деятельность в россии запрещена. три статьи о новых подходах к обработке речи активация устройства без специального слова, новая архитектура для верификации спикера и необычный подход к оптимизации памяти — сегодня разберём несколько идей о том, как работать с речью. adaptive knowledge distillation for device-directed speech detection интонационный споттер от apple — модель на 5 млн параметров, которая способна по интонации понимать, когда человек обращается к колонке. авторы утверждают, что она уже используется на некоторых смартфонах. сейчас в работе версия для колонок, но пока они столкнулись с проблемой в данных, которую не описывают подробно. можно предположить, что проблема в более сложной акустике. модель обучена на нескольких сотнях тысяч размеченных сэмплов и дополнительных псевдолейблах. авторы одновременно учат и инферят общую тушку для трёх споттеров: hey siri, siri и интонационного. по их словам, это позволяет существенно увеличить качество модели на всех трёх задачах. ещё очень помогает трёхуровневая дистилляция с asr. ученик — конформер. сверху три адаптера для споттеров, а учитель — asr на 80 млн параметров, 12 conformer-слоёв и энкодер. masv: speaker verification with global and local context mamba технология верификации голоса для смарт-очков от meta* — стриминговая модель masv, новая архитектура, в которой блоки ecapa-tdnn дополняются mamba-модулем. такая комбинация позволяет учитывать и локальные, и глобальные зависимости, но остаётся достаточно лёгкой для того, чтобы работать на устройствах с ограниченными ресурсами и с длинными аудиовходами. модель оценивали на внутренних данных: в студийных условиях записали около 5 млн высказываний от 30 тысяч человек. датасет получился действительно большим, качество предложенной модели на нём выглядит высоким. но без оценки на реальных пользовательских сценариях для смарт-очков результаты могут оказаться нерепрезентативными: студийная запись на один микрофон не отражает типичные режимы работы устройства. unfolding a few structures for the many: memory-efficient compression of conformer and speech foundation models необычный подход к оптимизации памяти, которую модель потребляет во время инференса. вместо того чтобы хранить десятки отдельных слоёв, авторы обучают небольшой трансформер с несколькими блоками так, чтобы одни и те же слои можно было последовательно использовать несколько раз. на инференсе это даёт логически более глубокую сеть без добавления новых параметров. большую модель и её варианты с разной логической глубиной тренируют совместно в одном цикле. чтобы выровнять их поведение друг относительно друга, авторы добавляют самодистилляцию: минимизируют kl-дивергенцию между самой глубокой и самой короткой конфигурациями. качество компактной модели заметно уступает исходной глубокой архитектуре. но при логическом дублировании слоёв (многократном прохождении через одни и те же блоки), сжатая модель практически догоняет большую, при этом потребляя меньше памяти. борис шелудько ❣ специально для speech info * компания meta признана экстремистской; её деятельность в россии запрещена.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-10T08:28:01+00:00" href="./posts/104.html">2025-12-10 08:28 UTC</a></div>
      </div>
      <div class="post-body"><strong>Три статьи о новых подходах к обработке речи<br></strong><br>Активация устройства без специального слова, новая архитектура для верификации спикера и необычный подход к оптимизации памяти — сегодня разберём несколько идей о том, как работать с речью.<br><br><a href="https://arxiv.org/abs/2508.02801" rel="nofollow noopener noreferrer"><strong>Adaptive Knowledge Distillation for Device-Directed Speech Detection</strong></a><br><br>Интонационный споттер от Apple — модель на 5 млн параметров, которая способна по интонации понимать, когда человек обращается к колонке. Авторы утверждают, что она уже используется на некоторых смартфонах. Сейчас в работе версия для колонок, но пока они столкнулись с проблемой в данных, которую не описывают подробно. Можно предположить, что проблема в более сложной акустике.  <br><br>Модель обучена на нескольких сотнях тысяч размеченных сэмплов и дополнительных псевдолейблах. Авторы одновременно учат и инферят общую тушку для трёх споттеров: Hey Siri, Siri и интонационного. По их словам, это позволяет существенно увеличить качество модели на всех трёх задачах. <br><br>Ещё очень помогает трёхуровневая дистилляция с ASR. Ученик — конформер. Сверху три адаптера для споттеров, а учитель — ASR на 80 млн параметров, 12 conformer-слоёв и энкодер.<br><br><a href="https://arxiv.org/abs/2412.10989" rel="nofollow noopener noreferrer"><strong>MASV: Speaker Verification With Global And Local Context Mamba</strong></a><br><br>Технология верификации голоса для смарт-очков от Meta* — стриминговая модель MASV, новая архитектура, в которой блоки ECAPA-TDNN дополняются Mamba-модулем. Такая комбинация позволяет учитывать и локальные, и глобальные зависимости, но остаётся достаточно лёгкой для того, чтобы работать на устройствах с ограниченными ресурсами и с длинными аудиовходами.<br><br>Модель оценивали на внутренних данных: в студийных условиях записали около 5 млн высказываний от 30 тысяч человек. Датасет получился действительно большим, качество предложенной модели на нём выглядит высоким. Но без оценки на реальных пользовательских сценариях для смарт-очков результаты могут оказаться нерепрезентативными: студийная запись на один микрофон не отражает типичные режимы работы устройства. <br><br><a href="https://arxiv.org/pdf/2505.21237" rel="nofollow noopener noreferrer"><strong>Unfolding A Few Structures for The Many: Memory-Efficient Compression of Conformer and Speech Foundation Models</strong></a><br><br>Необычный подход к оптимизации памяти, которую модель потребляет во время инференса. Вместо того чтобы хранить десятки отдельных слоёв, авторы обучают небольшой трансформер с несколькими блоками так, чтобы одни и те же слои можно было последовательно использовать несколько раз. На инференсе это даёт логически более глубокую сеть без добавления новых параметров.<br><br>Большую модель и её варианты с разной логической глубиной тренируют совместно в одном цикле. Чтобы выровнять их поведение друг относительно друга, авторы добавляют самодистилляцию: минимизируют KL-дивергенцию между самой глубокой и самой короткой конфигурациями.<br><br>Качество компактной модели заметно уступает исходной глубокой архитектуре. Но при логическом дублировании слоёв (многократном прохождении через одни и те же блоки), сжатая модель практически догоняет большую, при этом потребляя меньше памяти.<br><br><em>Борис Шелудько</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><em><br></em><br><em>* Компания Meta</em> <em>признана экстремистской; её деятельность в России запрещена.</em></div>
      <div class="actions">
        <span>835 просмотров · 24 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/104" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/104.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="103" data-search="bfa: real-time multilingual text-to-speech forced alignment сегодня разберём статью о bournemouth forced aligner (bfa) — достойном преемнике знаменитого montreal forced aligner (mfa). forced alignment — это процедура определения временных границ фонем в аудио. долгое время популярным решением был точный, но медленный mfa на hmm-gmm. современные нейросетевые решения, вроде whisperx, быстрее, но часто уступают старичку mfa в качестве. приходится выбирать: либо скорость, либо точность. новая статья о bfa предлагает решение этой проблемы. что под капотом 1. contextless universal phoneme encoder (cupe). энкодер анализирует акустику каждого фрейма «без контекста», то есть независимо от соседних фонем. это ключевое отличие от классических моделей, использующих трифоны, и одна из главных причин прироста скорости. универсальность достигается за счёт обучения на широком наборе фонем из разных языков (librispeech, mls), что позволяет модели отлично обобщаться. авторы показали, что модель, обученная на семи европейских языках (без английского), успешно справляется с выравниванием английской речи. 2. ctc-декодер. ctc-алгоритм выравнивает последовательность фонем относительно аудио, но авторы модифицировали его для forced alignment. целевая последовательность для декодера строится как [blank, p1, blank, p2, ...]. эти blank-токены между фонемами используются для явного моделирования пауз и межфонемных промежутков. 3. multi-task-обучение. используется архитектура с двумя головами: одна для 67 классов фонем, другая для 17 укрупнённых фонемных групп. что это даёт на практике предсказание onset и offset. это главная фишка. bfa предсказывает не только начало, но и конец каждой фонемы, что позволяет моделировать межфонемные паузы в отличие от традиционных алайнеров. отличная скорость. за счёт бесконтекстной архитектуры bfa работает до 240 раз быстрее mfa. например, обработка корпуса buckeye занимает 1 час против 7 дней у mfa. умный декодинг. система использует иерархический подход (divide-and-conquer), разбивая аудио по найденным паузам на независимые сегменты и выравнивая каждый отдельно. специальный постпроцессинг гарантирует, что 100% фонем из транскрипции будут найдены и расставлены в аудио. что по метрикам recall у bfa сопоставим с mfa, особенно на разумных порогах в 40–60 мс. precision получился чуть ниже, но авторы заявляют, что это ожидаемый эффект: bfa предсказывает вдвое больше границ (onset + offset), а сравнивается с эталонной разметкой, где есть только onset. и да, название bfa выбрано не случайно: авторы продолжают традицию называть форс-алайнеры в честь города или университета, где над ними ведётся основная работа. так montreal forced aligner был связан с монреалем, а bournemouth forced aligner назван в честь борнмута. владимир гогорян ❣ специально для speech info bfa: real-time multilingual text-to-speech forced alignment сегодня разберём статью о bournemouth forced aligner (bfa) — достойном преемнике знаменитого montreal forced aligner (mfa). forced alignment — это процедура определения временных границ фонем в аудио. долгое время популярным решением был точный, но медленный mfa на hmm-gmm. современные нейросетевые решения, вроде whisperx, быстрее, но часто уступают старичку mfa в качестве. приходится выбирать: либо скорость, либо точность. новая статья о bfa предлагает решение этой проблемы. что под капотом 1. contextless universal phoneme encoder (cupe). энкодер анализирует акустику каждого фрейма «без контекста», то есть независимо от соседних фонем. это ключевое отличие от классических моделей, использующих трифоны, и одна из главных причин прироста скорости. универсальность достигается за счёт обучения на широком наборе фонем из разных языков (librispeech, mls), что позволяет модели отлично обобщаться. авторы показали, что модель, обученная на семи европейских языках (без английского), успешно справляется с выравниванием английской речи. 2. ctc-декодер. ctc-алгоритм выравнивает последовательность фонем относительно аудио, но авторы модифицировали его для forced alignment. целевая последовательность для декодера строится как [blank, p1, blank, p2, ...]. эти blank-токены между фонемами используются для явного моделирования пауз и межфонемных промежутков. 3. multi-task-обучение . используется архитектура с двумя головами: одна для 67 классов фонем, другая для 17 укрупнённых фонемных групп. что это даёт на практике предсказание onset и offset. это главная фишка. bfa предсказывает не только начало, но и конец каждой фонемы, что позволяет моделировать межфонемные паузы в отличие от традиционных алайнеров. отличная скорость. за счёт бесконтекстной архитектуры bfa работает до 240 раз быстрее mfa. например, обработка корпуса buckeye занимает 1 час против 7 дней у mfa. умный декодинг. система использует иерархический подход (divide-and-conquer), разбивая аудио по найденным паузам на независимые сегменты и выравнивая каждый отдельно. специальный постпроцессинг гарантирует, что 100% фонем из транскрипции будут найдены и расставлены в аудио. что по метрикам recall у bfa сопоставим с mfa, особенно на разумных порогах в 40–60 мс. precision получился чуть ниже, но авторы заявляют, что это ожидаемый эффект: bfa предсказывает вдвое больше границ (onset + offset), а сравнивается с эталонной разметкой, где есть только onset. и да, название bfa выбрано не случайно: авторы продолжают традицию называть форс-алайнеры в честь города или университета, где над ними ведётся основная работа. так montreal forced aligner был связан с монреалем, а bournemouth forced aligner назван в честь борнмута. владимир гогорян ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-12-02T10:51:50+00:00" href="./posts/103.html">2025-12-02 10:51 UTC</a></div>
      </div>
      <div class="post-body"><strong>BFA: Real-time Multilingual Text-to-Speech Forced Alignment</strong><br><br>Сегодня разберём <a href="https://arxiv.org/abs/2509.23147v1" rel="nofollow noopener noreferrer">статью</a> о Bournemouth Forced Aligner (BFA) — достойном преемнике знаменитого Montreal Forced Aligner (MFA).<br><br>Forced Alignment — это процедура определения временных границ фонем в аудио. Долгое время популярным решением был точный, но медленный MFA на HMM-GMM. Современные нейросетевые решения, вроде WhisperX, быстрее, но часто уступают старичку MFA в качестве. Приходится выбирать: либо скорость, либо точность. Новая статья о BFA предлагает решение этой проблемы.<br><br><strong>Что под капотом</strong><br><br><strong>1. Contextless Universal Phoneme Encoder (CUPE).</strong> Энкодер анализирует акустику каждого фрейма «без контекста», то есть независимо от соседних фонем. Это ключевое отличие от классических моделей, использующих трифоны, и одна из главных причин прироста скорости. Универсальность достигается за счёт обучения на широком наборе фонем из разных языков (LibriSpeech, MLS), что позволяет модели отлично обобщаться. Авторы показали, что модель, обученная на семи европейских языках (без английского), успешно справляется с выравниванием английской речи.<br><br><strong>2. CTC-декодер.</strong> CTC-алгоритм выравнивает последовательность фонем относительно аудио, но авторы модифицировали его для forced alignment. Целевая последовательность для декодера строится как [blank, p1, blank, p2, ...]. Эти blank-токены между фонемами используются для явного моделирования пауз и межфонемных промежутков.<br><br><strong>3. Multi-task-обучение</strong>. Используется архитектура с двумя головами: одна для 67 классов фонем, другая для 17 укрупнённых фонемных групп.<br><br><strong>Что это даёт на практике<br></strong><br><strong>Предсказание onset и offset. </strong>Это главная фишка. BFA предсказывает не только начало, но и конец каждой фонемы, что позволяет моделировать межфонемные паузы в отличие от традиционных алайнеров. <br><br><strong>Отличная скорость.</strong> За счёт бесконтекстной архитектуры BFA работает до 240 раз быстрее MFA. Например, обработка корпуса Buckeye занимает 1 час против 7 дней у MFA. <br><br><strong>Умный декодинг.</strong> Система использует иерархический подход (divide-and-conquer), разбивая аудио по найденным паузам на независимые сегменты и выравнивая каждый отдельно. Специальный постпроцессинг гарантирует, что 100% фонем из транскрипции будут найдены и расставлены в аудио.<br><br><strong>Что по метрикам<br></strong><br>Recall у BFA сопоставим с MFA, особенно на разумных порогах в 40–60 мс. Precision получился чуть ниже, но авторы заявляют, что это ожидаемый эффект: BFA предсказывает вдвое больше границ (onset + offset), а сравнивается с эталонной разметкой, где есть только onset.<br><br>И да, название BFA выбрано не случайно: авторы продолжают традицию называть форс-алайнеры в честь города или университета, где над ними ведётся основная работа. Так Montreal Forced Aligner был связан с Монреалем, а Bournemouth Forced Aligner назван в честь Борнмута.<br><br><em>Владимир Гогорян</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/103_480.webp" srcset="../assets/media/thumbs/103_480.webp 480w, ../assets/media/103.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="103" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 578 просмотров · 29 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/103" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/103.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="100" data-search="omnivinci: enhancing architecture and data for omni-modal understanding llm [2/2] во второй части обзора статьи мы подробно поговорим о тренировке модели и разберём разницу между implicit и explicit learning. обучение модели обучение модели можно разделить на два больших этапа — modality-specific и omni-modal части соответственно, llm-backbone при этом берётся предобученная (авторы используют qwen2.5-7b-instruct). обучение vision-модулей состоит из следующих стадий: - stage 1: vision projector alignment — учится только vision-проектор, решается задача генерации простых описаний. - stage 2: vision encoder alignment — учатся vision-энкодер и vision-проектор. - stage 3: vision pre-training — core-стадия, vision-энкодер заморожен, цель — finetune vision-проектора и llm. используются мультимодальные данные, модель учится интерпретировать и генерировать подписи к картинкам. - stage 4: image instruction tuning — finetune модели на задачи vision instruction following: ответы на общие и knowledge-based-вопросы, генерация сложных подписей, logical и vision reasoning, интерпретация документов, обработка диаграмм, etc. учатся все модули. - stage 5: video instruction tuning — финальная стадия, все части модели учатся на задачу понимания видео (распознавание активности (activity recognition); трекинг объекта во времени (по фреймам), time-sensitive qa). цель — получить у модели способность к temporal reasoning. после vision-этапа авторы получают «vision preliminary checkpoint» — достаточно хорошо обученные на vision-задачи энкодер, проектор и llm. обучение аудиомодулей делится на две стадии: - stage 1: audio projector &amp; encoder alignment. параметры llm и vision-части заморожены, учимся на задачи audio-based qa, captioning, asr. цель — обучить проектор аудиопредставлениям, согласованным с семантическим пространством языковой модели. - stage 2: audio instruction tuning: параметры llm не заморожены, llm учится вместе с аудиоэнкодером и аудиопроектором. учимся на все те же задачи + на задачу перевода речи; идея стадии в том, что разнообразные аудиальные задачи при обученном проекторе помогут аудиоэнкодеру выучить и низкоуровневые акустические признаки, и высокоуровневые семантические представления. omni-modal joint training во время мультимодального этапа обучения vision- и аудиоэнкодеры заморожены, учатся все остальные модули (omnialignnet, проекторы и llm). в статье описываются два подхода: implicit и explicit learning. implicit learning использует существующие датасеты video qa, где модель неявно учится интегрировать обе модальности, не получая однозначной информации о том, какая часть ответа взята из видеоряда, а какая — из звука. explicit learning использует синтетические данные, в которых указывается взаимосвязь между модальностями. главная разработка авторов — data engine, генерирующий отдельные описания для видео и аудио, а затем использующий llm с ризонингом (deepseek r1) для создания объединенных подписей, указывающих на то, как визуальная и аудиальная информация дополняют друг друга. проблема, которую решает этот подход — устранение «modality-specific hallucination» (fig 1). ключевой вывод мультимодальной стадии: описание видео, основанное на одной модальности, часто неточно; интеграция обеих модальностей критична, и explicit learning эффективно решает эту задачу (fig 2). финальная стадия обучения включает rl с использованием grpo. важный результат: grpo на audio-visual-данных сходится быстрее и качественнее, чем на чисто визуальных, что подтверждает ценность мультимодального подхода (fig 3). заключение в статье omnivinci представлен комплексный подход к созданию мультимодальных языковых моделей, включающий архитектурные инновации и продуманную стратегию обучения с разделением на modality-specific- и omni-modal-этапы. ключевой вклад — систематическое исследование подходов к мультимодальному обучению. авторы демонстрируют, что explicit learning с синтетическими данными эффективнее решает проблему modality-specific hallucination и улучшает общее качество модели. екатерина козлова ❣ специально для speech info omnivinci: enhancing architecture and data for omni-modal understanding llm [2/2] во второй части обзора статьи мы подробно поговорим о тренировке модели и разберём разницу между implicit и explicit learning. обучение модели обучение модели можно разделить на два больших этапа — modality-specific и omni-modal части соответственно, llm-backbone при этом берётся предобученная (авторы используют qwen2.5-7b-instruct). обучение vision-модулей состоит из следующих стадий: - stage 1: vision projector alignment — учится только vision-проектор, решается задача генерации простых описаний. - stage 2: vision encoder alignment — учатся vision-энкодер и vision-проектор. - stage 3: vision pre-training — core-стадия, vision-энкодер заморожен, цель — finetune vision-проектора и llm. используются мультимодальные данные, модель учится интерпретировать и генерировать подписи к картинкам. - stage 4: image instruction tuning — finetune модели на задачи vision instruction following: ответы на общие и knowledge-based-вопросы, генерация сложных подписей, logical и vision reasoning, интерпретация документов, обработка диаграмм, etc. учатся все модули. - stage 5: video instruction tuning — финальная стадия, все части модели учатся на задачу понимания видео (распознавание активности (activity recognition); трекинг объекта во времени (по фреймам), time-sensitive qa). цель — получить у модели способность к temporal reasoning. после vision-этапа авторы получают «vision preliminary checkpoint» — достаточно хорошо обученные на vision-задачи энкодер, проектор и llm. обучение аудиомодулей делится на две стадии: - stage 1: audio projector &amp;amp; encoder alignment. параметры llm и vision-части заморожены, учимся на задачи audio-based qa, captioning, asr. цель — обучить проектор аудиопредставлениям, согласованным с семантическим пространством языковой модели. - stage 2: audio instruction tuning: параметры llm не заморожены, llm учится вместе с аудиоэнкодером и аудиопроектором. учимся на все те же задачи + на задачу перевода речи; идея стадии в том, что разнообразные аудиальные задачи при обученном проекторе помогут аудиоэнкодеру выучить и низкоуровневые акустические признаки, и высокоуровневые семантические представления. omni-modal joint training во время мультимодального этапа обучения vision- и аудиоэнкодеры заморожены, учатся все остальные модули (omnialignnet, проекторы и llm). в статье описываются два подхода: implicit и explicit learning. implicit learning использует существующие датасеты video qa, где модель неявно учится интегрировать обе модальности, не получая однозначной информации о том, какая часть ответа взята из видеоряда, а какая — из звука. explicit learning использует синтетические данные, в которых указывается взаимосвязь между модальностями. главная разработка авторов — data engine, генерирующий отдельные описания для видео и аудио, а затем использующий llm с ризонингом (deepseek r1) для создания объединенных подписей, указывающих на то, как визуальная и аудиальная информация дополняют друг друга. проблема, которую решает этот подход — устранение «modality-specific hallucination» (fig 1). ключевой вывод мультимодальной стадии: описание видео, основанное на одной модальности, часто неточно; интеграция обеих модальностей критична, и explicit learning эффективно решает эту задачу (fig 2). финальная стадия обучения включает rl с использованием grpo. важный результат: grpo на audio-visual-данных сходится быстрее и качественнее, чем на чисто визуальных, что подтверждает ценность мультимодального подхода (fig 3). заключение в статье omnivinci представлен комплексный подход к созданию мультимодальных языковых моделей, включающий архитектурные инновации и продуманную стратегию обучения с разделением на modality-specific- и omni-modal-этапы. ключевой вклад — систематическое исследование подходов к мультимодальному обучению. авторы демонстрируют, что explicit learning с синтетическими данными эффективнее решает проблему modality-specific hallucination и улучшает общее качество модели. екатерина козлова ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-27T08:17:01+00:00" href="./posts/100.html">2025-11-27 08:17 UTC</a></div>
      </div>
      <div class="post-body"><strong>OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM [2/2]</strong><br><br>Во второй части обзора <a href="https://arxiv.org/abs/2510.15870v2" rel="nofollow noopener noreferrer">статьи</a> мы подробно поговорим о тренировке модели и разберём разницу между implicit и explicit learning.<br><br><strong>Обучение модели</strong><br><br>Обучение модели можно разделить на два больших этапа — modality-specific и omni-modal части соответственно, LLM-backbone при этом берётся предобученная (авторы используют Qwen2.5-7B-Instruct).<br><br>Обучение vision-модулей состоит из следующих стадий:<br><br>- Stage 1: Vision Projector Alignment — учится только vision-проектор, решается задача генерации простых описаний.<br>- Stage 2: Vision Encoder Alignment — учатся vision-энкодер и vision-проектор.<br>- Stage 3: Vision Pre-training — core-стадия, vision-энкодер заморожен, цель — finetune vision-проектора и LLM. Используются мультимодальные данные, модель учится интерпретировать и генерировать подписи к картинкам.<br>- Stage 4: Image Instruction Tuning — finetune модели на задачи vision instruction following: ответы на общие и knowledge-based-вопросы, генерация сложных подписей, logical и vision reasoning, интерпретация документов, обработка диаграмм, etc. Учатся все модули.<br>- Stage 5: Video Instruction Tuning — финальная стадия, все части модели учатся на задачу понимания видео (распознавание активности (activity recognition); трекинг объекта во времени (по фреймам), time-sensitive QA). Цель —  получить у модели способность к temporal reasoning. <br><br>После vision-этапа авторы получают «vision preliminary checkpoint» — достаточно хорошо обученные на vision-задачи энкодер, проектор и LLM. <br><br>Обучение аудиомодулей делится на две стадии:<br><br>- Stage 1: Audio Projector &amp; Encoder Alignment. Параметры LLM и vision-части заморожены, учимся на задачи audio-based QA, captioning, ASR. Цель — обучить проектор аудиопредставлениям, согласованным с семантическим пространством языковой модели.<br>- Stage 2: Audio Instruction Tuning: параметры LLM не заморожены, LLM учится вместе с аудиоэнкодером и аудиопроектором. Учимся на все те же задачи + на задачу перевода речи; идея стадии в том, что разнообразные аудиальные задачи при обученном проекторе помогут аудиоэнкодеру выучить и низкоуровневые акустические признаки, и высокоуровневые семантические представления. <br><br><strong>Omni-Modal Joint Training </strong><br><strong><br></strong>Во время мультимодального этапа обучения vision- и аудиоэнкодеры заморожены, учатся все остальные модули (OmniAlignNet, проекторы и LLM). В статье описываются два подхода: implicit и explicit learning. Implicit learning использует существующие датасеты Video QA, где модель неявно учится интегрировать обе модальности, не получая однозначной информации о том, какая часть ответа взята из видеоряда, а какая — из звука. Explicit learning использует синтетические данные, в которых указывается взаимосвязь между модальностями. Главная разработка авторов — data engine, генерирующий отдельные описания для видео и аудио, а затем использующий LLM с ризонингом (Deepseek R1) для создания объединенных подписей, указывающих на то, как визуальная и аудиальная информация дополняют друг друга. Проблема, которую решает этот подход — устранение «modality-specific hallucination» (fig 1). Ключевой вывод мультимодальной стадии: описание видео, основанное на одной модальности, часто неточно; интеграция обеих модальностей критична, и explicit learning эффективно решает эту задачу (fig 2).<strong><br></strong><br>Финальная стадия обучения включает RL с использованием GRPO. Важный результат: GRPO на audio-visual-данных сходится быстрее и качественнее, чем на чисто визуальных, что подтверждает ценность мультимодального подхода (fig 3).<br><br><strong>Заключение</strong><br><br>В статье OmniVinci представлен комплексный подход к созданию мультимодальных языковых моделей, включающий архитектурные инновации и продуманную стратегию обучения с разделением на modality-specific- и omni-modal-этапы. Ключевой вклад — систематическое исследование подходов к мультимодальному обучению. Авторы демонстрируют, что explicit learning с синтетическими данными эффективнее решает проблему modality-specific hallucination и улучшает общее качество модели.<br><br><em>Екатерина Козлова</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/100_480.webp" srcset="../assets/media/thumbs/100_480.webp 480w, ../assets/media/100.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="100" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/101_480.webp" srcset="../assets/media/thumbs/101_480.webp 480w, ../assets/media/101.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="100" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/102_480.webp" srcset="../assets/media/thumbs/102_480.webp 480w, ../assets/media/102.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="100" data-image-index="2" /></div></div>
      <div class="actions">
        <span>608 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/100" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/100.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="97" data-search="omnivinci: enhancing architecture and data for omni-modal understanding llm [1/2] сегодня начинаем разбирать статью, представляющую omnivinci — мультимодальную llm от nvidia, сравнимую по качеству с sota-моделями на бенчмарках всех модальностей. главным вкладом своей работы авторы считают не столько численные результаты на бенчмарках, сколько тот факт, что в техрепорте они объясняют все дизайн-решения, связанные с архитектурой модели и сбором данных для тренировки. одно из таких экспериментально подтвержденных решений — использование в качестве аудиоэнкодера энкодера из audio flamingo 3 (альтернативой выступал аудиоэнкодер qwen2.5). но особое внимание авторы уделяют трём идеям: omnialignnet, temporal embedding grouping и constrained rotary time embedding — о них и пойдёт речь в посте. omnialignnet в процессе обучения модели каждое видео разбивается на аудиопоток и поток изображений; при этом семантически эти потоки связаны, так как звук может дополнять картинку (и наоборот). чтобы аудиоэмбеддинги и эмбеддинги картинок были в одном латентном пространстве, модели и нужен модуль omnialignnet. общий пайплайн работы модуля выглядит следующим образом: 1) для аудиального и визуального потоков получаем последовательность эмбеддингов; 2) используем эти последовательности как key-value-эмбеддинги для cross attention; смешиваем их с query-эмбеддингом (свой для каждого потока) и получаем для каждого видео два мультимодальных эмбеддинга (audio-omni и visual-omni); 3) мультимодальные эмбеддинги прогоняем через три self-attention-слоя и l2-норму; 4) для батча мультимодальных эмбеддингов максимизируем кросс-модальное расстояние (скалярное прооизведение) для эмбеддингов, соответствующих разным сэмплам, и минимизируем в обратном случае (для эмбеддингов, соответствующих одинаковым сэмплам) — contrastive loss, похожий на то, что было в clip (симметричная кросс-энтропия из vision в audio и наоборот). omnialignnet хорошо справляется с моделированием верхнеуровневых семантических связей между аудиальными и визуальными эмбеддингами. при этом для того, чтобы моделировать более низкоуровневые связи, авторы предлагают два вида преобразования эмбеддингов, речь о которых пойдет дальше. teg: temporal embedding grouping идея teg в том, что правильное упорядочивание эмбеддингов разных модельностей помогает языковой модели лучше улавливать локальные смысловые зависимости. гиперпараметр этого метода — размер временного окна t_g, которое контролирует гранулярность группировки эмбеддингов: эмбеддинги делятся на чанки размером t_g; модальности внутри чанков чередуются. авторы утверждают, что такая гранулярная конкатенация эмбеддингов улучшает качество модели по сравнению с подходом, где эмбеддинги конкатенируются крупными блоками (блок vision → блок audio → блок vision…). constrained rotary time embedding (crte) crte — это модификация rotary time embeddings (rote, не путать с rope), трёхстадийный процесс, состоящий из генерации базовых частот, модификации этих частот и rotary-части, т.е. поворота эмбеддингов. на этапе генерации базовых частот в crte предлагается добавить гиперпараметр t_max — этот множитель добавляется в знаменатель при вычислении базовых частот. чем меньше t_max, тем больше учитываются близкие друг другу эмбеддинги (и наоборот): w_i = 2π/(t_max·θ^(i/c)). на этапе модификации базовых частот crte продолжает идею rote: для определения углов поворота эмбеддингов используются настоящие расстояния в секундах, в отличие от дискретных позиций у rope: ω_{i,j} = ω_i · t_j, где t_j — реальная временная метка. авторы проводят ablation study и доказывают, что все предложенные модификации действительно улучшают качество модели на мультимодальных бенчмарках (см. третий скриншот). в продолжении разбора мы подробнее расскажем, какие ещё эксперименты были проведены авторами статьи, а также о разнице между implicit learning и explicit learning у мультимодальных моделей. екатерина козлова ❣ специально для speech info omnivinci: enhancing architecture and data for omni-modal understanding llm [1/2] сегодня начинаем разбирать статью , представляющую omnivinci — мультимодальную llm от nvidia, сравнимую по качеству с sota-моделями на бенчмарках всех модальностей. главным вкладом своей работы авторы считают не столько численные результаты на бенчмарках, сколько тот факт, что в техрепорте они объясняют все дизайн-решения, связанные с архитектурой модели и сбором данных для тренировки. одно из таких экспериментально подтвержденных решений — использование в качестве аудиоэнкодера энкодера из audio flamingo 3 (альтернативой выступал аудиоэнкодер qwen2.5). но особое внимание авторы уделяют трём идеям: omnialignnet, temporal embedding grouping и constrained rotary time embedding — о них и пойдёт речь в посте. omnialignnet в процессе обучения модели каждое видео разбивается на аудиопоток и поток изображений; при этом семантически эти потоки связаны, так как звук может дополнять картинку (и наоборот). чтобы аудиоэмбеддинги и эмбеддинги картинок были в одном латентном пространстве, модели и нужен модуль omnialignnet. общий пайплайн работы модуля выглядит следующим образом: 1) для аудиального и визуального потоков получаем последовательность эмбеддингов; 2) используем эти последовательности как key-value-эмбеддинги для cross attention; смешиваем их с query-эмбеддингом (свой для каждого потока) и получаем для каждого видео два мультимодальных эмбеддинга (audio-omni и visual-omni); 3) мультимодальные эмбеддинги прогоняем через три self-attention-слоя и l2-норму; 4) для батча мультимодальных эмбеддингов максимизируем кросс-модальное расстояние (скалярное прооизведение) для эмбеддингов, соответствующих разным сэмплам, и минимизируем в обратном случае (для эмбеддингов, соответствующих одинаковым сэмплам) — contrastive loss, похожий на то, что было в clip (симметричная кросс-энтропия из vision в audio и наоборот). omnialignnet хорошо справляется с моделированием верхнеуровневых семантических связей между аудиальными и визуальными эмбеддингами. при этом для того, чтобы моделировать более низкоуровневые связи, авторы предлагают два вида преобразования эмбеддингов, речь о которых пойдет дальше. teg: temporal embedding grouping идея teg в том, что правильное упорядочивание эмбеддингов разных модельностей помогает языковой модели лучше улавливать локальные смысловые зависимости. гиперпараметр этого метода — размер временного окна t_g, которое контролирует гранулярность группировки эмбеддингов: эмбеддинги делятся на чанки размером t_g; модальности внутри чанков чередуются. авторы утверждают, что такая гранулярная конкатенация эмбеддингов улучшает качество модели по сравнению с подходом, где эмбеддинги конкатенируются крупными блоками (блок vision → блок audio → блок vision…). constrained rotary time embedding (crte) crte — это модификация rotary time embeddings (rote, не путать с rope), трёхстадийный процесс, состоящий из генерации базовых частот, модификации этих частот и rotary-части, т.е. поворота эмбеддингов. на этапе генерации базовых частот в crte предлагается добавить гиперпараметр t_max — этот множитель добавляется в знаменатель при вычислении базовых частот. чем меньше t_max, тем больше учитываются близкие друг другу эмбеддинги (и наоборот): w_i = 2π/(t_max·θ^(i/c)). на этапе модификации базовых частот crte продолжает идею rote: для определения углов поворота эмбеддингов используются настоящие расстояния в секундах, в отличие от дискретных позиций у rope: ω_{i,j} = ω_i · t_j, где t_j — реальная временная метка. авторы проводят ablation study и доказывают, что все предложенные модификации действительно улучшают качество модели на мультимодальных бенчмарках (см. третий скриншот). в продолжении разбора мы подробнее расскажем, какие ещё эксперименты были проведены авторами статьи, а также о разнице между implicit learning и explicit learning у мультимодальных моделей. екатерина козлова ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-19T10:19:17+00:00" href="./posts/97.html">2025-11-19 10:19 UTC</a></div>
      </div>
      <div class="post-body"><strong>OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM [1/2]<br></strong><br>Сегодня начинаем разбирать <a href="https://arxiv.org/abs/2510.15870v2" rel="nofollow noopener noreferrer">статью</a>, представляющую OmniVinci — мультимодальную LLM от Nvidia, сравнимую по качеству с SOTA-моделями на бенчмарках всех модальностей. Главным вкладом своей работы авторы считают не столько численные результаты на бенчмарках, сколько тот факт, что в техрепорте они объясняют все дизайн-решения, связанные с архитектурой модели и сбором данных для тренировки. Одно из таких экспериментально подтвержденных решений — использование в качестве аудиоэнкодера энкодера из Audio Flamingo 3 (альтернативой выступал аудиоэнкодер Qwen2.5). Но особое внимание авторы уделяют трём идеям: OmniAlignNet, Temporal Embedding Grouping и Constrained Rotary Time Embedding — о них и пойдёт речь в посте.<br><br><strong>OmniAlignNet</strong><br><br>В процессе обучения модели каждое видео разбивается на аудиопоток и поток изображений; при этом семантически эти потоки связаны, так как звук может дополнять картинку (и наоборот). Чтобы аудиоэмбеддинги и эмбеддинги картинок были в одном латентном пространстве, модели и нужен модуль OmniAlignNet. <br><br>Общий пайплайн работы модуля выглядит следующим образом:<br><br>1) для аудиального и визуального потоков получаем последовательность эмбеддингов;<br><br>2) используем эти последовательности как key-value-эмбеддинги для cross attention; смешиваем их с query-эмбеддингом (свой для каждого потока) и получаем для каждого видео два мультимодальных эмбеддинга (audio-omni и visual-omni);<br><br>3) мультимодальные эмбеддинги прогоняем через три self-attention-слоя и L2-норму;<br><br>4) для батча мультимодальных эмбеддингов максимизируем кросс-модальное расстояние (скалярное прооизведение) для эмбеддингов, соответствующих разным сэмплам, и минимизируем в обратном случае (для эмбеддингов, соответствующих одинаковым сэмплам) — contrastive loss, похожий на то, что было в CLIP (симметричная кросс-энтропия из vision в audio и наоборот).<br><br>OmniAlignNet хорошо справляется с моделированием верхнеуровневых семантических связей между аудиальными и визуальными эмбеддингами. При этом для того, чтобы моделировать более низкоуровневые связи, авторы предлагают два вида преобразования эмбеддингов, речь о которых пойдет дальше.  <br><br><strong>TEG: Temporal Embedding Grouping</strong><br><br>Идея TEG в том, что правильное упорядочивание эмбеддингов разных модельностей помогает языковой модели лучше улавливать локальные смысловые зависимости. Гиперпараметр этого метода — размер временного окна T_g, которое контролирует гранулярность группировки эмбеддингов: эмбеддинги делятся на чанки размером T_g; модальности внутри чанков чередуются. <br><br>Авторы утверждают, что такая гранулярная конкатенация эмбеддингов улучшает качество модели по сравнению с подходом, где эмбеддинги конкатенируются крупными блоками (блок vision →  блок audio →  блок vision…).<br><br><strong>Constrained Rotary Time Embedding (CRTE)<br></strong><br>CRTE — это модификация Rotary Time Embeddings (RoTE, не путать с RoPE), трёхстадийный процесс, состоящий из генерации базовых частот, модификации этих частот и rotary-части, т.е. поворота эмбеддингов. <br><br>На этапе генерации базовых частот в CRTE предлагается добавить гиперпараметр T_max — этот множитель добавляется в знаменатель при вычислении базовых частот. Чем меньше T_max, тем больше учитываются близкие друг другу эмбеддинги (и наоборот): w_i = 2π/(T_max·θ^(i/C)).<br><br>На этапе модификации базовых частот CRTE продолжает идею RoTE: для определения углов поворота эмбеддингов используются настоящие расстояния в секундах, в отличие от дискретных позиций у RoPE: Ω_{i,j} = ω_i · t_j, где t_j — реальная временная метка.<br><br>Авторы проводят ablation study и доказывают, что все предложенные модификации действительно улучшают качество модели на мультимодальных бенчмарках (см. третий скриншот). <br><br>В продолжении разбора мы подробнее расскажем, какие ещё эксперименты были проведены авторами статьи, а также о разнице между implicit learning и explicit learning у мультимодальных моделей.<br><br><em>Екатерина Козлова</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/97_480.webp" srcset="../assets/media/thumbs/97_480.webp 480w, ../assets/media/97.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="97" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/98_480.webp" srcset="../assets/media/thumbs/98_480.webp 480w, ../assets/media/98.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="97" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/99_480.webp" srcset="../assets/media/thumbs/99_480.webp 480w, ../assets/media/99.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="97" data-image-index="2" /></div></div>
      <div class="actions">
        <span>609 просмотров · 30 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/97" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/97.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="95" data-search="vevo2: bridging controllable speech and singing voice generation via unified prosody learning разбираем статью о vevo2 — унифицированной модели для генерации контролируемой речи и пения. цель авторов — создать гибкий механизм независимого управления текстом, просодией (мелодией), стилем (акцентом, эмоциями, вибрато) и тембром для обеих модальностей. в этом посте разберём вклад, который работа вносит в индустрию. вклад в данные для пения во-первых, авторы решают проблему дефицита аннотированных данных для пения. предлагаются два аудиотокенизатора (не требующих ручной аннотации для музыкальных данных): — prosody tokenizer (6.25 гц) — vq-vae, обучаемый на реконструкции хромаграммы; кодирует просодию речи, мелодию пения и даже инструментальных звуков. — content-style tokenizer (12.5 гц) — vq-vae, реконструирующий хромаграмму и скрытые состояния whisper; кодирует лингвистический контент, просодию и стиль для речи и пения, устойчив к различному тембру, что авторы демонстрируют результатами в voice conversion. выбор хромаграммы с низкой частотой обусловлен простотой расчёта, устойчивостью к шуму и различным источникам, а также octave-free-представлением (снижает разрыв диапазона f0 между речью и пением). архитектура vevo2 включает два этапа: 1. авторегрессивное моделирование content-style-токенов (ar-трансформер, инициализированный qwen 2.5 (0,5b): — на вход принимает текст + (опционально) prosody-токены + content-style токены референса. — поддерживает explicit prosody learning (epl) (просодия как явный ввод) и implicit prosody learning (ipl) (просодия генерируется in-context). — во время претрейна стратегии epl/ipl чередуются равновероятно для всех данных — это унифицирует обучение речи и пения. 2. акустическое моделирование (flow-matching): — преобразует content-style-токены в мел-спектрограмму, обуславливаясь на референс тембра. — финальный waveform — через vocos-вокодер, дообученный на речь и пение. вклад в пострейн (grpo) этот этап нужен для повышения разборчивости речи и просодической схожести с контролирующей последовательностью, а также для обобщения на инструментальные источники мелодии. используется сумма двух наград: — intelligibility reward: обучается на контрастив хороших-плохих пар (текст, content-style токены). стратегии epl/ipl как и на претрейне чередуются равновероятно. — prosody similarity reward: косинусная близость между хромаграммой ground-truth и реконструкцией (через декодер content-style tokenizer) из сгенерированных content-style-токенов. унифицированное моделирование даёт взаимные преимущества: обилие речевых данных улучшает качество пения, пение — выразительность и просодический контроль речи. vevo2 достигает sota в svs, svc, humming-to-singing, instrument-to-singing и близких к лучшим результатов в tts/vc. дмитрий попов ❣ специально для speech info vevo2: bridging controllable speech and singing voice generation via unified prosody learning разбираем статью о vevo2 — унифицированной модели для генерации контролируемой речи и пения. цель авторов — создать гибкий механизм независимого управления текстом, просодией (мелодией), стилем (акцентом, эмоциями, вибрато) и тембром для обеих модальностей. в этом посте разберём вклад, который работа вносит в индустрию. вклад в данные для пения во-первых, авторы решают проблему дефицита аннотированных данных для пения. предлагаются два аудиотокенизатора (не требующих ручной аннотации для музыкальных данных): — prosody tokenizer (6.25 гц) — vq-vae, обучаемый на реконструкции хромаграммы; кодирует просодию речи, мелодию пения и даже инструментальных звуков. — content-style tokenizer (12.5 гц) — vq-vae, реконструирующий хромаграмму и скрытые состояния whisper; кодирует лингвистический контент, просодию и стиль для речи и пения, устойчив к различному тембру, что авторы демонстрируют результатами в voice conversion. выбор хромаграммы с низкой частотой обусловлен простотой расчёта, устойчивостью к шуму и различным источникам, а также octave-free-представлением (снижает разрыв диапазона f0 между речью и пением). архитектура vevo2 включает два этапа: 1. авторегрессивное моделирование content-style-токенов (ar-трансформер, инициализированный qwen 2.5 (0,5b): — на вход принимает текст + (опционально) prosody-токены + content-style токены референса. — поддерживает explicit prosody learning (epl) (просодия как явный ввод) и implicit prosody learning (ipl) (просодия генерируется in-context). — во время претрейна стратегии epl/ipl чередуются равновероятно для всех данных — это унифицирует обучение речи и пения. 2. акустическое моделирование (flow-matching): — преобразует content-style-токены в мел-спектрограмму, обуславливаясь на референс тембра. — финальный waveform — через vocos-вокодер, дообученный на речь и пение. вклад в пострейн (grpo) этот этап нужен для повышения разборчивости речи и просодической схожести с контролирующей последовательностью, а также для обобщения на инструментальные источники мелодии. используется сумма двух наград: — intelligibility reward: обучается на контрастив хороших-плохих пар (текст, content-style токены). стратегии epl/ipl как и на претрейне чередуются равновероятно. — prosody similarity reward: косинусная близость между хромаграммой ground-truth и реконструкцией (через декодер content-style tokenizer) из сгенерированных content-style-токенов. унифицированное моделирование даёт взаимные преимущества: обилие речевых данных улучшает качество пения, пение — выразительность и просодический контроль речи. vevo2 достигает sota в svs, svc, humming-to-singing, instrument-to-singing и близких к лучшим результатов в tts/vc. дмитрий попов ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-12T11:07:32+00:00" href="./posts/95.html">2025-11-12 11:07 UTC</a></div>
      </div>
      <div class="post-body"><strong>Vevo2: Bridging Controllable Speech and Singing Voice Generation via Unified Prosody Learning</strong><br><br>Разбираем <a href="https://arxiv.org/pdf/2508.16332v1" rel="nofollow noopener noreferrer">статью</a> о Vevo2 — унифицированной модели для генерации контролируемой речи и пения. Цель авторов — создать гибкий механизм независимого управления текстом, просодией (мелодией), стилем (акцентом, эмоциями, вибрато) и тембром для обеих модальностей. В этом посте разберём вклад, который работа вносит в индустрию.<br><br><strong>Вклад в данные для пения<br></strong><br>Во-первых, авторы решают проблему дефицита аннотированных данных для пения. Предлагаются два аудиотокенизатора (не требующих ручной аннотации для музыкальных данных):<br><br>— Prosody Tokenizer (6.25 Гц) — VQ-VAE, обучаемый на реконструкции хромаграммы; кодирует просодию речи, мелодию пения и даже инструментальных звуков.<br>— Content-Style Tokenizer (12.5 Гц) — VQ-VAE, реконструирующий хромаграмму и скрытые состояния Whisper; кодирует лингвистический контент, просодию и стиль для речи и пения, устойчив к различному тембру, что авторы демонстрируют результатами в Voice Conversion.<br><br>Выбор хромаграммы с низкой частотой обусловлен простотой расчёта, устойчивостью к шуму и различным источникам, а также octave-free-представлением (снижает разрыв диапазона F0 между речью и пением).<br><br><strong>Архитектура Vevo2 включает два этапа:<br></strong><br>1. Авторегрессивное моделирование Content-Style-токенов (AR-трансформер, инициализированный Qwen 2.5 (0,5B):<br><br>— На вход принимает текст + (опционально) Prosody-токены + Content-Style токены референса.<br>— Поддерживает Explicit Prosody Learning (EPL) (просодия как явный ввод) и Implicit Prosody Learning (IPL) (просодия генерируется in-context).<br>— Во время претрейна стратегии EPL/IPL чередуются равновероятно для всех данных — это унифицирует обучение речи и пения.<br><br>2. Акустическое моделирование (Flow-Matching):<br>— Преобразует Content-Style-токены в мел-спектрограмму, обуславливаясь на референс тембра.<br>— Финальный waveform — через Vocos-вокодер, дообученный на речь и пение.<br><br><strong>Вклад в пострейн (GRPO)<br></strong><br>Этот этап нужен для повышения разборчивости речи и просодической схожести с контролирующей последовательностью, а также для обобщения на инструментальные источники мелодии.<br><br>Используется сумма двух наград:<br>— Intelligibility Reward: обучается на контрастив хороших-плохих пар (текст, Content-Style токены). Стратегии EPL/IPL как и на претрейне чередуются равновероятно.<br>— Prosody Similarity Reward: косинусная близость между хромаграммой ground-truth и реконструкцией (через декодер Content-Style Tokenizer) из сгенерированных Content-Style-токенов.<br><br>Унифицированное моделирование даёт взаимные преимущества: обилие речевых данных улучшает качество пения, пение — выразительность и просодический контроль речи. Vevo2 достигает SOTA в SVS, SVC, humming-to-singing, instrument-to-singing и близких к лучшим результатов в TTS/VC.<br><br><em>Дмитрий Попов</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/95_480.webp" srcset="../assets/media/thumbs/95_480.webp 480w, ../assets/media/95.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="95" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/96_480.webp" srcset="../assets/media/thumbs/96_480.webp 480w, ../assets/media/96.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="95" data-image-index="1" /></div></div>
      <div class="actions">
        <span>1 255 просмотров · 24 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/95" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/95.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="94" data-search="qwen3-omni technical report [2/2] продолжаем разбор техрепорта qwen3-omni. в первой части рассказали об архитектурных отличиях от qwen2.5-omni. в генерации аудио также произошли изменения. помимо talker, используются ещё две модели: mtp-модуль (авторегрессионная dense-модель размером 80м) и code2wav (декодер для кодеков, сверточная модель размером 200м), которые работают с rvq-токенами (residual vector quantization). схема работы следующая: - поверх talker есть линейный слой, который предсказывает нулевой кодбук. - с помощью mtp-модели, вместо того, чтобы предсказывать оставшиеся rvq-токены последовательно, предсказываются сразу все — по аналогии с multi token prediction (такой подход используется, например, в deepseek). - полученные rvq-токены подаются в модель code2wav, которая преобразует их в вейвформу. этот подход эффективнее, чем в qwen2.5-omni, где использовались трансформерные модели для отображения в мел-спектрограмму и только затем — в вейвформу. как и в случае qwen2.5-omni, значительная часть статьи уделена тому, как модель стримится. авторы вновь используют асинхронный prefilling. как только thinker заканчивает prefilling для текущего чанка, его выходы отдаются в talker, чтобы он тоже мог начать prefilling. при этом thinker уже начинает обрабатывать следующий чанк. также исследователи заявляют, что используют только левый контекст для генерации аудио, в отличие от qwen2.5-omni, где создавалась задержка из-за того, что необходимо было накопить немного правого контекста. как и для qwen2.5-omni, предобучение проходит в три этапа: - на первом замораживается llm и обучаются только энкодеры и адаптеры для них (encoder alignment stage). причём начинают именно с адаптеров. в качестве инициализации для llm используется qwen3, для энкодера изображений — qwen3-vl, для энкодера аудио — новый аудиоэнкодер, который обучили ранее. - на второй стадии все параметры размораживаются, добавляются более разнообразные мультимодальные данные и задачи. - на третьем этапе увеличивается контекстное окно с 8192 до 32768 токенов, чтобы модель могла обрабатывать длинные входы. также в данные добавляются более длинные аудио/видео. дальше начинается post-training, который разделён для thinker и talker. для thinker была только одна стадия — sft (supervised fine-tuning), теперь добавлены ещё две: дистилляция, которая используется для получения более компактных llm (по принципу strong-to-weak distillation из qwen3), и rl (gsppo) — обучение с подкреплением, где оценивается качество отклика модели. для задач с чёткими критериями (mathematics, coding) применяются награды, которые вычисляются по заранее заданным правилам. для остальных задач, где сложно сформулировать чёткую награду, используется подход llm-as-a-judge, где для оценки ответа модели используются qwen3 и qwen2.5-vl. для talker раньше было три стадии, теперь — четыре. первая — предварительное обучение на большом объёме данных с мультимодальным контекстом; вторая — добавление более качественных данных для борьбы с галлюцинациями после первой стадии; затем dpo (direct preference optimization) и speaker fine-tuning, чтобы talker научился копировать тембр и интонации во время генерации аудио. в качестве бонуса исследователи выпустили в опенсорс qwen3-omni-30b-a3b-captioner — модель для решения задачи audio captioning на основе qwen3-omni-30b-a3b. в результатах показано, что модель не теряет в качестве по сравнению с немультимодальными: сильна в asr (китайский, английский), превосходит в музыкальных задачах, держит sota в тексте и визуале и поддерживает межъязыковой voice cloning. александр паланевич ❣ специально для speech info qwen3-omni technical report [2/2] продолжаем разбор техрепорта qwen3-omni. в первой части рассказали об архитектурных отличиях от qwen2.5-omni. в генерации аудио также произошли изменения. помимо talker, используются ещё две модели: mtp-модуль (авторегрессионная dense-модель размером 80м) и code2wav (декодер для кодеков, сверточная модель размером 200м), которые работают с rvq-токенами (residual vector quantization). схема работы следующая: - поверх talker есть линейный слой, который предсказывает нулевой кодбук. - с помощью mtp-модели, вместо того, чтобы предсказывать оставшиеся rvq-токены последовательно, предсказываются сразу все — по аналогии с multi token prediction (такой подход используется, например, в deepseek). - полученные rvq-токены подаются в модель code2wav, которая преобразует их в вейвформу. этот подход эффективнее, чем в qwen2.5-omni, где использовались трансформерные модели для отображения в мел-спектрограмму и только затем — в вейвформу. как и в случае qwen2.5-omni, значительная часть статьи уделена тому, как модель стримится. авторы вновь используют асинхронный prefilling. как только thinker заканчивает prefilling для текущего чанка, его выходы отдаются в talker, чтобы он тоже мог начать prefilling. при этом thinker уже начинает обрабатывать следующий чанк. также исследователи заявляют, что используют только левый контекст для генерации аудио, в отличие от qwen2.5-omni, где создавалась задержка из-за того, что необходимо было накопить немного правого контекста. как и для qwen2.5-omni, предобучение проходит в три этапа: - на первом замораживается llm и обучаются только энкодеры и адаптеры для них (encoder alignment stage). причём начинают именно с адаптеров. в качестве инициализации для llm используется qwen3, для энкодера изображений — qwen3-vl, для энкодера аудио — новый аудиоэнкодер, который обучили ранее. - на второй стадии все параметры размораживаются, добавляются более разнообразные мультимодальные данные и задачи. - на третьем этапе увеличивается контекстное окно с 8192 до 32768 токенов, чтобы модель могла обрабатывать длинные входы. также в данные добавляются более длинные аудио/видео. дальше начинается post-training, который разделён для thinker и talker. для thinker была только одна стадия — sft (supervised fine-tuning), теперь добавлены ещё две: дистилляция, которая используется для получения более компактных llm (по принципу strong-to-weak distillation из qwen3), и rl (gsppo) — обучение с подкреплением, где оценивается качество отклика модели. для задач с чёткими критериями (mathematics, coding) применяются награды, которые вычисляются по заранее заданным правилам. для остальных задач, где сложно сформулировать чёткую награду, используется подход llm-as-a-judge, где для оценки ответа модели используются qwen3 и qwen2.5-vl. для talker раньше было три стадии, теперь — четыре. первая — предварительное обучение на большом объёме данных с мультимодальным контекстом; вторая — добавление более качественных данных для борьбы с галлюцинациями после первой стадии; затем dpo (direct preference optimization) и speaker fine-tuning, чтобы talker научился копировать тембр и интонации во время генерации аудио. в качестве бонуса исследователи выпустили в опенсорс qwen3-omni-30b-a3b-captioner — модель для решения задачи audio captioning на основе qwen3-omni-30b-a3b. в результатах показано, что модель не теряет в качестве по сравнению с немультимодальными: сильна в asr (китайский, английский), превосходит в музыкальных задачах, держит sota в тексте и визуале и поддерживает межъязыковой voice cloning. александр паланевич ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-11-06T12:20:15+00:00" href="./posts/94.html">2025-11-06 12:20 UTC</a></div>
      </div>
      <div class="post-body"><strong>Qwen3-Omni Technical Report [2/2]</strong><br><strong><br></strong>Продолжаем разбор <a href="https://arxiv.org/html/2509.17765v1" rel="nofollow noopener noreferrer">техрепорта</a> Qwen3-Omni. В первой части <a href="https://t.me/speechinfo/93" rel="nofollow noopener noreferrer">рассказали</a> об архитектурных отличиях от Qwen2.5-Omni.<br><br>В генерации аудио также произошли изменения. Помимо Talker, используются ещё две модели: MTP-модуль (авторегрессионная dense-модель размером 80М) и Code2Wav (декодер для кодеков, сверточная модель размером 200М), которые работают с RVQ-токенами (Residual Vector Quantization). Схема работы следующая:<br><br>- Поверх Talker есть линейный слой, который предсказывает нулевой кодбук.<br>- С помощью MTP-модели, вместо того, чтобы предсказывать оставшиеся RVQ-токены последовательно, предсказываются сразу все — по аналогии с multi token prediction (такой подход используется, например, в Deepseek).<br>- Полученные RVQ-токены подаются в модель Code2Wav, которая преобразует их в вейвформу. Этот подход эффективнее, чем в Qwen2.5-Omni, где использовались трансформерные модели для отображения в мел-спектрограмму и только затем — в вейвформу.<br><br>Как и в случае Qwen2.5-Omni, значительная часть статьи уделена тому, как модель стримится. Авторы вновь используют асинхронный prefilling. Как только Thinker заканчивает prefilling для текущего чанка, его выходы отдаются в Talker, чтобы он тоже мог начать prefilling. При этом Thinker уже начинает обрабатывать следующий чанк.<br><br>Также исследователи заявляют, что используют только левый контекст для генерации аудио, в отличие от Qwen2.5-Omni, где создавалась задержка из-за того, что необходимо было накопить немного правого контекста. <br><br>Как и для Qwen2.5-Omni, предобучение проходит в три этапа:<br><br>- На первом замораживается LLM и обучаются только энкодеры и адаптеры для них (Encoder Alignment Stage). Причём начинают именно с адаптеров. В качестве инициализации для LLM используется Qwen3, для энкодера изображений — Qwen3-VL, для энкодера аудио — новый аудиоэнкодер, который обучили ранее.<br>- На второй стадии все параметры размораживаются, добавляются более разнообразные мультимодальные данные и задачи.<br>- На третьем этапе увеличивается контекстное окно с 8192 до 32768 токенов, чтобы модель могла обрабатывать длинные входы. Также в данные добавляются более длинные аудио/видео.<br><br>Дальше начинается post-training, который разделён для Thinker и Talker. <br><br>Для Thinker была только одна стадия — SFT (supervised fine-tuning), теперь добавлены ещё две: дистилляция, которая используется для получения более компактных LLM (по принципу Strong-to-Weak Distillation из Qwen3), и RL (GSPPO) — обучение с подкреплением, где оценивается качество отклика модели. Для задач с чёткими критериями (mathematics, coding) применяются награды, которые вычисляются по заранее заданным правилам. Для остальных задач, где сложно сформулировать чёткую награду, используется подход LLM-as-a-judge, где для оценки ответа модели используются Qwen3 и Qwen2.5-VL.<br><br>Для Talker раньше было три стадии, теперь — четыре. Первая — предварительное обучение на большом объёме данных с мультимодальным контекстом; вторая — добавление более качественных данных для борьбы с галлюцинациями после первой стадии; затем DPO (Direct Preference Optimization) и Speaker Fine-Tuning, чтобы Talker научился копировать тембр и интонации во время генерации аудио.<br><br>В качестве бонуса исследователи выпустили в опенсорс Qwen3-Omni-30B-A3B-Captioner — модель для решения задачи audio captioning на основе Qwen3-Omni-30B-A3B.<br><br>В результатах показано, что модель не теряет в качестве по сравнению с немультимодальными: сильна в ASR (китайский, английский), превосходит в музыкальных задачах, держит SOTA в тексте и визуале и поддерживает межъязыковой voice cloning.<br><br><em>Александр Паланевич</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a></div>
      <div class="actions">
        <span>1 018 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/94" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/94.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="93" data-search="qwen3-omni technical report [1/2] сегодня начинаем разбирать техрепорт qwen 3 omni — самого нового мультимодального qwen. авторы заявляют, что модель достигает sota-результатов или близких к ним сразу на всех типах данных. качество не ухудшается ни в одном направлении по сравнению с немультимодальными моделями qwen. другими словами, qwen 3 omni показывает качество на тексте не хуже, чем текстовая версия qwen 3 или визуальная qwen 3-vl, при сопоставимых размерах моделей. из интересных нововведений: модель умеет обрабатывать очень длинные входы — до 40 минут. также она поддерживает большое количество языков: как для взаимодействий текстом (119), так и в задачах speech-understanding (19) или speech-generation (10). в статье отмечается, что улучшен ризонинг независимо от модальности входа, а latency остаётся низкой — всё работает достаточно быстро. идейно qwen 3 omni очень похож на qwen 2.5 omni: — используется thinker-talker-архитектура. thinker — языковая модель, которая умеет принимать на вход данные разных модальностей и выдавать текст. talker принимает выходы thinker и генерирует аудио. важное отличие от предыдущего qwen в том, что теперь thinker/talker — это moe-модели (mixture of experts). — разные модальности кодируются за счёт соответствующих энкодеров. в qwen 3 omni эти энкодеры обновили: для картинок вместо qwen 2.5 vl используется qwen 3 vl, а для аудио авторы обучили свой энкодер с нуля. одно из основных отличий новой модели от 2.5 omni заключается в том, как выходы thinker подаются в talker. для изображений и аудио по-прежнему используют хиддены thinker для соответствующих модальностей, а вот текст теперь передаётся в виде обычных текстовых эмбеддингов. по словам авторов, эмбеддинги уже достаточно хорошо отражают текст, а скрытые состояния избыточны. такой подход делает систему гибче: можно использовать разные промпты для thinker и talker или добавлять дополнительный контекст (например, через rag), не ухудшая качество. как уже упоминалось, в статье используется новый аудиоэнкодер: вместо дообучения whisper, авторы обучают свою encoder-decoder-модель с нуля. из интересного в плане архитектуры можно выделить более сильный downsampling factor: 8 вместо 4 (то есть применяется более сжатое представление в аудиомодальности, фреймы по 80 мс вместо 40 мс). для обучения под разные задачи использовали 20 млн часов аудио. из них 80% — задача asr на китайские и английские псевдолейблы, 10% — задача asr для других языков и ещё 10% — задача audio understanding. во время обучения используется window attention с разными размерами окна, чтобы модель могла одинаково хорошо работать и в офлайн-сценариях (с большим контекстом), и в стриминговом режиме (с коротким). после обучения декодер выбрасывается, а энкодер используется для кодирования аудио в самом qwen 3 omni. основное изменение для видеомодальности заключается в том, как видео подаётся на вход модели. теперь изображения и аудио чередуются не фиксированными двухсекундными блоками, как раньше, а динамически — в потоке, с гибким соотношением кадров, что делает мультимодальный стриминг более естественным. в следующей части поговорим о том, как в новой модели поменялась генерация аудио, как проходило предобучение и что авторы говорят о результатах. александр паланевич ❣ специально для speech info qwen3-omni technical report [1/2] сегодня начинаем разбирать техрепорт qwen 3 omni — самого нового мультимодального qwen. авторы заявляют, что модель достигает sota-результатов или близких к ним сразу на всех типах данных. качество не ухудшается ни в одном направлении по сравнению с немультимодальными моделями qwen. другими словами, qwen 3 omni показывает качество на тексте не хуже, чем текстовая версия qwen 3 или визуальная qwen 3-vl, при сопоставимых размерах моделей. из интересных нововведений: модель умеет обрабатывать очень длинные входы — до 40 минут. также она поддерживает большое количество языков: как для взаимодействий текстом (119), так и в задачах speech-understanding (19) или speech-generation (10). в статье отмечается, что улучшен ризонинг независимо от модальности входа, а latency остаётся низкой — всё работает достаточно быстро. идейно qwen 3 omni очень похож на qwen 2.5 omni: — используется thinker-talker-архитектура. thinker — языковая модель, которая умеет принимать на вход данные разных модальностей и выдавать текст. talker принимает выходы thinker и генерирует аудио. важное отличие от предыдущего qwen в том, что теперь thinker/talker — это moe-модели (mixture of experts). — разные модальности кодируются за счёт соответствующих энкодеров. в qwen 3 omni эти энкодеры обновили: для картинок вместо qwen 2.5 vl используется qwen 3 vl, а для аудио авторы обучили свой энкодер с нуля. одно из основных отличий новой модели от 2.5 omni заключается в том, как выходы thinker подаются в talker. для изображений и аудио по-прежнему используют хиддены thinker для соответствующих модальностей, а вот текст теперь передаётся в виде обычных текстовых эмбеддингов. по словам авторов, эмбеддинги уже достаточно хорошо отражают текст, а скрытые состояния избыточны. такой подход делает систему гибче: можно использовать разные промпты для thinker и talker или добавлять дополнительный контекст (например, через rag), не ухудшая качество. как уже упоминалось, в статье используется новый аудиоэнкодер: вместо дообучения whisper, авторы обучают свою encoder-decoder-модель с нуля. из интересного в плане архитектуры можно выделить более сильный downsampling factor: 8 вместо 4 (то есть применяется более сжатое представление в аудиомодальности, фреймы по 80 мс вместо 40 мс). для обучения под разные задачи использовали 20 млн часов аудио. из них 80% — задача asr на китайские и английские псевдолейблы, 10% — задача asr для других языков и ещё 10% — задача audio understanding. во время обучения используется window attention с разными размерами окна, чтобы модель могла одинаково хорошо работать и в офлайн-сценариях (с большим контекстом), и в стриминговом режиме (с коротким). после обучения декодер выбрасывается, а энкодер используется для кодирования аудио в самом qwen 3 omni. основное изменение для видеомодальности заключается в том, как видео подаётся на вход модели. теперь изображения и аудио чередуются не фиксированными двухсекундными блоками, как раньше, а динамически — в потоке, с гибким соотношением кадров, что делает мультимодальный стриминг более естественным. в следующей части поговорим о том, как в новой модели поменялась генерация аудио, как проходило предобучение и что авторы говорят о результатах. александр паланевич ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-31T12:18:01+00:00" href="./posts/93.html">2025-10-31 12:18 UTC</a></div>
      </div>
      <div class="post-body"><strong>Qwen3-Omni Technical Report [1/2]</strong><br><strong><br></strong>Сегодня начинаем разбирать <a href="https://arxiv.org/html/2509.17765v1" rel="nofollow noopener noreferrer">техрепорт</a> Qwen 3 Omni — самого нового мультимодального Qwen. Авторы заявляют, что модель достигает SOTA-результатов или близких к ним сразу на всех типах данных. Качество не ухудшается ни в одном направлении по сравнению с немультимодальными моделями Qwen. Другими словами, Qwen 3 Omni показывает качество на тексте не хуже, чем текстовая версия Qwen 3 или визуальная Qwen 3-VL, при сопоставимых размерах моделей.<br><br>Из интересных нововведений: модель умеет обрабатывать очень длинные входы — до 40 минут. Также она поддерживает большое количество языков: как для взаимодействий текстом (119), так и в задачах speech-understanding (19) или speech-generation (10). В статье отмечается, что улучшен ризонинг независимо от модальности входа, а latency остаётся низкой — всё работает достаточно быстро.<br><br>Идейно Qwen 3 Omni очень похож на Qwen 2.5 Omni:<br><br>— Используется Thinker-Talker-архитектура. Thinker — языковая модель, которая умеет принимать на вход данные разных модальностей и выдавать текст. Talker принимает выходы Thinker и генерирует аудио. Важное отличие от предыдущего Qwen в том, что теперь Thinker/Talker — это MoE-модели (Mixture of Experts).<br><br>— Разные модальности кодируются за счёт соответствующих энкодеров. В Qwen 3 Omni эти энкодеры обновили: для картинок вместо Qwen 2.5 VL используется Qwen 3 VL, а для аудио авторы обучили свой энкодер с нуля.<br><br>Одно из основных отличий новой модели от 2.5 Omni заключается в том, как выходы Thinker подаются в Talker. Для изображений и аудио по-прежнему используют хиддены Thinker для соответствующих модальностей, а вот текст теперь передаётся в виде обычных текстовых эмбеддингов. По словам авторов, эмбеддинги уже достаточно хорошо отражают текст, а скрытые состояния избыточны. Такой подход делает систему гибче: можно использовать разные промпты для Thinker и Talker или добавлять дополнительный контекст (например, через RAG), не ухудшая качество.<br><br>Как уже упоминалось, в статье используется новый аудиоэнкодер: вместо дообучения Whisper, авторы обучают свою encoder-decoder-модель с нуля. Из интересного в плане архитектуры можно выделить более сильный downsampling factor: 8 вместо 4 (то есть применяется более сжатое представление в аудиомодальности, фреймы по 80 мс вместо 40 мс).<br><br>Для обучения под разные задачи использовали 20 млн часов аудио. Из них 80% — задача ASR на китайские и английские псевдолейблы, 10% — задача ASR для других языков и ещё 10% — задача audio understanding. Во время обучения используется window attention с разными размерами окна, чтобы модель могла одинаково хорошо работать и в офлайн-сценариях (с большим контекстом), и в стриминговом режиме (с коротким). После обучения декодер выбрасывается, а энкодер используется для кодирования аудио в самом Qwen 3 Omni.<br><br>Основное изменение для видеомодальности заключается в том, как видео подаётся на вход модели. Теперь изображения и аудио чередуются не фиксированными двухсекундными блоками, как раньше, а динамически — в потоке, с гибким соотношением кадров, что делает мультимодальный стриминг более естественным.<br><br>В следующей части поговорим о том, как в новой модели поменялась генерация аудио, как проходило предобучение и что авторы говорят о результатах.<br><br><em>Александр Паланевич</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/93_480.webp" srcset="../assets/media/thumbs/93_480.webp 480w, ../assets/media/93.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="93" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 035 просмотров · 33 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/93" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/93.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="90" data-search="три идеи для улучшения asr: durep, owsm-biasing и pinyin-guided asr сегодня делимся подборкой трёх свежих работ по архитектурам и подходам в распознавании речи. все они так или иначе решают задачу повышения устойчивости и адаптивности моделей. durep: dual-mode speech representation learning via asr-aware distillation команда из amazon предложила пайплайн для обучения, который включает несколько стадий. сначала используется bestrq pretraining, затем проводится full-context fine-tuning — тут ничего нового. но дальше начинается интересное: авторы предлагают особый тип дистилляции, по сути дополнительный претрейн, после которого уже выполняется финальная настройка. под dual-mode здесь понимается не «аудио–текст», как может показаться, а режим, объединяющий стриминг и full-context. у исследователей есть full-context-энкодер, и они хотят получить dual-mode-энкодер, который можно использовать как претрейн. для этого применяют кодбук и токенизацию, обучаясь с кросс-энтропийной функцией потерь — почти как в обычном претрейне. ключевой нюанс — жонглирование масками. они рандомизируют как левый, так и правый контексты при обучении, что помогает улучшить качество кодирования. для стриминга это работает, потому что модель учится с учителем, имеющим полный контекст, и получает больше информации. для full-context улучшения можно объяснить тем, что дистилляция с варьирующимися масками предотвращает переобучение: датасет у авторов не слишком сложный, и такая регуляризация помогает повысить общую устойчивость модели. owsm-biasing: contextualizing open whisper-style speech models for automatic speech recognition with dynamic vocabulary основная идея статьи в том, чтобы к seq2seq-модели добавить biasing list, который позволяет учитывать редкие или специфические слова (например, имена собственные). редкие слова обычно разбиваются на несколько bpe-токенов, что мешает корректному распознаванию. чтобы избежать этого, каждое слово из biasing list представляется как единый токен — элемент динамического словаря (&lt;raphael&gt;, &lt;nelly&gt; и т.д.). выходная матрица логитов расширяется за счёт этих слов, что позволяет модели напрямую предсказывать редкие слова как единые токены. обучаются при этом только эмбеддинги и, по сути, деэмбеддинги. нюанс в том, что при генерации декодеру приходится выдавать такие токены, опираясь только на сигнал от biasing encoder. это необычно, но при хорошем обучении, вероятно, сработает. во время тренировки в biasing list случайно добавляют слова из обучающего набора, чтобы модель училась работать с разнообразными словами. pinyin-guided chinese speech recognition with large language model пиньинь — это упрощённая запись китайских иероглифов латинскими буквами (иногда с цифрами, обозначающими тоны). по сути, авторы обучают asr-модель на базе llm. китайские иероглифы могут произноситься по-разному, и здесь возникает дисбаланс: прозвучало одно, записано другое — модель может путаться. поэтому авторы предлагают ввести промежуточное состояние: сначала модель выдаёт pinyin-токены, которые напрямую отражают произнесённое, а уже потом конвертирует их в целевые токены — сами китайские иероглифы. концептуально интересно, что модель фактически делает нечто вроде ризонинга: не выдаёт результат сразу, а проходит через дополнительный слой осмысления. идею можно применять и в других задачах: например, в переводе — сначала генерировать промежуточные слова на исходном языке, затем переводить; в музыке — восстанавливать произнесённые названия треков через промежуточное фонетическое представление; аналогично и с англицизмами — сначала фиксировать звучание, потом корректировать форму. евгений ганкович ❣ специально для speech info три идеи для улучшения asr: durep, owsm-biasing и pinyin-guided asr сегодня делимся подборкой трёх свежих работ по архитектурам и подходам в распознавании речи. все они так или иначе решают задачу повышения устойчивости и адаптивности моделей. durep: dual-mode speech representation learning via asr-aware distillation команда из amazon предложила пайплайн для обучения, который включает несколько стадий. сначала используется bestrq pretraining, затем проводится full-context fine-tuning — тут ничего нового. но дальше начинается интересное: авторы предлагают особый тип дистилляции, по сути дополнительный претрейн, после которого уже выполняется финальная настройка. под dual-mode здесь понимается не «аудио–текст», как может показаться, а режим, объединяющий стриминг и full-context. у исследователей есть full-context-энкодер, и они хотят получить dual-mode-энкодер, который можно использовать как претрейн. для этого применяют кодбук и токенизацию, обучаясь с кросс-энтропийной функцией потерь — почти как в обычном претрейне. ключевой нюанс — жонглирование масками. они рандомизируют как левый, так и правый контексты при обучении, что помогает улучшить качество кодирования. для стриминга это работает, потому что модель учится с учителем, имеющим полный контекст, и получает больше информации. для full-context улучшения можно объяснить тем, что дистилляция с варьирующимися масками предотвращает переобучение: датасет у авторов не слишком сложный, и такая регуляризация помогает повысить общую устойчивость модели. owsm-biasing: contextualizing open whisper-style speech models for automatic speech recognition with dynamic vocabulary основная идея статьи в том, чтобы к seq2seq-модели добавить biasing list, который позволяет учитывать редкие или специфические слова (например, имена собственные). редкие слова обычно разбиваются на несколько bpe-токенов, что мешает корректному распознаванию. чтобы избежать этого, каждое слово из biasing list представляется как единый токен — элемент динамического словаря (&amp;lt;raphael&amp;gt;, &amp;lt;nelly&amp;gt; и т.д.). выходная матрица логитов расширяется за счёт этих слов, что позволяет модели напрямую предсказывать редкие слова как единые токены. обучаются при этом только эмбеддинги и, по сути, деэмбеддинги. нюанс в том, что при генерации декодеру приходится выдавать такие токены, опираясь только на сигнал от biasing encoder. это необычно, но при хорошем обучении, вероятно, сработает. во время тренировки в biasing list случайно добавляют слова из обучающего набора, чтобы модель училась работать с разнообразными словами. pinyin-guided chinese speech recognition with large language model пиньинь — это упрощённая запись китайских иероглифов латинскими буквами (иногда с цифрами, обозначающими тоны). по сути, авторы обучают asr-модель на базе llm. китайские иероглифы могут произноситься по-разному, и здесь возникает дисбаланс: прозвучало одно, записано другое — модель может путаться. поэтому авторы предлагают ввести промежуточное состояние: сначала модель выдаёт pinyin-токены, которые напрямую отражают произнесённое, а уже потом конвертирует их в целевые токены — сами китайские иероглифы. концептуально интересно, что модель фактически делает нечто вроде ризонинга: не выдаёт результат сразу, а проходит через дополнительный слой осмысления. идею можно применять и в других задачах: например, в переводе — сначала генерировать промежуточные слова на исходном языке, затем переводить; в музыке — восстанавливать произнесённые названия треков через промежуточное фонетическое представление; аналогично и с англицизмами — сначала фиксировать звучание, потом корректировать форму. евгений ганкович ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-20T12:32:48+00:00" href="./posts/90.html">2025-10-20 12:32 UTC</a></div>
      </div>
      <div class="post-body"><strong>Три идеи для улучшения ASR: DuRep, OWSM-Biasing и Pinyin-Guided ASR</strong><br><br>Сегодня делимся подборкой трёх свежих работ по архитектурам и подходам в распознавании речи. Все они так или иначе решают задачу повышения устойчивости и адаптивности моделей.<br><br><a href="https://arxiv.org/pdf/2505.19774" rel="nofollow noopener noreferrer"><strong>DuRep: Dual-Mode Speech Representation Learning via ASR-Aware Distillation<br></strong></a><br>Команда из Amazon предложила пайплайн для обучения, который включает несколько стадий. Сначала используется BestRQ pretraining, затем проводится full-context fine-tuning — тут ничего нового. Но дальше начинается интересное: авторы предлагают особый тип дистилляции, по сути дополнительный претрейн, после которого уже выполняется финальная настройка.<br><br>Под dual-mode здесь понимается не «аудио–текст», как может показаться, а режим, объединяющий стриминг и full-context. У исследователей есть full-context-энкодер, и они хотят получить dual-mode-энкодер, который можно использовать как претрейн. Для этого применяют кодбук и токенизацию, обучаясь с кросс-энтропийной функцией потерь — почти как в обычном претрейне.<br><br>Ключевой нюанс — жонглирование масками. Они рандомизируют как левый, так и правый контексты при обучении, что помогает улучшить качество кодирования.<br><br>Для стриминга это работает, потому что модель учится с учителем, имеющим полный контекст, и получает больше информации. Для full-context улучшения можно объяснить тем, что дистилляция с варьирующимися масками предотвращает переобучение: датасет у авторов не слишком сложный, и такая регуляризация помогает повысить общую устойчивость модели.<br><br><a href="https://arxiv.org/pdf/2506.09448" rel="nofollow noopener noreferrer"><strong>OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary</strong></a><br><br>Основная идея статьи в том, чтобы к seq2seq-модели добавить biasing list, который позволяет учитывать редкие или специфические слова (например, имена собственные). Редкие слова обычно разбиваются на несколько BPE-токенов, что мешает корректному распознаванию. Чтобы избежать этого, каждое слово из biasing list представляется как единый токен — элемент динамического словаря (&lt;Raphael&gt;, &lt;Nelly&gt; и т.д.).<br><br>Выходная матрица логитов расширяется за счёт этих слов, что позволяет модели напрямую предсказывать редкие слова как единые токены. Обучаются при этом только эмбеддинги и, по сути, деэмбеддинги.<br><br>Нюанс в том, что при генерации декодеру приходится выдавать такие токены, опираясь только на сигнал от biasing encoder. Это необычно, но при хорошем обучении, вероятно, сработает.<br><br>Во время тренировки в biasing list случайно добавляют слова из обучающего набора, чтобы модель училась работать с разнообразными словами.<br><br><a href="https://www.isca-archive.org/interspeech_2025/zhengjie25_interspeech.pdf" rel="nofollow noopener noreferrer"><strong>Pinyin-Guided Chinese Speech Recognition with Large Language Model</strong></a><br><br>Пиньинь — это упрощённая запись китайских иероглифов латинскими буквами (иногда с цифрами, обозначающими тоны). По сути, авторы обучают ASR-модель на базе LLM. <br><br>Китайские иероглифы могут произноситься по-разному, и здесь возникает дисбаланс: прозвучало одно, записано другое — модель может путаться. Поэтому авторы предлагают ввести промежуточное состояние: сначала модель выдаёт pinyin-токены, которые напрямую отражают произнесённое, а уже потом конвертирует их в целевые токены — сами китайские иероглифы.<br><br>Концептуально интересно, что модель фактически делает нечто вроде ризонинга: не выдаёт результат сразу, а проходит через дополнительный слой осмысления.<br><br>Идею можно применять и в других задачах: например, в переводе — сначала генерировать промежуточные слова на исходном языке, затем переводить; в музыке — восстанавливать произнесённые названия треков через промежуточное фонетическое представление; аналогично и с англицизмами — сначала фиксировать звучание, потом корректировать форму.<br><br><em>Евгений Ганкович </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/90_480.webp" srcset="../assets/media/thumbs/90_480.webp 480w, ../assets/media/90.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="90" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/91_480.webp" srcset="../assets/media/thumbs/91_480.webp 480w, ../assets/media/91.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="90" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/92_480.webp" srcset="../assets/media/thumbs/92_480.webp 480w, ../assets/media/92.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="90" data-image-index="2" /></div></div>
      <div class="actions">
        <span>949 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/90" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/90.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="89" data-search="работы об аудиокодеках и новых подходах к сжатию речи большинство статей на конференции interspeech традиционно представлены академией. в силу ограниченности ресурсов в них нет результатов обучения на действительно больших датасетах или надёжных асессорских замеров. поэтому их можно рассматривать скорее в качестве источника идей, чем как решения для продакшна. сегодня разберём несколько таких работ. lscodec: low-bitrate and speaker-decoupled discrete speech codec авторы исходят из того, что кодирование в последовательности токенов глобальной, не зависящей от времени информации приводит к её дублированию для каждого таймстемпа и лишней трате capacity. оптимальнее кодировать только то, что меняется со временем, а остальное передавать отдельно — в виде фиксированного вектора. в качестве глобальной информации в работе используют тембр голоса спикера. обучающий сэмпл включает два аудио: таргет и промпт от того же спикера. перед подачей в энкодер тембр таргета искусственно искажается, а декодеру дополнительно передаются ssl-фичи промпта через position-agnostic cross-attention. модель учится предсказывать мел-спектрограмму и ssl-семантические токены исходного таргета (до искажения). в результате выход энкодера не содержит информации о тембре таргета, и декодер учится извлекать её из промпта (а благодаря боттлнеку эта информация не зашивается в токены энкодера). при этом position-agnostic attention предотвращает утечку из промпта остальной, зависящей от времени, информации. статья интересна идейно, но использование в качестве глобальной информации только тембра кажется слишком ограничивающим. в списке ссылок приведена работа с icassp 2024 с аналогичной мотивацией, но более общим подходом. fewer-token neural speech codec with time-invariant codes архитектура учится end-to-end и состоит из нескольких частей: энкодер и квантайзер для токенов переменной длины; энкодер (с average pooling на последнем слое), квантайзер для фиксированного глобального вектора и совместный декодер. чтобы закодировать в глобальном векторе именно не зависящую от времени информацию, добавляется дополнительная компонента лосса. вычисляется глобальный вектор для другого аудио того же спикера и минимизируется косинусное расстояние между ним (с навешенным stop-gradient) и глобальным вектором таргета. towards bitrate-efficient and noise-robust speech coding with variable bitrate rvq статья содержит две основные идеи. мотивация первой: в токенах можно не кодировать информацию о шуме, тем самым объединив задачи кодирования и enhancement и дополнительно сэкономив capacity. модель учится в две стадии. на первой кодек просто обучается на чистых данных. на второй — его учат удалять из токенов данные о шуме, то есть получать одинаковые токены для чистого и шумного аудио. для этого в энкодер добавляют новый denoising-слой, а во время обучения искусственно зашумляют каждое аудио и добавляют к лоссу дополнительную компоненту: l1-расстояние между входом denoising-слоя для чистого аудио и выходом для шумного. вторая идея — адаптивный bitrate в зависимости от количества информации, содержащейся в каждом фрейме аудио. например, фреймы с голосом можно кодировать более детально, чем фреймы с тишиной. для этого на выходах энкодера обучается предиктор, возвращающий количество первых rvq-токенов, которые необходимо просуммировать для данного фрейма. к лоссу добавляется дополнительная компонента — суммарное число предсказанных токенов. дарья петренко ❣ специально для speech info работы об аудиокодеках и новых подходах к сжатию речи большинство статей на конференции interspeech традиционно представлены академией. в силу ограниченности ресурсов в них нет результатов обучения на действительно больших датасетах или надёжных асессорских замеров. поэтому их можно рассматривать скорее в качестве источника идей, чем как решения для продакшна. сегодня разберём несколько таких работ. lscodec: low-bitrate and speaker-decoupled discrete speech codec авторы исходят из того, что кодирование в последовательности токенов глобальной, не зависящей от времени информации приводит к её дублированию для каждого таймстемпа и лишней трате capacity. оптимальнее кодировать только то, что меняется со временем, а остальное передавать отдельно — в виде фиксированного вектора. в качестве глобальной информации в работе используют тембр голоса спикера. обучающий сэмпл включает два аудио: таргет и промпт от того же спикера. перед подачей в энкодер тембр таргета искусственно искажается, а декодеру дополнительно передаются ssl-фичи промпта через position-agnostic cross-attention. модель учится предсказывать мел-спектрограмму и ssl-семантические токены исходного таргета (до искажения). в результате выход энкодера не содержит информации о тембре таргета, и декодер учится извлекать её из промпта (а благодаря боттлнеку эта информация не зашивается в токены энкодера). при этом position-agnostic attention предотвращает утечку из промпта остальной, зависящей от времени, информации. статья интересна идейно, но использование в качестве глобальной информации только тембра кажется слишком ограничивающим. в списке ссылок приведена работа с icassp 2024 с аналогичной мотивацией, но более общим подходом. fewer-token neural speech codec with time-invariant codes архитектура учится end-to-end и состоит из нескольких частей: энкодер и квантайзер для токенов переменной длины; энкодер (с average pooling на последнем слое), квантайзер для фиксированного глобального вектора и совместный декодер. чтобы закодировать в глобальном векторе именно не зависящую от времени информацию, добавляется дополнительная компонента лосса. вычисляется глобальный вектор для другого аудио того же спикера и минимизируется косинусное расстояние между ним (с навешенным stop-gradient) и глобальным вектором таргета. towards bitrate-efficient and noise-robust speech coding with variable bitrate rvq статья содержит две основные идеи. мотивация первой: в токенах можно не кодировать информацию о шуме, тем самым объединив задачи кодирования и enhancement и дополнительно сэкономив capacity. модель учится в две стадии. на первой кодек просто обучается на чистых данных. на второй — его учат удалять из токенов данные о шуме, то есть получать одинаковые токены для чистого и шумного аудио. для этого в энкодер добавляют новый denoising-слой, а во время обучения искусственно зашумляют каждое аудио и добавляют к лоссу дополнительную компоненту: l1-расстояние между входом denoising-слоя для чистого аудио и выходом для шумного. вторая идея — адаптивный bitrate в зависимости от количества информации, содержащейся в каждом фрейме аудио. например, фреймы с голосом можно кодировать более детально, чем фреймы с тишиной. для этого на выходах энкодера обучается предиктор, возвращающий количество первых rvq-токенов, которые необходимо просуммировать для данного фрейма. к лоссу добавляется дополнительная компонента — суммарное число предсказанных токенов. дарья петренко ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-10-13T10:05:17+00:00" href="./posts/89.html">2025-10-13 10:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Работы об аудиокодеках и новых подходах к сжатию речи</strong><br><strong><br></strong>Большинство статей на конференции Interspeech традиционно представлены академией. В силу ограниченности ресурсов в них нет результатов обучения на действительно больших датасетах или надёжных асессорских замеров. Поэтому их можно рассматривать скорее в качестве источника идей, чем как решения для продакшна. Сегодня разберём несколько таких работ.<br><br><a href="https://arxiv.org/abs/2410.15764" rel="nofollow noopener noreferrer"><strong>LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec<br></strong></a><br>Авторы исходят из того, что кодирование в последовательности токенов глобальной, не зависящей от времени информации приводит к её дублированию для каждого таймстемпа и лишней трате capacity. Оптимальнее кодировать только то, что меняется со временем, а остальное передавать отдельно — в виде фиксированного вектора.<br><br>В качестве глобальной информации в работе используют тембр голоса спикера. Обучающий сэмпл включает два аудио: таргет и промпт от того же спикера. Перед подачей в энкодер тембр таргета искусственно искажается, а декодеру дополнительно передаются SSL-фичи промпта через position-agnostic cross-attention. Модель учится предсказывать мел-спектрограмму и SSL-семантические токены исходного таргета (до искажения).<br><br>В результате выход энкодера не содержит информации о тембре таргета, и декодер учится извлекать её из промпта (а благодаря боттлнеку эта информация не зашивается в токены энкодера). При этом position-agnostic attention предотвращает утечку из промпта остальной, зависящей от времени, информации.<br><br>Статья интересна идейно, но использование в качестве глобальной информации только тембра кажется слишком ограничивающим. В списке ссылок приведена работа с ICASSP 2024 с аналогичной мотивацией, но более общим подходом.<br><br><a href="https://arxiv.org/abs/2310.00014" rel="nofollow noopener noreferrer"><strong>Fewer-token Neural Speech Codec with Time-invariant Codes<br></strong></a><br>Архитектура учится end-to-end и состоит из нескольких частей: энкодер и квантайзер для токенов переменной длины; энкодер (с average pooling на последнем слое), квантайзер для фиксированного глобального вектора и совместный декодер.<br><br>Чтобы закодировать в глобальном векторе именно не зависящую от времени информацию, добавляется дополнительная компонента лосса. Вычисляется глобальный вектор для другого аудио того же спикера и минимизируется косинусное расстояние между ним (с навешенным stop-gradient) и глобальным вектором таргета. <br><br><a href="https://arxiv.org/abs/2506.16538" rel="nofollow noopener noreferrer"><strong>Towards Bitrate-Efficient and Noise-Robust Speech Coding with Variable Bitrate RVQ<br></strong></a><br>Статья содержит две основные идеи.<br><br>Мотивация первой: в токенах можно не кодировать информацию о шуме, тем самым объединив задачи кодирования и enhancement и дополнительно сэкономив capacity.<br><br>Модель учится в две стадии. На первой кодек просто обучается на чистых данных. На второй — его учат удалять из токенов данные о шуме, то есть получать одинаковые токены для чистого и шумного аудио. Для этого в энкодер добавляют новый denoising-слой, а во время обучения искусственно зашумляют каждое аудио и добавляют к лоссу дополнительную компоненту: L1-расстояние между входом denoising-слоя для чистого аудио и выходом для шумного.<br><br>Вторая идея — адаптивный bitrate в зависимости от количества информации, содержащейся в каждом фрейме аудио. Например, фреймы с голосом можно кодировать более детально, чем фреймы с тишиной. Для этого на выходах энкодера обучается предиктор, возвращающий количество первых RVQ-токенов, которые необходимо просуммировать для данного фрейма. К лоссу добавляется дополнительная компонента — суммарное число предсказанных токенов.<br><br><em>Дарья Петренко </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a></div>
      <div class="actions">
        <span>840 просмотров · 30 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/89" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/89.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="87" data-search="smartmos: modeling subjective audio quality evaluation for real-time applications сегодня разбираем статью от meta* о решении smartmos, применяемом в звонках (whatsapp и др.) для оценки качества звука после шумоподавления и других алгоритмов обработки. в продакшене важно не только понимать общий показатель качества, но и иметь возможность разложить его на составляющие: насколько хорошо слышна речь, насколько повлияли потери пакетов, сколько шума осталось и какую аудиозапись в итоге получает собеседник на свой девайс. именно такую детализированную оценку даёт smartmos. для этой задачи использована небольшая стриминговая нейросеть. она работает прямо на устройстве и предсказывает скоры для двух задач по 10-секундному сегменту аудио: noise suppression (ns) и packet loss concealment (plc). внутри noise suppression есть разделение по аспектам: speech mos, noise mos и overall mos. интересно, что архитектура энкодера совпадает с используемой в оффлайн-asr в умных очках meta. обучение делается на сегментах длиной около 10 секунд. логика в том, что на длинных кусках качество можно оценить надёжнее, поскольку короткие отрезки в середине разговора могут давать искажённые результаты. данные для обучения собираются из тестов реальных приложений — в релизном процессе есть тестировщики, которые записывают аудио по сценариям, эти записи логируются и размечаются людьми. чтобы компенсировать нехватку данных, авторы использовали не только человеческую разметку (mos-оценки), но и часть выборки с semi-supervised-метками. чтобы модель была достаточно лёгкой для запуска на любых устройствах, применяются оптимизации: — используется vad, тишина дропается, чтобы не тратить ресурсы; — сегменты фиксированы по 10 секунд; — энкодер принимает куски по 100 мс, обрабатывая их в стримминговом режиме; — декодер (предиктор) аккумулирует все выходы энкодера и выдаёт одну оценку на весь сегмент. такой подход позволяет существенно снизить нагрузку на cpu: пиковая нагрузка распределяется более равномерно по времени. meta уже использует это решение в продакшене на всех типах звонков. подобные решения будут полезны всем, кто делает продукты для звонков. в реальности у нас почти никогда нет простого способа измерить, насколько хорошо работает шумоподавление. модель вроде smartmos могла бы закрыть этот пробел и дать мониторинг качества прямо в проде. борис шелудько ❣ специально для speech info * компания meta, владеющая whatsapp, признана экстремистской; её деятельность в россии запрещена. smartmos: modeling subjective audio quality evaluation for real-time applications сегодня разбираем статью от meta* о решении smartmos, применяемом в звонках (whatsapp и др.) для оценки качества звука после шумоподавления и других алгоритмов обработки. в продакшене важно не только понимать общий показатель качества, но и иметь возможность разложить его на составляющие: насколько хорошо слышна речь, насколько повлияли потери пакетов, сколько шума осталось и какую аудиозапись в итоге получает собеседник на свой девайс. именно такую детализированную оценку даёт smartmos. для этой задачи использована небольшая стриминговая нейросеть. она работает прямо на устройстве и предсказывает скоры для двух задач по 10-секундному сегменту аудио: noise suppression (ns) и packet loss concealment (plc). внутри noise suppression есть разделение по аспектам: speech mos, noise mos и overall mos. интересно, что архитектура энкодера совпадает с используемой в оффлайн-asr в умных очках meta. обучение делается на сегментах длиной около 10 секунд. логика в том, что на длинных кусках качество можно оценить надёжнее, поскольку короткие отрезки в середине разговора могут давать искажённые результаты. данные для обучения собираются из тестов реальных приложений — в релизном процессе есть тестировщики, которые записывают аудио по сценариям, эти записи логируются и размечаются людьми. чтобы компенсировать нехватку данных, авторы использовали не только человеческую разметку (mos-оценки), но и часть выборки с semi-supervised-метками. чтобы модель была достаточно лёгкой для запуска на любых устройствах, применяются оптимизации: — используется vad, тишина дропается, чтобы не тратить ресурсы; — сегменты фиксированы по 10 секунд; — энкодер принимает куски по 100 мс, обрабатывая их в стримминговом режиме; — декодер (предиктор) аккумулирует все выходы энкодера и выдаёт одну оценку на весь сегмент. такой подход позволяет существенно снизить нагрузку на cpu: пиковая нагрузка распределяется более равномерно по времени. meta уже использует это решение в продакшене на всех типах звонков. подобные решения будут полезны всем, кто делает продукты для звонков. в реальности у нас почти никогда нет простого способа измерить, насколько хорошо работает шумоподавление. модель вроде smartmos могла бы закрыть этот пробел и дать мониторинг качества прямо в проде. борис шелудько ❣ специально для speech info * компания meta, владеющая whatsapp, признана экстремистской; её деятельность в россии запрещена.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-30T12:01:06+00:00" href="./posts/87.html">2025-09-30 12:01 UTC</a></div>
      </div>
      <div class="post-body"><strong>SMARTMOS: Modeling Subjective Audio Quality Evaluation for Real-Time Applications<br></strong><br>Сегодня разбираем <a href="https://www.isca-archive.org/interspeech_2025/balasubramanian25_interspeech.pdf" rel="nofollow noopener noreferrer">статью</a> от Meta* о решении SMARTMOS, применяемом в звонках (WhatsApp и др.) для оценки качества звука после шумоподавления и других алгоритмов обработки. В продакшене важно не только понимать общий показатель качества, но и иметь возможность разложить его на составляющие: насколько хорошо слышна речь, насколько повлияли потери пакетов, сколько шума осталось и какую аудиозапись в итоге получает собеседник на свой девайс. Именно такую детализированную оценку даёт SMARTMOS.<br><br>Для этой задачи использована небольшая стриминговая нейросеть. Она работает прямо на устройстве и предсказывает скоры для двух задач по 10-секундному сегменту аудио: Noise Suppression (NS) и Packet Loss Concealment (PLC). Внутри Noise Suppression есть разделение по аспектам: Speech MOS, Noise MOS и Overall MOS. Интересно, что архитектура энкодера совпадает с используемой в оффлайн-ASR в умных очках Meta.<br><br>Обучение делается на сегментах длиной около 10 секунд. Логика в том, что на длинных кусках качество можно оценить надёжнее, поскольку короткие отрезки в середине разговора могут давать искажённые результаты.<br><br>Данные для обучения собираются из тестов реальных приложений — в релизном процессе есть тестировщики, которые записывают аудио по сценариям, эти записи логируются и размечаются людьми. Чтобы компенсировать нехватку данных, авторы использовали не только человеческую разметку (MOS-оценки), но и часть выборки с semi-supervised-метками.<br><br>Чтобы модель была достаточно лёгкой для запуска на любых устройствах, применяются оптимизации:<br><br>— используется VAD, тишина дропается, чтобы не тратить ресурсы;<br>— сегменты фиксированы по 10 секунд;<br>— энкодер принимает куски по 100 мс, обрабатывая их в стримминговом режиме;<br>— декодер (предиктор) аккумулирует все выходы энкодера и выдаёт одну оценку на весь сегмент.<br><br>Такой подход позволяет существенно снизить нагрузку на CPU: пиковая нагрузка распределяется более равномерно по времени.<br><br>Meta уже использует это решение в продакшене на всех типах звонков. Подобные решения будут полезны всем, кто делает продукты для звонков. В реальности у нас почти никогда нет простого способа измерить, насколько хорошо работает шумоподавление. Модель вроде SMARTMOS могла бы закрыть этот пробел и дать мониторинг качества прямо в проде.<br><br><em>Борис Шелудько </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><em><br></em><br><em>* Компания Meta, владеющая WhatsApp, признана экстремистской; её деятельность в России запрещена.</em><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/87_480.webp" srcset="../assets/media/thumbs/87_480.webp 480w, ../assets/media/87.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="87" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/88_480.webp" srcset="../assets/media/thumbs/88_480.webp 480w, ../assets/media/88.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="87" data-image-index="1" /></div></div>
      <div class="actions">
        <span>1 068 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/87" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/87.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="78" data-search="interspeech 2025: впечатления инженеров яндекса мы по традиции попросили инженеров яндекса подвести личные итоги конференции — на этот раз interspeech 2025 — и рассказать, чем она запомнилась. в карточках собрали заметки, впечатления и самые интересные работы. статьи, которые упоминаются в посте: — low-bitrate and speaker-decoupled discrete speech codec; — fine-tuning text-to-speech diffusion models using reinforcement learning with human feedback; — в статье improving noise robustness of llm-based zero-shot tts via discrete acoustic token denoising; — adaptive knowledge distillation for device-directed speech detection. speech info * компания meta признана экстремистской; её деятельность в россии запрещена. interspeech 2025: впечатления инженеров яндекса мы по традиции попросили инженеров яндекса подвести личные итоги конференции — на этот раз interspeech 2025 — и рассказать, чем она запомнилась. в карточках собрали заметки, впечатления и самые интересные работы. статьи, которые упоминаются в посте: — low-bitrate and speaker-decoupled discrete speech codec ; — fine-tuning text-to-speech diffusion models using reinforcement learning with human feedback ; — в статье improving noise robustness of llm-based zero-shot tts via discrete acoustic token denoising ; — adaptive knowledge distillation for device-directed speech detection . speech info * компания meta признана экстремистской; её деятельность в россии запрещена.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-25T16:50:51+00:00" href="./posts/78.html">2025-09-25 16:50 UTC</a></div>
      </div>
      <div class="post-body"><strong>Interspeech 2025: впечатления инженеров Яндекса<br><br></strong>Мы по традиции попросили инженеров Яндекса подвести личные итоги конференции — на этот раз Interspeech 2025 — и рассказать, чем она запомнилась. В карточках собрали заметки, впечатления и самые интересные работы.<br><br>Статьи, которые упоминаются в посте:<br>— <a href="https://arxiv.org/abs/2410.15764" rel="nofollow noopener noreferrer">Low-Bitrate and Speaker-Decoupled Discrete Speech Codec</a>;<br>— <a href="https://arxiv.org/pdf/2508.03123v1" rel="nofollow noopener noreferrer">Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback</a>;<a href="https://arxiv.org/pdf/2508.03123v1" rel="nofollow noopener noreferrer"><br></a>— <a href="https://www.isca-archive.org/interspeech_2025/lu25b_interspeech.pdf" rel="nofollow noopener noreferrer">В статье Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising</a>;<br>— <a href="https://arxiv.org/html/2508.02801v1" rel="nofollow noopener noreferrer">Adaptive Knowledge Distillation for Device-Directed Speech Detection</a>.<br><br><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><br><br><em>* Компания Meta признана экстремистской; её деятельность в России запрещена.</em><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/78_480.webp" srcset="../assets/media/thumbs/78_480.webp 480w, ../assets/media/78.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/79_480.webp" srcset="../assets/media/thumbs/79_480.webp 480w, ../assets/media/79.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/80_480.webp" srcset="../assets/media/thumbs/80_480.webp 480w, ../assets/media/80.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/81_480.webp" srcset="../assets/media/thumbs/81_480.webp 480w, ../assets/media/81.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/82_480.webp" srcset="../assets/media/thumbs/82_480.webp 480w, ../assets/media/82.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/83_480.webp" srcset="../assets/media/thumbs/83_480.webp 480w, ../assets/media/83.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/84_480.webp" srcset="../assets/media/thumbs/84_480.webp 480w, ../assets/media/84.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/85_480.webp" srcset="../assets/media/thumbs/85_480.webp 480w, ../assets/media/85.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="7" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/86_480.webp" srcset="../assets/media/thumbs/86_480.webp 480w, ../assets/media/86.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="8" /></div></div>
      <div class="actions">
        <span>820 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/78" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/78.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="77" data-search="интересные статьи на speech synthesis workshop 2/2 разберём ещё две любопытные работы с speech synthesis workshop. одна посвящена управлению стилем на уровне слов, другая — синтезу речи с невербальными характеристиками. lina-style: word-level style control in tts via interleaved synthetic data авторы предложили, как из небольшой выборки с разметкой стиля и большого неразмеченного корпуса построить полностью синтетический датасет с локальными (на уровне слова) метками стиля и его интенсивностью, а затем дообучить модель, чтобы она кондишенилась на метки. для этого они использовали свою предыдущую работу, модель lina-speech. архитектурно это текстовый энкодер и аудиодекодер с gated linear attention (gla). gla, кстати, позволяет легко использовать prefix free prompting через initial state-tuning. этим и воспользовались авторы. сначала они взяли претрейн lina-speech на неэмоциональной речи. дотюнили его через initial state-tuning на несколько стилей (neutral, happy, confused, enunciated). затем синтезировали несколько вариантов одной и той же реплики в разных стилях. во время синтеза также использовали classifier‑free guidance (cfg), случайно сэмплировали альфа, поэтому насинтезированные аудио получились в разных стилях и с разной их интенсивностью. для каждого аудио построили соответствие текста аудиотокенам. для этого извлекли матрицы soft-алайнмента текста и аудио и превратили их в матрицы hard-алайнмента с помощью monotonic alignment search (mas). таким образом получили соответствие токенов аудио отдельным словам. склеили слова из разных стилей в одно предложение и получили синтетический интерливинг-датасет с word-level-разметкой на стиль. осталось затюнить итоговую модель. на этом этапе дообучили базовый претрейн, добавив новые параметры: эмбеддинги стилей, интенсивностей и linear для их комбинации. почему это круто потому что это — пример сбора синтетического датасета с локальными метками стиля с нуля. в изначальном датасете сэмплов с word-level-разметкой не было. ну и успешное обучение на синте подтвердило, что метод рабочий. позалипать на сэмплы можно тут. nonverbaltts: a public english corpus of text-aligned nonverbal vocalizations with emotion annotations for text-to-speech янднекс тоже привёз свою статью, написанную совместно с коллегами из vk lab. в ней предложили датасет для синтеза речи с невербальными характеристиками на английском языке и рассказали о пайплайне его сбора. невербальные характеристики — это смех, вздох, кашель и другие звуки, которые мы издаём в речи и которые не являются словами. в реальной жизни таких невербальных характеристик много, но разметки для них часто нет. авторы взяли два опенсорсных датасета — expresso и voxceleb — и сначала с помощью опенсорсных моделей получили грубую разметку по невербальным характеристикам и эмоциям. затем уточнили результаты с помощью ручной разметки и отфильтровалы шумные сэмплы (например, аудио со смехом, который оказался закадровым). после этого зафьюзили варианты правильных транскрипций от нескольких разметчиков и получили итоговый датасет: 13 часов аудио с 10 типами невербальных характеристик. затюнили на своём датасете cosyvocie и сравнились с cosyvoice2, который обучался на проприетарном датасете, нестатзначимо проиграли в sbs. в статье раскрыли детали пайплайна разметки, а датасет выложили на hugging face. там немного, но это честная работа. почему это круто синтез с невербальными характеристиками нужен для синтеза спонтанного и разговорного стилей речи. nvtts может быть использован для файнтьюна, а также как стартовая точка для скейла и unsupervised-разметки датасета большего размера. дарья дятлова ❣ специально для speech info интересные статьи на speech synthesis workshop 2/2 разберём ещё две любопытные работы с speech synthesis workshop. одна посвящена управлению стилем на уровне слов, другая — синтезу речи с невербальными характеристиками. lina-style: word-level style control in tts via interleaved synthetic data авторы предложили, как из небольшой выборки с разметкой стиля и большого неразмеченного корпуса построить полностью синтетический датасет с локальными (на уровне слова) метками стиля и его интенсивностью, а затем дообучить модель, чтобы она кондишенилась на метки. для этого они использовали свою предыдущую работу, модель lina-speech. архитектурно это текстовый энкодер и аудиодекодер с gated linear attention (gla). gla, кстати, позволяет легко использовать prefix free prompting через initial state-tuning. этим и воспользовались авторы. сначала они взяли претрейн lina-speech на неэмоциональной речи. дотюнили его через initial state-tuning на несколько стилей (neutral, happy, confused, enunciated). затем синтезировали несколько вариантов одной и той же реплики в разных стилях. во время синтеза также использовали classifier‑free guidance (cfg), случайно сэмплировали альфа, поэтому насинтезированные аудио получились в разных стилях и с разной их интенсивностью. для каждого аудио построили соответствие текста аудиотокенам. для этого извлекли матрицы soft-алайнмента текста и аудио и превратили их в матрицы hard-алайнмента с помощью monotonic alignment search (mas). таким образом получили соответствие токенов аудио отдельным словам. склеили слова из разных стилей в одно предложение и получили синтетический интерливинг-датасет с word-level-разметкой на стиль. осталось затюнить итоговую модель. на этом этапе дообучили базовый претрейн, добавив новые параметры: эмбеддинги стилей, интенсивностей и linear для их комбинации. почему это круто потому что это — пример сбора синтетического датасета с локальными метками стиля с нуля. в изначальном датасете сэмплов с word-level-разметкой не было. ну и успешное обучение на синте подтвердило, что метод рабочий. позалипать на сэмплы можно тут . nonverbaltts: a public english corpus of text-aligned nonverbal vocalizations with emotion annotations for text-to-speech янднекс тоже привёз свою статью, написанную совместно с коллегами из vk lab. в ней предложили датасет для синтеза речи с невербальными характеристиками на английском языке и рассказали о пайплайне его сбора. невербальные характеристики — это смех, вздох, кашель и другие звуки, которые мы издаём в речи и которые не являются словами. в реальной жизни таких невербальных характеристик много, но разметки для них часто нет. авторы взяли два опенсорсных датасета — expresso и voxceleb — и сначала с помощью опенсорсных моделей получили грубую разметку по невербальным характеристикам и эмоциям. затем уточнили результаты с помощью ручной разметки и отфильтровалы шумные сэмплы (например, аудио со смехом, который оказался закадровым). после этого зафьюзили варианты правильных транскрипций от нескольких разметчиков и получили итоговый датасет: 13 часов аудио с 10 типами невербальных характеристик. затюнили на своём датасете cosyvocie и сравнились с cosyvoice2, который обучался на проприетарном датасете, нестатзначимо проиграли в sbs. в статье раскрыли детали пайплайна разметки, а датасет выложили на hugging face. там немного, но это честная работа. почему это круто синтез с невербальными характеристиками нужен для синтеза спонтанного и разговорного стилей речи. nvtts может быть использован для файнтьюна, а также как стартовая точка для скейла и unsupervised-разметки датасета большего размера. дарья дятлова ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-15T09:04:04+00:00" href="./posts/77.html">2025-09-15 09:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>Интересные статьи на Speech Synthesis Workshop 2/2<br></strong><br>Разберём ещё две любопытные работы с Speech Synthesis Workshop. Одна посвящена управлению стилем на уровне слов, другая — синтезу речи с невербальными характеристиками.<br><br><a href="https://www.isca-archive.org/ssw_2025/lemerle25_ssw.html" rel="nofollow noopener noreferrer"><strong>Lina-Style: Word-Level Style Control in TTS via Interleaved Synthetic Data</strong></a><strong><br></strong><br>Авторы предложили, как из небольшой выборки с разметкой стиля и большого неразмеченного корпуса построить полностью синтетический датасет с локальными (на уровне слова) метками стиля и его интенсивностью, а затем дообучить модель, чтобы она кондишенилась на метки. Для этого они использовали свою предыдущую работу, модель Lina-Speech. Архитектурно это текстовый энкодер и аудиодекодер с Gated Linear Attention (GLA). GLA, кстати, позволяет легко использовать prefix free prompting через initial state-tuning. Этим и воспользовались авторы.<br><br>Сначала они взяли претрейн Lina-Speech на неэмоциональной речи. Дотюнили его через initial state-tuning на несколько стилей (neutral, happy, confused, enunciated). Затем синтезировали несколько вариантов одной и той же реплики в разных стилях. Во время синтеза также использовали classifier‑free guidance (CFG), случайно сэмплировали альфа, поэтому насинтезированные аудио получились в разных стилях и с разной их интенсивностью. <br><br>Для каждого аудио построили соответствие текста аудиотокенам. Для этого извлекли матрицы soft-алайнмента текста и аудио и превратили их в матрицы hard-алайнмента с помощью Monotonic Alignment Search (MAS). Таким образом получили соответствие токенов аудио отдельным словам. Склеили слова из разных стилей в одно предложение и получили синтетический интерливинг-датасет с word-level-разметкой на стиль.  <br><br>Осталось затюнить итоговую модель. На этом этапе дообучили базовый претрейн, добавив новые параметры: эмбеддинги стилей, интенсивностей и linear для их комбинации.<br><br><strong>Почему это круто</strong><br><br>Потому что это — пример сбора синтетического датасета с локальными метками стиля с нуля. В изначальном датасете сэмплов с word-level-разметкой не было. Ну и успешное обучение на синте подтвердило, что метод рабочий. Позалипать на сэмплы можно <a href="https://anonymsubm.github.io/" rel="nofollow noopener noreferrer">тут</a>.<br> <br><a href="https://arxiv.org/abs/2507.13155" rel="nofollow noopener noreferrer"><strong>NonverbalTTS: A Public English Corpus of Text-Aligned Nonverbal Vocalizations with Emotion Annotations for Text-to-Speech</strong></a><br><br>Янднекс тоже привёз свою статью, написанную совместно с коллегами из VK Lab. В ней предложили датасет для синтеза речи с невербальными характеристиками на английском языке и рассказали о пайплайне его сбора. Невербальные характеристики — это смех, вздох, кашель и другие звуки, которые мы издаём в речи и которые не являются словами. <br><br>В реальной жизни таких невербальных характеристик много, но разметки для них часто нет. Авторы взяли два опенсорсных датасета — Expresso и VoxCeleb — и сначала с помощью опенсорсных моделей получили грубую разметку по невербальным характеристикам и эмоциям. Затем уточнили результаты с помощью ручной разметки и отфильтровалы шумные сэмплы (например, аудио со смехом, который оказался закадровым). После этого зафьюзили варианты правильных транскрипций от нескольких разметчиков и получили итоговый датасет: 13 часов аудио с 10 типами невербальных характеристик.<br><br>Затюнили на своём датасете CosyVocie и сравнились с CosyVoice2, который обучался на проприетарном датасете, нестатзначимо проиграли в SbS. В статье раскрыли детали пайплайна разметки, а датасет <a href="https://huggingface.co/datasets/deepvk/NonverbalTTS" rel="nofollow noopener noreferrer">выложили</a> на Hugging Face. Там немного, но это честная работа. <br><br><strong>Почему это круто<br></strong><br>Синтез с невербальными характеристиками нужен для синтеза спонтанного и разговорного стилей речи. NVTTS может быть использован для файнтьюна, а также как стартовая точка для скейла и unsupervised-разметки датасета большего размера.<br><br><em>Дарья Дятлова</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/77_480.webp" srcset="../assets/media/thumbs/77_480.webp 480w, ../assets/media/77.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="77" data-image-index="0" /></div></div>
      <div class="actions">
        <span>915 просмотров · 14 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/77" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/77.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="76" data-search="интересные статьи на speech synthesis workshop 1/2 speech synthesis workshop — это воркшоп, который проходит при конференции interspeech. в этом году разработчик службы синтеза речи дарья дятлова побывала на мероприятии и поделилась интересными статьями. analyzing and improving speaker similarity assessment for speech synthesis авторы сравнивают схожесть оригинального голоса спикера и синтезированного моделью с помощью автоматической метрики. выдеяют четыре тезиса-предпосылки. 1. обычно для такого сравнения используют косинусную близость двух векторов, полученных из эмбедов модели для верификации спикеров (sv). лучшая sv-модель — не всегда лучший экстрактор фичей для метрики схожести спикеров. 2. характеристики голоса спикера можно разделить на две группы. первые — спектральные — статичны и не меняются от записи. по ним легко отделить одного спикера от другого. вторые — темп, длительность, громкость речи — динамичны и могут меняться от записи к записи. эмбеддинги большинства моделей для sv не содержат информации о динамических характеристиках аудио. 3. эмбединги моделей sv содержат информацию о чистоте сигнала и длительности аудио — это не прямая характеристика голоса спикера, а определенный баес датасета или tts-модели. при конструировании метрики на основе этих эмбедингов стоит иметь это ввиду и применять определённые фильтры, которые помогают сгладить эффект. 4. грубым способом оценки ритма может быть оценка темпа речи спикера. однако такой способ — сравнение средних величин количества слогов на секунду аудио — признак с плохой разделительной способностью. что предложили в статье предложили метрику u3d (unit duration distribution distance), которая оценивает одну из динамических характеристик речи спикера — её ритм. метрика основана на сравнении распределений длительности групп фонем. это не фонемы в буквальном смысле (они извлекаются в unsupervised-сетапе путём кластеризации эмбеддингов hubert), но дальше для простоты буду называть их фонемами. каждому элементу в последовательности из спич-юнитов присваивается индекс ближайшей к нему фонемы, после чего последовательность разделяется на сегменты идущих друг за другом фонем. для каждой фонемы считается длительность в количестве спич-юнитов для каждой записи спикера. затем тестовая и контрольная выборки распределений сравниваются через метрику вассерштейна. в результате авторы показали, что метрика обладает высокой разделительной способностью и робастна к сравнению схожих спикеров. почему это круто большая часть статьи посвящена не самой метрике, а подводке к тому, зачем вообще она нужна и почему не всегда достаточно просто считать косинусную близость между эмбедами какой-то модели верификации спикеров и называть это speaker-similarity. пайплайн unsupervised-разметки легко адаптируется и хорошо ложится не только на задачу ритма и не только для подсчёта метрики. продолжение следует. дарья дятлова ❣ специально для speech info интересные статьи на speech synthesis workshop 1/2 speech synthesis workshop — это воркшоп, который проходит при конференции interspeech . в этом году разработчик службы синтеза речи дарья дятлова побывала на мероприятии и поделилась интересными статьями. analyzing and improving speaker similarity assessment for speech synthesis авторы сравнивают схожесть оригинального голоса спикера и синтезированного моделью с помощью автоматической метрики. выдеяют четыре тезиса-предпосылки. 1. обычно для такого сравнения используют косинусную близость двух векторов, полученных из эмбедов модели для верификации спикеров (sv). лучшая sv-модель — не всегда лучший экстрактор фичей для метрики схожести спикеров. 2. характеристики голоса спикера можно разделить на две группы. первые — спектральные — статичны и не меняются от записи. по ним легко отделить одного спикера от другого. вторые — темп, длительность, громкость речи — динамичны и могут меняться от записи к записи. эмбеддинги большинства моделей для sv не содержат информации о динамических характеристиках аудио. 3. эмбединги моделей sv содержат информацию о чистоте сигнала и длительности аудио — это не прямая характеристика голоса спикера, а определенный баес датасета или tts-модели. при конструировании метрики на основе этих эмбедингов стоит иметь это ввиду и применять определённые фильтры, которые помогают сгладить эффект. 4. грубым способом оценки ритма может быть оценка темпа речи спикера. однако такой способ — сравнение средних величин количества слогов на секунду аудио — признак с плохой разделительной способностью. что предложили в статье предложили метрику u3d (unit duration distribution distance), которая оценивает одну из динамических характеристик речи спикера — её ритм. метрика основана на сравнении распределений длительности групп фонем. это не фонемы в буквальном смысле (они извлекаются в unsupervised-сетапе путём кластеризации эмбеддингов hubert), но дальше для простоты буду называть их фонемами. каждому элементу в последовательности из спич-юнитов присваивается индекс ближайшей к нему фонемы, после чего последовательность разделяется на сегменты идущих друг за другом фонем. для каждой фонемы считается длительность в количестве спич-юнитов для каждой записи спикера. затем тестовая и контрольная выборки распределений сравниваются через метрику вассерштейна. в результате авторы показали, что метрика обладает высокой разделительной способностью и робастна к сравнению схожих спикеров. почему это круто большая часть статьи посвящена не самой метрике, а подводке к тому, зачем вообще она нужна и почему не всегда достаточно просто считать косинусную близость между эмбедами какой-то модели верификации спикеров и называть это speaker-similarity. пайплайн unsupervised-разметки легко адаптируется и хорошо ложится не только на задачу ритма и не только для подсчёта метрики. продолжение следует. дарья дятлова ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-10T09:00:01+00:00" href="./posts/76.html">2025-09-10 09:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>Интересные статьи на Speech Synthesis Workshop 1/2<br></strong><br>Speech Synthesis Workshop — это воркшоп, который проходит при конференции <a href="https://www.interspeech2025.org/home" rel="nofollow noopener noreferrer">Interspeech</a>. В этом году разработчик службы синтеза речи Дарья Дятлова побывала на мероприятии и поделилась интересными статьями. <br><br><a href="https://www.isca-archive.org/ssw_2025/carbonneau25_ssw.html" rel="nofollow noopener noreferrer"><strong>Analyzing and Improving Speaker Similarity Assessment for Speech Synthesis</strong></a><br><br>Авторы сравнивают схожесть оригинального голоса спикера и синтезированного моделью с помощью автоматической метрики. Выдеяют четыре тезиса-предпосылки.<br><br>1. Обычно для такого сравнения используют косинусную близость двух векторов, полученных из эмбедов модели для верификации спикеров (SV). Лучшая SV-модель — не всегда лучший экстрактор фичей для метрики схожести спикеров.<br><br>2. Характеристики голоса спикера можно разделить на две группы. Первые — спектральные — статичны и не меняются от записи. По ним легко отделить одного спикера от другого. Вторые — темп, длительность, громкость речи — динамичны и могут меняться от записи к записи. Эмбеддинги большинства моделей для SV не содержат информации о динамических характеристиках аудио. <br><br>3. Эмбединги моделей SV содержат информацию о чистоте сигнала и длительности аудио — это не прямая характеристика голоса спикера, а определенный баес датасета или TTS-модели. При конструировании метрики на основе этих эмбедингов стоит иметь это ввиду и применять определённые фильтры, которые помогают сгладить эффект. <br><br>4. Грубым способом оценки ритма может быть оценка темпа речи спикера. Однако такой способ — сравнение средних величин количества слогов на секунду аудио — признак с плохой разделительной способностью. <br><br><strong>Что предложили<br></strong><br>В статье предложили метрику U3D (Unit Duration Distribution Distance), которая оценивает одну из динамических характеристик речи спикера — её ритм. Метрика основана на сравнении распределений длительности групп фонем. Это не фонемы в буквальном смысле (они извлекаются в unsupervised-сетапе путём кластеризации эмбеддингов HuBERT), но дальше для простоты буду называть их фонемами. <br><br>Каждому элементу в последовательности из спич-юнитов присваивается индекс ближайшей к нему фонемы, после чего последовательность разделяется на сегменты идущих друг за другом фонем. Для каждой фонемы считается длительность в количестве спич-юнитов для каждой записи спикера. Затем тестовая и контрольная выборки распределений сравниваются через метрику Вассерштейна. В результате авторы показали, что метрика обладает высокой разделительной способностью и робастна к сравнению схожих спикеров.<br><br><strong>Почему это круто<br></strong><br>Большая часть статьи посвящена не самой метрике, а подводке к тому, зачем вообще она нужна и почему не всегда достаточно просто считать косинусную близость между эмбедами какой-то модели верификации спикеров и называть это speaker-similarity. <br>Пайплайн unsupervised-разметки легко адаптируется и хорошо ложится не только на задачу ритма и не только для подсчёта метрики.<br><br>Продолжение следует.<br><br><em>Дарья Дятлова</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a></div>
      <div class="actions">
        <span>1 190 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/76" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/76.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="75" data-search="audio flamingo 3 сегодня разбираем статью о модели audio flamingo 3, в которой авторы предлагают новый энкодер af-whisper. одно из его ключевых отличий — умение обрабатывать все три типа аудио: речь, звуки и музыку. по словам авторов, большинство аудио-llm используют три отдельных энкодера для этих задач. основной вклад работы, который отмечают в статье: новый аудиоэнкодер, использование chain-of-thought, поддержка multi-turn-диалогов, понимание длинных записей и voice-to-voice-диалоги. кроме этого, модель полностью в открытом доступе, включая код обучения и использованные датасеты. что касается бенчмарков, авторы сравнились как с опенсорсными, так и с проприетарными решениями — модель везде показала лучшие результаты. наиболее близкой они считают gpt-4 audio, но при этом отмечают её закрытость. архитектура af-whisper построен на базе whisper, который дообучили вместе с llm. есть недостаток — модель не стриминговая: работает с 30-секундными фрагментами звука, обрабатывая соседние куски без маскировки, а затем собирает аудио произвольной длины. whisper разморозили, добавили адаптер с кросс-аттеншеном (как в audio flamingo 2) и сгенерировали синтетические описания готовых аудио с помощью gpt-4.1. этапы обучения сначала обучают только адаптер, чтобы не повредить энкодер и llm. затем идёт совместное обучение энкодера и адаптера, после чего следуют дополнительные стадии: 1) sft — разморожена вся сеть; 2) расширение контекста и reasoning — разморожена только llm; 3) для добавления диалогов снова разморожена вся сеть. данные основная часть статьи посвящена описанию данных. первая и вторая стадии используют пост-опенсорсные датасеты, конвертированные в единый формат: например, в задачах asr модель явно просили выполнить транскрибацию. далее идёт стадия audio skills xl с добавлением пар «вопрос-ответ». авторы отмечают, что одного asr и классификации аудио по открытым датасетам недостаточно для появления reasoning, поэтому они генерируют дополнительные данные: — берут 30-секундные аудиофрагменты; — создают датасет из 4,5 млн новых вопросов-ответов, в основном multi-choice; — источники — youtube8m, music4all, million song dataset; — на основе метаданных и аудио gpt-4.1 генерирует промпты и ответы. для описания звуковых событий применяют аналогичный подход, используя также audio flamingo 2, который умеет давать базовые описания. reasoning отдельный датасет сделали для длинных аудио с задачами на рассуждение. разметку reasoning добавляли в небольшое число сэмплов и прямо на уровне промпта просили модель объяснить ход рассуждений. ответы — до 40 слов. префиксы для reasoning генерировали в gemini, так как он давал меньше галлюцинаций. примеры решаемых задач: определение сарказма и эмоционального состояния, извлечение информации из длинного аудио, определение порядка событий, суммаризация, отслеживание смены темы разговора. multi-turn-диалоги последним этапом авторы добавили данные для естественных многошаговых диалогов — 75 тысяч примеров, сгенерированных gpt. инфраструктура обучение проводили на кластере из 128 a100. авторы отмечают, что ключ к качественной модели — чистый датасет. в экспериментах вариант с reasoning давал заметный прирост качества. всеволод ковшов ❣ специально для speech info audio flamingo 3 сегодня разбираем статью о модели audio flamingo 3, в которой авторы предлагают новый энкодер af-whisper. одно из его ключевых отличий — умение обрабатывать все три типа аудио: речь, звуки и музыку. по словам авторов, большинство аудио-llm используют три отдельных энкодера для этих задач. основной вклад работы, который отмечают в статье: новый аудиоэнкодер, использование chain-of-thought, поддержка multi-turn-диалогов, понимание длинных записей и voice-to-voice-диалоги. кроме этого, модель полностью в открытом доступе, включая код обучения и использованные датасеты. что касается бенчмарков, авторы сравнились как с опенсорсными, так и с проприетарными решениями — модель везде показала лучшие результаты. наиболее близкой они считают gpt-4 audio, но при этом отмечают её закрытость. архитектура af-whisper построен на базе whisper, который дообучили вместе с llm. есть недостаток — модель не стриминговая: работает с 30-секундными фрагментами звука, обрабатывая соседние куски без маскировки, а затем собирает аудио произвольной длины. whisper разморозили, добавили адаптер с кросс-аттеншеном (как в audio flamingo 2) и сгенерировали синтетические описания готовых аудио с помощью gpt-4.1. этапы обучения сначала обучают только адаптер, чтобы не повредить энкодер и llm. затем идёт совместное обучение энкодера и адаптера, после чего следуют дополнительные стадии: 1) sft — разморожена вся сеть; 2) расширение контекста и reasoning — разморожена только llm; 3) для добавления диалогов снова разморожена вся сеть. данные основная часть статьи посвящена описанию данных. первая и вторая стадии используют пост-опенсорсные датасеты, конвертированные в единый формат: например, в задачах asr модель явно просили выполнить транскрибацию. далее идёт стадия audio skills xl с добавлением пар «вопрос-ответ». авторы отмечают, что одного asr и классификации аудио по открытым датасетам недостаточно для появления reasoning, поэтому они генерируют дополнительные данные: — берут 30-секундные аудиофрагменты; — создают датасет из 4,5 млн новых вопросов-ответов, в основном multi-choice; — источники — youtube8m, music4all, million song dataset; — на основе метаданных и аудио gpt-4.1 генерирует промпты и ответы. для описания звуковых событий применяют аналогичный подход, используя также audio flamingo 2, который умеет давать базовые описания. reasoning отдельный датасет сделали для длинных аудио с задачами на рассуждение. разметку reasoning добавляли в небольшое число сэмплов и прямо на уровне промпта просили модель объяснить ход рассуждений. ответы — до 40 слов. префиксы для reasoning генерировали в gemini, так как он давал меньше галлюцинаций. примеры решаемых задач: определение сарказма и эмоционального состояния, извлечение информации из длинного аудио, определение порядка событий, суммаризация, отслеживание смены темы разговора. multi-turn-диалоги последним этапом авторы добавили данные для естественных многошаговых диалогов — 75 тысяч примеров, сгенерированных gpt. инфраструктура обучение проводили на кластере из 128 a100. авторы отмечают, что ключ к качественной модели — чистый датасет. в экспериментах вариант с reasoning давал заметный прирост качества. всеволод ковшов ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-01T08:02:43+00:00" href="./posts/75.html">2025-09-01 08:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Audio Flamingo 3</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/abs/2507.08128" rel="nofollow noopener noreferrer">статью</a> о модели Audio Flamingo 3, в которой авторы предлагают новый энкодер AF-Whisper. Одно из его ключевых отличий — умение обрабатывать все три типа аудио: речь, звуки и музыку. По словам авторов, большинство аудио-LLM используют три отдельных энкодера для этих задач.<br><br>Основной вклад работы, который отмечают в статье: новый аудиоэнкодер, использование chain-of-thought, поддержка multi-turn-диалогов, понимание длинных записей и voice-to-voice-диалоги. Кроме этого, модель полностью в открытом доступе, включая код обучения и использованные датасеты.<br><br>Что касается бенчмарков, авторы сравнились как с опенсорсными, так и с проприетарными решениями — модель везде показала лучшие результаты. Наиболее близкой они считают GPT-4 Audio, но при этом отмечают её закрытость.<br><br><strong>Архитектура</strong><br><br>AF-Whisper построен на базе Whisper, который дообучили вместе с LLM. Есть недостаток — модель не стриминговая: работает с 30-секундными фрагментами звука, обрабатывая соседние куски без маскировки, а затем собирает аудио произвольной длины. Whisper разморозили, добавили адаптер с кросс-аттеншеном (как в Audio Flamingo 2) и сгенерировали синтетические описания готовых аудио с помощью GPT-4.1.<br><br><strong>Этапы обучения<br></strong><br>Сначала обучают только адаптер, чтобы не повредить энкодер и LLM. Затем идёт совместное обучение энкодера и адаптера, после чего следуют дополнительные стадии: 1) SFT — разморожена вся сеть; 2) расширение контекста и reasoning — разморожена только LLM; 3) для добавления диалогов снова разморожена вся сеть.<br><br><strong>Данные</strong><br><br>Основная часть статьи посвящена описанию данных. Первая и вторая стадии используют пост-опенсорсные датасеты, конвертированные в единый формат: например, в задачах ASR модель явно просили выполнить транскрибацию. Далее идёт стадия Audio Skills XL с добавлением пар «вопрос-ответ».<br><br>Авторы отмечают, что одного ASR и классификации аудио по открытым датасетам недостаточно для появления reasoning, поэтому они генерируют дополнительные данные:<br><br>— берут 30-секундные аудиофрагменты;<br>— создают датасет из 4,5 млн новых вопросов-ответов, в основном multi-choice;<br>— источники — YouTube8M, Music4All, Million Song Dataset;<br>— на основе метаданных и аудио GPT-4.1 генерирует промпты и ответы.<br><br>Для описания звуковых событий применяют аналогичный подход, используя также Audio Flamingo 2, который умеет давать базовые описания.<br><br><strong>Reasoning<br></strong><br>Отдельный датасет сделали для длинных аудио с задачами на рассуждение. Разметку reasoning добавляли в небольшое число сэмплов и прямо на уровне промпта просили модель объяснить ход рассуждений. Ответы — до 40 слов. Префиксы для reasoning генерировали в Gemini, так как он давал меньше галлюцинаций.<br><br>Примеры решаемых задач: определение сарказма и эмоционального состояния, извлечение информации из длинного аудио, определение порядка событий, суммаризация,<br>отслеживание смены темы разговора.<br><br><strong>Multi-turn-диалоги<br></strong><br>Последним этапом авторы добавили данные для естественных многошаговых диалогов — 75 тысяч примеров, сгенерированных GPT.<br><br><strong>Инфраструктура</strong><br><br>Обучение проводили на кластере из 128 A100. Авторы отмечают, что ключ к качественной модели — чистый датасет. В экспериментах вариант с reasoning давал заметный прирост качества.<br><br><em>Всеволод Ковшов</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/75_480.webp" srcset="../assets/media/thumbs/75_480.webp 480w, ../assets/media/75.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="75" data-image-index="0" /></div></div>
      <div class="actions">
        <span>995 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/75" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/75.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="74" data-search="streaming sortformer: speaker cache-based online speaker diarization with arrival-time ordering сегодня разбираем статью с interspeech 2025 от nvidia, посвящённую стриминговой end-to-end-диаризации спикеров с использованием arrival-time ordering cache. основное применение — интеграция в multi-talker asr. на конференции статью представлял основной автор, исследователь nvidia, иван меденников. в работе представляют улучшение предыдущей модели sortformer, в которой были предложены архитектура с sort loss и метод выравнивания сегментов диаризации с токенами asr. ключевой новинкой стал arrival-order speaker cache (aosc) — кэш эмбеддингов спикеров, упорядоченных по времени появления. модель работает в скользящем окне: в кэш добавляются фреймы с наивысшими оценками уверенности для каждого спикера, с динамическим распределением (минимум k фреймов на спикера и silence embeddings для переходов). интересные аспекты: — sort loss (сортирует спикеров по времени появления, в отличие от attractor-based eend) не заменяет полностью pil, но их комбинация работает лучше, так как sort loss выполяет функцию регуляризации. — стриминговая версия превосходит офлайн-версию на длинных записях, устраняя train-inference mismatch (обучение на 90-секундных сегментах vs произвольная длина теста). — инициализация от предобученного офлайн sortformer полезна, но fine-tuning с aosc обязателен, так как фреймы в кэше могут быть непоследовательными. — обучение проводилось на 5150 часах симулированных смесей и 2030 часах реальных данных. с синтетикой нужно быть осторожными: модель склонна к оверфиту на background noise. эксперименты показывают sota для e2e-онлайн-диаризации с достаточно низкой latency. ограничения модели следующие: фиксированное максимальное число спикеров (4 в работе), масштабирование требует данных с большим числом дикторов и растёт вычислительная сложность pil (o(n!) для перестановок). дмитрий попов ❣ специально для speech info streaming sortformer: speaker cache-based online speaker diarization with arrival-time ordering сегодня разбираем статью с interspeech 2025 от nvidia, посвящённую стриминговой end-to-end-диаризации спикеров с использованием arrival-time ordering cache. основное применение — интеграция в multi-talker asr. на конференции статью представлял основной автор, исследователь nvidia, иван меденников. в работе представляют улучшение предыдущей модели sortformer , в которой были предложены архитектура с sort loss и метод выравнивания сегментов диаризации с токенами asr. ключевой новинкой стал arrival-order speaker cache (aosc) — кэш эмбеддингов спикеров, упорядоченных по времени появления. модель работает в скользящем окне: в кэш добавляются фреймы с наивысшими оценками уверенности для каждого спикера, с динамическим распределением (минимум k фреймов на спикера и silence embeddings для переходов). интересные аспекты: — sort loss (сортирует спикеров по времени появления, в отличие от attractor-based eend) не заменяет полностью pil, но их комбинация работает лучше, так как sort loss выполяет функцию регуляризации. — стриминговая версия превосходит офлайн-версию на длинных записях, устраняя train-inference mismatch (обучение на 90-секундных сегментах vs произвольная длина теста). — инициализация от предобученного офлайн sortformer полезна, но fine-tuning с aosc обязателен, так как фреймы в кэше могут быть непоследовательными. — обучение проводилось на 5150 часах симулированных смесей и 2030 часах реальных данных. с синтетикой нужно быть осторожными: модель склонна к оверфиту на background noise. эксперименты показывают sota для e2e-онлайн-диаризации с достаточно низкой latency. ограничения модели следующие: фиксированное максимальное число спикеров (4 в работе), масштабирование требует данных с большим числом дикторов и растёт вычислительная сложность pil (o(n!) для перестановок). дмитрий попов ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-27T08:05:43+00:00" href="./posts/74.html">2025-08-27 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization with Arrival-Time Ordering<br></strong><br>Сегодня разбираем <a href="https://arxiv.org/html/2507.18446v1" rel="nofollow noopener noreferrer">статью</a> с Interspeech 2025 от NVIDIA, посвящённую стриминговой end-to-end-диаризации спикеров с использованием Arrival-Time Ordering Cache. Основное применение — интеграция в multi-talker ASR. На конференции статью представлял основной автор, исследователь NVIDIA, Иван Меденников. <br><br>В работе представляют улучшение предыдущей модели <a href="https://arxiv.org/html/2409.06656v1" rel="nofollow noopener noreferrer">Sortformer</a>, в которой были предложены архитектура с Sort Loss и метод выравнивания сегментов диаризации с токенами ASR. Ключевой новинкой стал Arrival-Order Speaker Cache (AOSC) — кэш эмбеддингов спикеров, упорядоченных по времени появления. Модель работает в скользящем окне: в кэш добавляются фреймы с наивысшими оценками уверенности для каждого спикера, с динамическим распределением (минимум K фреймов на спикера и silence embeddings для переходов). <br><br>Интересные аспекты:<br><br>— Sort Loss (сортирует спикеров по времени появления, в отличие от attractor-based EEND) не заменяет полностью PIL, но их комбинация работает лучше, так как Sort Loss выполяет функцию регуляризации.<br>— Стриминговая версия превосходит офлайн-версию на длинных записях, устраняя train-inference mismatch (обучение на 90-секундных сегментах vs произвольная длина теста).<br>— Инициализация от предобученного офлайн Sortformer полезна, но fine-tuning с AOSC обязателен, так как фреймы в кэше могут быть непоследовательными.<br>— Обучение проводилось на 5150 часах симулированных смесей и 2030 часах реальных данных. С синтетикой нужно быть осторожными: модель склонна к оверфиту на background noise.<br><br>Эксперименты показывают SOTA для E2E-онлайн-диаризации с достаточно низкой latency. <br><br>Ограничения модели следующие: фиксированное максимальное число спикеров (4 в работе), масштабирование требует данных с большим числом дикторов и растёт вычислительная сложность PIL (O(N!) для перестановок). <br><br><em>Дмитрий Попов </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/74_480.webp" srcset="../assets/media/thumbs/74_480.webp 480w, ../assets/media/74.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="74" data-image-index="0" /></div></div>
      <div class="actions">
        <span>992 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/74" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/74.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="71" data-search="сегодня завершается interspeech 2025 под занавес конференции делимся несколькими атмосферными фото и видео: — фрагмент show&amp;tell-сессии с физической моделью голосового тракта. редкий случай, когда на конференции показывают не абстрактные алгоритмы, а реальную говорящую машину. — команда яндекса — как обычно, в эпицентре идей и технологий. — анонс interspeech 2026: в следующем году встречаемся в сиднее! speech info сегодня завершается interspeech 2025 под занавес конференции делимся несколькими атмосферными фото и видео: — фрагмент show&amp;amp;tell-сессии с физической моделью голосового тракта. редкий случай, когда на конференции показывают не абстрактные алгоритмы, а реальную говорящую машину. — команда яндекса — как обычно, в эпицентре идей и технологий. — анонс interspeech 2026: в следующем году встречаемся в сиднее! speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-21T13:50:01+00:00" href="./posts/71.html">2025-08-21 13:50 UTC</a></div>
      </div>
      <div class="post-body"><strong>Сегодня завершается Interspeech 2025<br></strong><br>Под занавес конференции делимся несколькими атмосферными фото и видео:<br><br>— Фрагмент Show&amp;Tell-сессии с физической моделью голосового тракта. Редкий случай, когда на конференции показывают не абстрактные алгоритмы, а реальную говорящую машину. <br><br>— Команда Яндекса — как обычно, в эпицентре идей и технологий.<br><br>— Анонс Interspeech 2026: в следующем году встречаемся в Сиднее!<br><br><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><video controls preload="metadata" src="../assets/media/71_37820807.mp4"></video><img class="media-img" loading="lazy" src="../assets/media/thumbs/72_480.webp" srcset="../assets/media/thumbs/72_480.webp 480w, ../assets/media/72.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="71" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/73_480.webp" srcset="../assets/media/thumbs/73_480.webp 480w, ../assets/media/73.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="71" data-image-index="1" /></div></div>
      <div class="actions">
        <span>987 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/71" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/71.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="69" data-search="в этом году яндекс привёз на interspeech статью multichannel keyword spotting for noisy conditions о нейросетевой архитектуре kws. решение объединяет мультиканальный вход и attention-механизм для более точного распознавания голосовых команд в шумных помещениях. подробнее о работе рассказывали здесь, а теперь делимся маленьким фоторепортажем с постера! speech info в этом году яндекс привёз на interspeech статью multichannel keyword spotting for noisy conditions о нейросетевой архитектуре kws. решение объединяет мультиканальный вход и attention-механизм для более точного распознавания голосовых команд в шумных помещениях. подробнее о работе рассказывали здесь , а теперь делимся маленьким фоторепортажем с постера! speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-20T12:06:21+00:00" href="./posts/69.html">2025-08-20 12:06 UTC</a></div>
      </div>
      <div class="post-body">В этом году Яндекс привёз на Interspeech статью <a href="https://arxiv.org/abs/2507.15558" rel="nofollow noopener noreferrer">Multichannel Keyword Spotting for Noisy Conditions</a> о нейросетевой архитектуре KWS.<br><br>Решение объединяет мультиканальный вход и attention-механизм для более точного распознавания голосовых команд в шумных помещениях. <br><br>Подробнее о работе рассказывали <a href="https://t.me/speechinfo/64" rel="nofollow noopener noreferrer">здесь</a>, а теперь делимся маленьким фоторепортажем с постера!<br><br><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/69_480.webp" srcset="../assets/media/thumbs/69_480.webp 480w, ../assets/media/69.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="69" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/70_480.webp" srcset="../assets/media/thumbs/70_480.webp 480w, ../assets/media/70.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="69" data-image-index="1" /></div></div>
      <div class="actions">
        <span>957 просмотров · 37 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/69" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/69.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="67" data-search="интересные статьи на interspeech 2025 в роттердаме проходит interspeech 2025, крупнейшая мировая конференция по речевым технологиям. тема этого года — fair and inclusive speech science and technology. спешим поделиться двумя интересными статьями от alibaba group (создателей cosy voice, около-sota tts-модели). long-context speech synthesis with context-aware memory для сохранения контекста и натуральности синтеза длинных предложений авторы предлагают механизм памяти — раздельно для текста и звука, — который авторегрессионно обновляется. при обучении история поддерживается на уровне параграфа. differentiable reward optimization for llm based tts system в работе предложили метод вычисления reward-функции напрямую из токенов аудиокодека с помощью multi-task reward. модель одновременно оценивает несколько аспектов синтеза: точность произношения (через задачу asr), эмоциональную окраску (ser), качество звучания (sqa), а также характеристики говорящего — пол и возраст. для дифференцируемости используют gumbel-softmax. главные результаты: sota на seed-tts, а также возможность контролировать в tts-системе эмоции, пол, возраст и mos (mean opinion score). работы отобрал ❣ дмитрий попов speech info интересные статьи на interspeech 2025 в роттердаме проходит interspeech 2025 , крупнейшая мировая конференция по речевым технологиям. тема этого года — fair and inclusive speech science and technology. спешим поделиться двумя интересными статьями от alibaba group (создателей cosy voice, около-sota tts-модели). long-context speech synthesis with context-aware memory для сохранения контекста и натуральности синтеза длинных предложений авторы предлагают механизм памяти — раздельно для текста и звука, — который авторегрессионно обновляется. при обучении история поддерживается на уровне параграфа. differentiable reward optimization for llm based tts system в работе предложили метод вычисления reward-функции напрямую из токенов аудиокодека с помощью multi-task reward. модель одновременно оценивает несколько аспектов синтеза: точность произношения (через задачу asr), эмоциональную окраску (ser), качество звучания (sqa), а также характеристики говорящего — пол и возраст. для дифференцируемости используют gumbel-softmax. главные результаты: sota на seed-tts, а также возможность контролировать в tts-системе эмоции, пол, возраст и mos (mean opinion score). работы отобрал ❣ дмитрий попов speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-20T09:12:51+00:00" href="./posts/67.html">2025-08-20 09:12 UTC</a></div>
      </div>
      <div class="post-body"><strong>Интересные статьи на Interspeech 2025</strong><br><br>В Роттердаме проходит <a href="https://www.interspeech2025.org/home" rel="nofollow noopener noreferrer">Interspeech 2025</a>, крупнейшая мировая конференция по речевым технологиям. Тема этого года — Fair and Inclusive Speech Science and Technology. Спешим поделиться двумя интересными статьями от Alibaba Group (создателей Cosy Voice, около-SOTA TTS-модели).<br><br><a href="https://www.isca-archive.org/interspeech_2025/li25b_interspeech.html?utm_source=chatgpt.com" rel="nofollow noopener noreferrer"><strong>Long-Context Speech Synthesis with Context-Aware Memory</strong></a><br>Для сохранения контекста и натуральности синтеза длинных предложений авторы предлагают механизм памяти — раздельно для текста и звука, — который авторегрессионно обновляется. При обучении история поддерживается на уровне параграфа.<br><br><a href="https://arxiv.org/pdf/2507.05911" rel="nofollow noopener noreferrer"><strong>Differentiable Reward Optimization for LLM based TTS system</strong></a><br>В работе предложили метод вычисления reward-функции напрямую из токенов аудиокодека с помощью Multi-Task Reward. Модель одновременно оценивает несколько аспектов синтеза: точность произношения (через задачу ASR), эмоциональную окраску (SER), качество звучания (SQA), а также характеристики говорящего — пол и возраст. Для дифференцируемости используют Gumbel-Softmax. Главные результаты: SOTA на SEED-TTS, а также возможность контролировать в TTS-системе эмоции, пол, возраст и MOS (Mean Opinion Score).<br><br><em>Работы отобрал </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Дмитрий Попов <br></em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/67_480.webp" srcset="../assets/media/thumbs/67_480.webp 480w, ../assets/media/67.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="67" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/68_480.webp" srcset="../assets/media/thumbs/68_480.webp 480w, ../assets/media/68.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="67" data-image-index="1" /></div></div>
      <div class="actions">
        <span>901 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/67" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/67.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="66" data-search="voxtral сегодня разбираем статью об опенсорсной модели voxtral от mistral ai. ключевая идея решения в том, чтобы к уже обученной текстовой llm «прикрутить» аудио. для этого используют готовый asr-энкодер (whisper) и адаптер, после чего ответы генерирует языковой декодер. аудио режут на фрагменты по 30 секунд, обрабатывают их энкодером, склеивают эмбеддинги и прореживают в четыре раза в адаптере, уменьшая длину последовательности. на вход декодеру можно подать и текстовые токены, например вопрос или инструкцию. есть две версии модели. в составе mini-версии — аудиоэнкодер на 640 млн параметров, адаптер на 25 млн, текстовые эмбеддинги на 400 млн и декодер на ~3,6 млрд (всего ~4,7 млрд); в small — аналогичный аудиоэнкодер и адаптер на 52 млн, но уже 670 млн в эмбеддингах и 22,9 млрд в декодере (всего ~24,3 млрд). контекст аудиоветки — до 32 тысяч токенов, что соответствует примерно 40 минутам звука. для предобучения длинное аудио сначала размечают (vad → транскрипция → диаризация), затем разбивают на пары (aₙ, tₙ) и учат на двух паттернах: repetition, где по аудио восстанавливают его транскрипцию, и continuation, где по аудио восстанавливают следующий текст. на первом проходе замораживают аудиоэнкодер и языковой декодер, обучая только адаптер — это заметно помогает в задачах понимания речи, тогда как на чистом asr почти не сказывается. стадия sft нужна, чтобы модель умела больше, чем просто распознавание речи. датасет sft состоит из синтетических примеров. в случае, когда инструкция передается текстом для длинных аудио, транскрипцию из asr обрабатывает llm, генерируя пары «вопрос-ответ». если же инструкция задана в аудио формате, то авторы адаптируют текстовые sft-датасеты с помощью озвучки инструкций через предобученную tts-модель. есть и стадия rl/dpo-подобного обучения по парам ответов, которая даёт выигрыш в основном на маленькой модели. при этом для задачи asr на большой модели данный этап даже снижал качество, поэтому в релиз он не вошёл. авторы отдельно показывают, что обучение только на interleaved-паттерне портит asr, а только на asr-паттерне — не даёт навыков понимания. смешение двух задач примерно 50/50 даёт хороший баланс распознавания и понимания. в бенчмарках voxtral улучшает whisper (взятый за энкодер) и показывает sota среди открытых моделей на части тестов по asr. в переводе речи и аудиопонимании результаты конкурентны открытым моделям, а по синтетическим мультимодальным тестам на озвученных tts данных местами уступают проприетарным системам уровня gpt-4o и gemini. при этом текстовые навыки llm после добавления аудио практически не страдают. влад батаев ❣ специально для speech info voxtral сегодня разбираем статью об опенсорсной модели voxtral от mistral ai. ключевая идея решения в том, чтобы к уже обученной текстовой llm «прикрутить» аудио. для этого используют готовый asr-энкодер (whisper) и адаптер, после чего ответы генерирует языковой декодер. аудио режут на фрагменты по 30 секунд, обрабатывают их энкодером, склеивают эмбеддинги и прореживают в четыре раза в адаптере, уменьшая длину последовательности. на вход декодеру можно подать и текстовые токены, например вопрос или инструкцию. есть две версии модели. в составе mini-версии — аудиоэнкодер на 640 млн параметров, адаптер на 25 млн, текстовые эмбеддинги на 400 млн и декодер на ~3,6 млрд (всего ~4,7 млрд); в small — аналогичный аудиоэнкодер и адаптер на 52 млн, но уже 670 млн в эмбеддингах и 22,9 млрд в декодере (всего ~24,3 млрд). контекст аудиоветки — до 32 тысяч токенов, что соответствует примерно 40 минутам звука. для предобучения длинное аудио сначала размечают (vad → транскрипция → диаризация), затем разбивают на пары (aₙ, tₙ) и учат на двух паттернах: repetition, где по аудио восстанавливают его транскрипцию, и continuation, где по аудио восстанавливают следующий текст. на первом проходе замораживают аудиоэнкодер и языковой декодер, обучая только адаптер — это заметно помогает в задачах понимания речи, тогда как на чистом asr почти не сказывается. стадия sft нужна, чтобы модель умела больше, чем просто распознавание речи. датасет sft состоит из синтетических примеров. в случае, когда инструкция передается текстом для длинных аудио, транскрипцию из asr обрабатывает llm, генерируя пары «вопрос-ответ». если же инструкция задана в аудио формате, то авторы адаптируют текстовые sft-датасеты с помощью озвучки инструкций через предобученную tts-модель. есть и стадия rl/dpo-подобного обучения по парам ответов, которая даёт выигрыш в основном на маленькой модели. при этом для задачи asr на большой модели данный этап даже снижал качество, поэтому в релиз он не вошёл. авторы отдельно показывают, что обучение только на interleaved-паттерне портит asr, а только на asr-паттерне — не даёт навыков понимания. смешение двух задач примерно 50/50 даёт хороший баланс распознавания и понимания. в бенчмарках voxtral улучшает whisper (взятый за энкодер) и показывает sota среди открытых моделей на части тестов по asr. в переводе речи и аудиопонимании результаты конкурентны открытым моделям, а по синтетическим мультимодальным тестам на озвученных tts данных местами уступают проприетарным системам уровня gpt-4o и gemini. при этом текстовые навыки llm после добавления аудио практически не страдают. влад батаев ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-15T09:34:44+00:00" href="./posts/66.html">2025-08-15 09:34 UTC</a></div>
      </div>
      <div class="post-body"><strong>Voxtral</strong><br><br>Сегодня разбираем <a href="https://arxiv.org/html/2507.13264v1" rel="nofollow noopener noreferrer">статью</a> об опенсорсной модели Voxtral от Mistral AI. Ключевая идея решения в том, чтобы к уже обученной текстовой LLM «прикрутить» аудио. Для этого используют готовый ASR-энкодер (Whisper) и адаптер, после чего ответы генерирует языковой декодер. Аудио режут на фрагменты по 30 секунд, обрабатывают их энкодером, склеивают эмбеддинги и прореживают в четыре раза в адаптере, уменьшая длину последовательности. На вход декодеру можно подать и текстовые токены, например вопрос или инструкцию.<br><br>Есть две версии модели. В составе Mini-версии — аудиоэнкодер на 640 млн параметров, адаптер на 25 млн, текстовые эмбеддинги на 400 млн и декодер на ~3,6 млрд (всего ~4,7 млрд); в Small — аналогичный аудиоэнкодер и адаптер на 52 млн, но уже 670 млн в эмбеддингах и 22,9 млрд в декодере (всего ~24,3 млрд). Контекст аудиоветки — до 32 тысяч токенов, что соответствует примерно 40 минутам звука.<br><br>Для предобучения длинное аудио сначала размечают (VAD → транскрипция → диаризация), затем разбивают на пары (Aₙ, Tₙ) и учат на двух паттернах: repetition, где по аудио восстанавливают его транскрипцию, и continuation, где по аудио восстанавливают следующий текст. На первом проходе замораживают аудиоэнкодер и языковой декодер, обучая только адаптер — это заметно помогает в задачах понимания речи, тогда как на чистом ASR почти не сказывается.<br><br>Стадия SFT нужна, чтобы модель умела больше, чем просто распознавание речи. Датасет SFT состоит из синтетических примеров. В случае, когда инструкция передается текстом для длинных аудио, транскрипцию из ASR обрабатывает LLM, генерируя пары «вопрос-ответ». Если же инструкция задана в аудио формате, то авторы адаптируют текстовые SFT-датасеты с помощью озвучки инструкций через предобученную TTS-модель.<br><br>Есть и стадия RL/DPO-подобного обучения по парам ответов, которая даёт выигрыш в основном на маленькой модели. При этом для задачи ASR на большой модели данный этап даже снижал качество, поэтому в релиз он не вошёл.<br><br>Авторы отдельно показывают, что обучение только на interleaved-паттерне портит ASR, а только на ASR-паттерне — не даёт навыков понимания. Смешение двух задач примерно 50/50 даёт хороший баланс распознавания и понимания.<br><br>В бенчмарках Voxtral улучшает Whisper (взятый за энкодер) и показывает SOTA среди открытых моделей на части тестов по ASR. В переводе речи и аудиопонимании результаты конкурентны открытым моделям, а по синтетическим мультимодальным тестам на озвученных TTS данных местами уступают проприетарным системам уровня GPT-4o и Gemini. При этом текстовые навыки LLM после добавления аудио практически не страдают.<br><br><em>Влад Батаев </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/66_480.webp" srcset="../assets/media/thumbs/66_480.webp 480w, ../assets/media/66.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="66" data-image-index="0" /></div></div>
      <div class="actions">
        <span>981 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/66" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/66.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="65" data-search="обзор статей с icassp 25. часть 4: другие интересные статьи в заключительной части — три статьи: оценка качества аудио с помощью self-supervised-моделей, сравнение претрейнов для speaker recognition и новый подход к мультиспикерной asr с учётом информации о говорящем. предыдущие части: 1, 2, 3. distillation and pruning for scalable self-supervised representation-based speech quality assessment авторы предлагают модель оценки качества речи на базе xls-r. сначала они обучают большую модель (xls-r-sqa) на разных датасетах, включая zoom-звонки, синтетические и музыкальные данные. чтобы учесть различия между датасетами, в архитектуру добавляют обучаемые scale и shift для каждого из них. на инференсе используется общий вариант модели, который, судя по результатам, хорошо работает на разных типах данных. но полученная модель слишком большая, чтобы использовать её для оценки качества шумоподавления. чтобы её уменьшить используют два способа: дистиллируют в меньшую (distillmos) и обрезку параметров (prunemos). обе версии показывают стабильное качество на звонках, синтетических и музыкальных датасетах. авторы сравнивают полученные модели с dnsmos — популярной системой оценки качества для шумоподавления, обученной на данных dns challenge. показывают, что dnsmos хорошо работает на звонках, но хуже обобщается на другие домены данных. основной вывод: distillmos и prunemos достигают сопоставимого качества при меньшем размере и лучше обобщаются за пределами звонковых сетов. однако использовать предполагается именно distillmos, потому что прунинг работает лучше при достаточно в большом количестве параметров. in search of optimal pretraining strategy for robust speaker recognition статья от российских авторов, которые изучают, как выбор претрейна влияет на устойчивость speaker verification моделей. они используют tdnn-архитектуру поверх разных замороженных энкодеров: hubert, w2v, asr-tdnn, и оценивают её на нескольких открытых датасетах. на voices и voxceleb1 системы на self-supervised фичах показывают сопоставимые или немного лучшие результаты по сравнению с бейзлайнами вроде ecapa-tdnn и cam++. однако основное внимание в статье уделено обобщающей способности. на sre&#x27;16, &#x27;19 и &#x27;21 (модели не обучались на этих датасетах) наименьший eer достигается при использовании asr-претрейна и его фьюжена с другими энкодерами. например, на sre’19 cam++ даёт 13.88, asr-tdnn — 16.42, а их фьюжен — 9.66. авторы также анализируют влияние масштаба энкодера на переносимость. эксперименты показывают, что более крупные энкодеры (например, обученные на librispeech и voxceleb) помогают лучше обобщаться, даже если downstream tdnn остаётся компактным. meta-cat: speaker-informed speech embeddings via meta information concatenation for multi-talker asr авторы исследуют задачу мультиспикерной asr: модель должна распознавать речь сразу нескольких говорящих и приписывать реплики каждому из них. решение основано на использовании speaker-aware эмбеддингов, собранных через элементное перемножение двух компонентов: asr-эмбеддингов и вероятностей принадлежности каждого временного кадра конкретному спикеру. модель состоит из замороженного энкодера для диаризации и обучаемых компонентов — asr-энкодера, speaker encoding слоя и rnnt-декодера. на вход модель получает аудио с несколькими спикерами и (опционально) короткий «query»-пример нужного говорящего. выходом становится либо полная транскрипция с разметкой по спикерам (ms-asr), либо только текст нужного говорящего (ts-asr). ключевая часть архитектуры — блок speaker encoding. он принимает asr-эмбеддинги и вероятности по спикерам (из диаризации) и формирует многомерное представление, в котором каждый из каналов отвечает за конкретного спикера. это представление затем поступает в декодер. авторы отдельно отмечают, что модель можно использовать и в сценарии, где нужно отслеживать только одного говорящего. в будущей работе авторы обещают поддержку стриминга. алексей рак ❣ специально для speech info обзор статей с icassp 25. часть 4: другие интересные статьи в заключительной части — три статьи: оценка качества аудио с помощью self-supervised-моделей, сравнение претрейнов для speaker recognition и новый подход к мультиспикерной asr с учётом информации о говорящем. предыдущие части: 1 , 2 , 3 . distillation and pruning for scalable self-supervised representation-based speech quality assessment авторы предлагают модель оценки качества речи на базе xls-r. сначала они обучают большую модель (xls-r-sqa) на разных датасетах, включая zoom-звонки, синтетические и музыкальные данные. чтобы учесть различия между датасетами, в архитектуру добавляют обучаемые scale и shift для каждого из них. на инференсе используется общий вариант модели, который, судя по результатам, хорошо работает на разных типах данных. но полученная модель слишком большая, чтобы использовать её для оценки качества шумоподавления. чтобы её уменьшить используют два способа: дистиллируют в меньшую (distillmos) и обрезку параметров (prunemos). обе версии показывают стабильное качество на звонках, синтетических и музыкальных датасетах. авторы сравнивают полученные модели с dnsmos — популярной системой оценки качества для шумоподавления, обученной на данных dns challenge. показывают, что dnsmos хорошо работает на звонках, но хуже обобщается на другие домены данных. основной вывод: distillmos и prunemos достигают сопоставимого качества при меньшем размере и лучше обобщаются за пределами звонковых сетов. однако использовать предполагается именно distillmos, потому что прунинг работает лучше при достаточно в большом количестве параметров. in search of optimal pretraining strategy for robust speaker recognition статья от российских авторов, которые изучают, как выбор претрейна влияет на устойчивость speaker verification моделей. они используют tdnn-архитектуру поверх разных замороженных энкодеров: hubert, w2v, asr-tdnn, и оценивают её на нескольких открытых датасетах. на voices и voxceleb1 системы на self-supervised фичах показывают сопоставимые или немного лучшие результаты по сравнению с бейзлайнами вроде ecapa-tdnn и cam++. однако основное внимание в статье уделено обобщающей способности. на sre&amp;#x27;16, &amp;#x27;19 и &amp;#x27;21 (модели не обучались на этих датасетах) наименьший eer достигается при использовании asr-претрейна и его фьюжена с другими энкодерами. например, на sre’19 cam++ даёт 13.88, asr-tdnn — 16.42, а их фьюжен — 9.66. авторы также анализируют влияние масштаба энкодера на переносимость. эксперименты показывают, что более крупные энкодеры (например, обученные на librispeech и voxceleb) помогают лучше обобщаться, даже если downstream tdnn остаётся компактным. meta-cat: speaker-informed speech embeddings via meta information concatenation for multi-talker asr авторы исследуют задачу мультиспикерной asr: модель должна распознавать речь сразу нескольких говорящих и приписывать реплики каждому из них. решение основано на использовании speaker-aware эмбеддингов, собранных через элементное перемножение двух компонентов: asr-эмбеддингов и вероятностей принадлежности каждого временного кадра конкретному спикеру. модель состоит из замороженного энкодера для диаризации и обучаемых компонентов — asr-энкодера, speaker encoding слоя и rnnt-декодера. на вход модель получает аудио с несколькими спикерами и (опционально) короткий «query»-пример нужного говорящего. выходом становится либо полная транскрипция с разметкой по спикерам (ms-asr), либо только текст нужного говорящего (ts-asr). ключевая часть архитектуры — блок speaker encoding. он принимает asr-эмбеддинги и вероятности по спикерам (из диаризации) и формирует многомерное представление, в котором каждый из каналов отвечает за конкретного спикера. это представление затем поступает в декодер. авторы отдельно отмечают, что модель можно использовать и в сценарии, где нужно отслеживать только одного говорящего. в будущей работе авторы обещают поддержку стриминга. алексей рак ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-08T14:19:52+00:00" href="./posts/65.html">2025-08-08 14:19 UTC</a></div>
      </div>
      <div class="post-body"><strong>Обзор статей с ICASSP 25. Часть 4: другие интересные статьи</strong><br><br>В заключительной части — три статьи: оценка качества аудио с помощью self-supervised-моделей, сравнение претрейнов для speaker recognition и новый подход к мультиспикерной ASR с учётом информации о говорящем. Предыдущие части: <a href="https://t.me/speechinfo/38" rel="nofollow noopener noreferrer">1</a>, <a href="https://t.me/speechinfo/39" rel="nofollow noopener noreferrer">2</a>, <a href="https://t.me/speechinfo/46" rel="nofollow noopener noreferrer">3</a>.<br><br><a href="https://arxiv.org/abs/2502.05356" rel="nofollow noopener noreferrer"><strong>Distillation and Pruning for Scalable Self-Supervised Representation-Based Speech Quality Assessment</strong></a><br><br>Авторы предлагают модель оценки качества речи на базе XLS-R. Сначала они обучают большую модель (XLS-R-SQA) на разных датасетах, включая Zoom-звонки, синтетические и музыкальные данные. Чтобы учесть различия между датасетами, в архитектуру добавляют обучаемые scale и shift для каждого из них. На инференсе используется общий вариант модели, который, судя по результатам, хорошо работает на разных типах данных. Но полученная модель слишком большая, чтобы использовать её для оценки качества шумоподавления.<br><br>Чтобы её уменьшить используют два способа: дистиллируют в меньшую (DistillMOS) и обрезку параметров (PruneMOS). Обе версии показывают стабильное качество на звонках, синтетических и музыкальных датасетах.<br><br>Авторы сравнивают полученные модели с DNSMOS — популярной системой оценки качества для шумоподавления, обученной на данных DNS Challenge. Показывают, что DNSMOS хорошо работает на звонках, но хуже обобщается на другие домены данных.<br><br>Основной вывод: DistillMOS и PruneMOS достигают сопоставимого качества при меньшем размере и лучше обобщаются за пределами звонковых сетов. Однако использовать предполагается именно DistillMOS, потому что прунинг работает лучше при достаточно в большом количестве параметров.<br><br><a href="https://www.researchgate.net/publication/390539139_In_Search_of_Optimal_Pretraining_Strategy_for_Robust_Speaker_Recognition" rel="nofollow noopener noreferrer"><strong>In Search of Optimal Pretraining Strategy for Robust Speaker Recognition</strong></a><br><br>Статья от российских авторов, которые изучают, как выбор претрейна влияет на устойчивость speaker verification моделей. Они используют TDNN-архитектуру поверх разных замороженных энкодеров: HuBERT, W2V, ASR-TDNN, и оценивают её на нескольких открытых датасетах.<br><br>На VOiCES и VoxCeleb1 системы на self-supervised фичах показывают сопоставимые или немного лучшие результаты по сравнению с бейзлайнами вроде ECAPA-TDNN и CAM++. Однако основное внимание в статье уделено обобщающей способности. На SRE&#x27;16, &#x27;19 и &#x27;21 (модели не обучались на этих датасетах) наименьший EER достигается при использовании ASR-претрейна и его фьюжена с другими энкодерами. Например, на SRE’19 CAM++ даёт 13.88, ASR-TDNN — 16.42, а их фьюжен — 9.66.<br><br>Авторы также анализируют влияние масштаба энкодера на переносимость. Эксперименты показывают, что более крупные энкодеры (например, обученные на LibriSpeech и VoxCeleb) помогают лучше обобщаться, даже если downstream TDNN остаётся компактным.<br><br><a href="https://arxiv.org/abs/2409.12352" rel="nofollow noopener noreferrer">META-CAT: Speaker-Informed Speech Embeddings via Meta Information Concatenation for Multi-talker ASR</a><br><br>Авторы исследуют задачу мультиспикерной ASR: модель должна распознавать речь сразу нескольких говорящих и приписывать реплики каждому из них. Решение основано на использовании speaker-aware эмбеддингов, собранных через элементное перемножение двух компонентов: ASR-эмбеддингов и вероятностей принадлежности каждого временного кадра конкретному спикеру.<br><br>Модель состоит из замороженного энкодера для диаризации и обучаемых компонентов — ASR-энкодера, speaker encoding слоя и RNNT-декодера. На вход модель получает аудио с несколькими спикерами и (опционально) короткий «query»-пример нужного говорящего. Выходом становится либо полная транскрипция с разметкой по спикерам (MS-ASR), либо только текст нужного говорящего (TS-ASR).<br><br>Ключевая часть архитектуры — блок speaker encoding. Он принимает ASR-эмбеддинги и вероятности по спикерам (из диаризации) и формирует многомерное представление, в котором каждый из каналов отвечает за конкретного спикера. Это представление затем поступает в декодер.<br><br>Авторы отдельно отмечают, что модель можно использовать и в сценарии, где нужно отслеживать только одного говорящего. В будущей работе авторы обещают поддержку стриминга.<br><br><em>Алексей Рак </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a></div>
      <div class="actions">
        <span>912 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/65" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/65.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="64" data-search="архитектура kws от яндекса: как колонка с алисой выбирает, куда слушать исследователи из яндекса представят на конференции interspeech 2025 в роттердаме статью multichannel keyword spotting for noisy conditions. мы поговорили с авторами и узнали, как устроена нейросетевая архитектура kws (keyword spotting), объединяющая два подхода: мультиканальный вход и attention-механизм для более точного распознавания голосовых команд в шумных помещениях. задача: распознавать команды в шуме в колонках есть задача голосовой активации — нужно, чтобы устройство услышало команду даже когда работает телевизор, пылесос или кто-то говорит фоном. для улучшения работы в таких условиях можно использовать алгоритмы шумоподавления, как, например, в zoom. однако даже если такие модели улучшают звук для пользователей, они могут заметно ухудшать качество для моделей. скорее всего, это происходит из-за того, что модели шумоподавления обучаются на синтетических данных, а модели голосовой активации — на реальных данных и учатся очищать шумы во внутреннем представлении сигнала. решение: несколько микрофонов получая информацию с нескольких микрофонов, можно сделать модель шумоподавления ощутимо лучше. микрофоны расположены в разных точках устройства, и звук доходит до каждого с небольшой задержкой — за счёт этого можно понимать, откуда он пришёл, и подавлять сигнал конкретного направления. в статье предлагается считать, что основной сигнал в каждый момент времени — это шум. и мы можем всегда очищать это направление. а чтобы услышать активационную фразу пользователя — «алиса» — для очистки использовать направления сигнала секунду назад. такой подход помогает убрать шумы, не затрагивая голосовую команду. но тогда возникает риск подавить голос пользователя, если он говорил до этого. поэтому канал после шумоподавления не используется отдельно, а подаётся вместе с обычным. чтобы модель могла выбрать между ними, добавили модуль attention. он получает оба сигнала, вычисляет веса для частот каждого канала и складывает их с этими весами. в итоге на вход основной модели поступает комбинированный сигнал. архитектура: svdf e2e + attention + anc в основе модели — svdf e2e. это базовая архитектура, которая уже используется в проде алисы. добавляются механизм внимания и блок адаптивного шумоподавления adaptive noise cancellation (anc). в статье эту архитектуру сравнивают с другими подходами. приходят к выводу, что, например, beamforming хоть и усиливает голос с нужного направления, но в целом работает хуже. ещё пробовали вариант с двумя отдельными моделями, где каждая обрабатывает свой канал, а активация срабатывает, если сработала хотя бы одна модель. такой способ даёт худшее качество и требует больше параметров. удивительнее всего было увидеть, что ensemble дает качество хуже, чем агрегация с помощью attention. модель не просто выбирает звуковой сигнал, который нужно слушать, а некоторую комбинацию из звуковых каналов. полученные результаты: frr — 5,5% при fa/h = 0,1. frr отвечает за отзывчивость, а fah — за ложные срабатывания, и в обоих случаях — чем меньше, тем лучше. этот результат превосходит beamforming (6,7%) и ensemble (6,4%). при этом модель остаётся компактной и не требует дополнительных ресурсов. обучение на двух датасетах модель обучалась и тестировалась на двух датасетах. лабораторный — содержит 900 симулированных шумовых сцен (улица, кухня, пылесос и прочее) с разными голосами и уровнями шума. полевой — это 10 млн анонимизированных примеров команд. технология, описанная в статье, проверена временем: она в проде с 2022 года и сейчас используется во всех современных колонках с алисой. архитектура хорошо масштабируется на edge-устройства — смарт-колонки и другие бытовые ии. в перспективе тот же механизм можно использовать, чтобы выбирать наилучший звуковой канал для передачи в облако. speech info архитектура kws от яндекса: как колонка с алисой выбирает, куда слушать исследователи из яндекса представят на конференции interspeech 2025 в роттердаме статью multichannel keyword spotting for noisy conditions . мы поговорили с авторами и узнали, как устроена нейросетевая архитектура kws (keyword spotting), объединяющая два подхода: мультиканальный вход и attention-механизм для более точного распознавания голосовых команд в шумных помещениях. задача: распознавать команды в шуме в колонках есть задача голосовой активации — нужно, чтобы устройство услышало команду даже когда работает телевизор, пылесос или кто-то говорит фоном. для улучшения работы в таких условиях можно использовать алгоритмы шумоподавления, как, например, в zoom. однако даже если такие модели улучшают звук для пользователей, они могут заметно ухудшать качество для моделей. скорее всего, это происходит из-за того, что модели шумоподавления обучаются на синтетических данных, а модели голосовой активации — на реальных данных и учатся очищать шумы во внутреннем представлении сигнала. решение: несколько микрофонов получая информацию с нескольких микрофонов, можно сделать модель шумоподавления ощутимо лучше. микрофоны расположены в разных точках устройства, и звук доходит до каждого с небольшой задержкой — за счёт этого можно понимать, откуда он пришёл, и подавлять сигнал конкретного направления. в статье предлагается считать, что основной сигнал в каждый момент времени — это шум. и мы можем всегда очищать это направление. а чтобы услышать активационную фразу пользователя — «алиса» — для очистки использовать направления сигнала секунду назад. такой подход помогает убрать шумы, не затрагивая голосовую команду. но тогда возникает риск подавить голос пользователя, если он говорил до этого. поэтому канал после шумоподавления не используется отдельно, а подаётся вместе с обычным. чтобы модель могла выбрать между ними, добавили модуль attention. он получает оба сигнала, вычисляет веса для частот каждого канала и складывает их с этими весами. в итоге на вход основной модели поступает комбинированный сигнал. архитектура: svdf e2e + attention + anc в основе модели — svdf e2e . это базовая архитектура, которая уже используется в проде алисы. добавляются механизм внимания и блок адаптивного шумоподавления adaptive noise cancellation (anc). в статье эту архитектуру сравнивают с другими подходами. приходят к выводу, что, например, beamforming хоть и усиливает голос с нужного направления, но в целом работает хуже. ещё пробовали вариант с двумя отдельными моделями, где каждая обрабатывает свой канал, а активация срабатывает, если сработала хотя бы одна модель. такой способ даёт худшее качество и требует больше параметров. удивительнее всего было увидеть, что ensemble дает качество хуже, чем агрегация с помощью attention. модель не просто выбирает звуковой сигнал, который нужно слушать, а некоторую комбинацию из звуковых каналов. полученные результаты: frr — 5,5% при fa/h = 0,1. frr отвечает за отзывчивость, а fah — за ложные срабатывания, и в обоих случаях — чем меньше, тем лучше. этот результат превосходит beamforming (6,7%) и ensemble (6,4%). при этом модель остаётся компактной и не требует дополнительных ресурсов. обучение на двух датасетах модель обучалась и тестировалась на двух датасетах. лабораторный — содержит 900 симулированных шумовых сцен (улица, кухня, пылесос и прочее) с разными голосами и уровнями шума. полевой — это 10 млн анонимизированных примеров команд. технология, описанная в статье, проверена временем: она в проде с 2022 года и сейчас используется во всех современных колонках с алисой. архитектура хорошо масштабируется на edge-устройства — смарт-колонки и другие бытовые ии. в перспективе тот же механизм можно использовать, чтобы выбирать наилучший звуковой канал для передачи в облако. speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-30T06:01:01+00:00" href="./posts/64.html">2025-07-30 06:01 UTC</a></div>
      </div>
      <div class="post-body"><strong>Архитектура KWS от Яндекса: как колонка с Алисой выбирает, куда слушать</strong><br><br>Исследователи из Яндекса представят на конференции <a href="https://www.interspeech2025.org/home" rel="nofollow noopener noreferrer">Interspeech 2025</a> в Роттердаме статью <a href="https://arxiv.org/abs/2507.15558" rel="nofollow noopener noreferrer">Multichannel Keyword Spotting for Noisy Conditions</a>. Мы поговорили с авторами и узнали, как устроена нейросетевая архитектура KWS (keyword spotting), объединяющая два подхода: мультиканальный вход и attention-механизм для более точного распознавания голосовых команд в шумных помещениях.<br><br><strong>Задача: распознавать команды в шуме </strong><br><br>В колонках есть задача голосовой активации — нужно, чтобы устройство услышало команду даже когда работает телевизор, пылесос или кто-то говорит фоном. Для улучшения работы в таких условиях можно использовать алгоритмы шумоподавления, как, например, в Zoom. Однако даже если такие модели улучшают звук для пользователей, они могут заметно ухудшать качество для моделей. Скорее всего, это происходит из-за того, что модели шумоподавления обучаются на синтетических данных, а модели голосовой активации — на реальных данных и учатся очищать шумы во внутреннем представлении сигнала.  <br><br><strong>Решение: несколько микрофонов</strong><br><br>Получая информацию с нескольких микрофонов, можно сделать модель шумоподавления ощутимо лучше. Микрофоны расположены в разных точках устройства, и звук доходит до каждого с небольшой задержкой — за счёт этого можно понимать, откуда он пришёл, и подавлять сигнал конкретного направления. <br><br>В статье предлагается считать, что основной сигнал в каждый момент времени — это шум. И мы можем всегда очищать это направление. А чтобы услышать активационную фразу пользователя — «Алиса» — для очистки использовать направления сигнала секунду назад. Такой подход помогает убрать шумы, не затрагивая голосовую команду.<br><br>Но тогда возникает риск подавить голос пользователя, если он говорил до этого. Поэтому канал после шумоподавления не используется отдельно, а подаётся вместе с обычным. Чтобы модель могла выбрать между ними, добавили модуль attention. Он получает оба сигнала, вычисляет веса для частот каждого канала и складывает их с этими весами. В итоге на вход основной модели поступает комбинированный сигнал.<br><strong><br>Архитектура: SVDF E2E + Attention + ANC</strong><br><br>В основе модели — <a href="https://arxiv.org/pdf/1812.02802" rel="nofollow noopener noreferrer">SVDF E2E</a>. Это базовая архитектура, которая уже используется в проде Алисы. Добавляются механизм внимания и блок адаптивного шумоподавления <a href="https://storage.googleapis.com/gweb-research2023-media/pubtools/5180.pdf" rel="nofollow noopener noreferrer">Adaptive Noise Cancellation</a> (ANC).<br><br>В статье эту архитектуру сравнивают с другими подходами. Приходят к выводу, что, например, Beamforming хоть и усиливает голос с нужного направления, но в целом работает хуже. Ещё пробовали вариант с двумя отдельными моделями, где каждая обрабатывает свой канал, а активация срабатывает, если сработала хотя бы одна модель. Такой способ даёт худшее качество и требует больше параметров. Удивительнее всего было увидеть, что Ensemble дает качество хуже, чем агрегация с помощью attention. Модель не просто выбирает звуковой сигнал, который нужно слушать, а некоторую комбинацию из звуковых каналов.<br><br>Полученные результаты: FRR — 5,5% при FA/h = 0,1. FRR отвечает за отзывчивость, а fah — за ложные срабатывания, и в обоих случаях — чем меньше, тем лучше. Этот результат превосходит Beamforming (6,7%) и Ensemble (6,4%). При этом модель остаётся компактной и не требует дополнительных ресурсов.<br><br><strong>Обучение на двух датасетах<br></strong><br>Модель обучалась и тестировалась на двух датасетах. Лабораторный — содержит 900 симулированных шумовых сцен (улица, кухня, пылесос и прочее) с разными голосами и уровнями шума. Полевой — это 10 млн анонимизированных примеров команд. <br><br>Технология, описанная в статье, проверена временем: она  в проде с 2022 года и сейчас используется во всех современных колонках с Алисой. Архитектура хорошо масштабируется на edge-устройства — смарт-колонки и другие бытовые ИИ. В перспективе тот же механизм можно использовать, чтобы выбирать наилучший звуковой канал для передачи в облако.<br><br><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer">Speech Info</a></div>
      <div class="actions">
        <span>1 131 просмотров · 30 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/64" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/64.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="63" data-search="aligner-encoders: self-attention transformers can be self-transducers можно ли выучить выравнивание между аудио и текстом без архитектур вроде rnn-t и без использования blank-токенов? авторы этой статьи считают, что можно. достаточно self-attention-энкодера с отдельной головой, которая решает, на каких шагах нужно выпускать токены. классические asr-модели (rnn-t, aed) формируют выравнивание во время декодирования: логиты зависят от возможных переходов по временной оси. это требует либо динамического программирования (в ctc), либо перебора всех допустимых путей (в rnn-t). в aligner-encoder модель учится решать, стоит ли выпускать токен на каждом аудиофрейме. в энкодер добавляют ff-слой (aligner head), обучаемый по меткам из ctc loss. принудительное выравнивание не требуется. токены добавляются только тогда, когда aligner говорит «да» — без использования blank-символов или графа выравнивания. модель не создаёт лишних гипотез, декодинг упрощается, сложность по памяти — существенно ниже: o(u×vocab) против o(u×t×vocab) у rnn-t. что касается архитектуры, энкодер состоит из 2d-свёрток и conformer-блоков (ffn, multi-head attention, 1d conv, residuals). вход — log-mel-спектрограммы (окно 32 мс, шаг 10 мс), токены — wordpiece, используется label smoothing (δ = 2/v), чтобы избежать смещения к коротким предсказаниям. начиная с 14-го слоя self-attention, первые текстовые токены начинают фокусироваться на соответствующих аудиофреймах — это можно проследить по диагональному паттерну. модель при этом «сдвигает» важные представления ближе к началу, сохраняя порядок токенов. в обычных энкодерах такого сдвига не происходит. модель обучалась на трёх англоязычных датасетах: — librispeech (960 часов), — voice search, — youtube (670 тысяч часов псевдозаписей длиной 5–15 секунд). для оценки на youtube выделили 30 часов 8-минутных аудиофрагментов (по 15 часов на валидацию и тест). модель показывает точность на уровне ctc-базлайна на librispeech и превосходит его на youtube. авторы также проверяют, можно ли использовать обученный aligner в других моделях. в одном из экспериментов инициализируют rnn-t слоями из выученного энкодера и получают улучшение по метрикам. это показывает, что aligner-head может использоваться как самостоятельный механизм выравнивания. несмотря на то, что в названии статьи сделан акцент на выравнивание, главная польза модели — в скорости и простоте. в сравнительном эксперименте все модели были одного размера (100 млн параметров). на обучении aligner оказался в 10 раз быстрее rnn-t (29 мс против 290 мс на шаг), главным образом за счёт отказа от сканирования по временной оси в join-сети. это также позволило снизить пиковое потребление памяти на 18 % (−1.4 гб). на инференсе модель тоже самая быстрая: каждый шаг декодера занимает 0,19 мс против 8,5 мс у aed. общая сложность — o(u), тогда как у rnn-t — o(u+t), где u — длина текста, t — длина аудио. переупорядочивание гипотез в beam почти не требуется. отдельно подчёркивается, что хоть aed и делает шаги почти так же быстро, как aligner, благодаря трансформерной природе он сходится за меньшее число итераций. илья новицкий ❣ специально для speech info aligner-encoders: self-attention transformers can be self-transducers можно ли выучить выравнивание между аудио и текстом без архитектур вроде rnn-t и без использования blank-токенов? авторы этой статьи считают, что можно. достаточно self-attention-энкодера с отдельной головой, которая решает, на каких шагах нужно выпускать токены. классические asr-модели (rnn-t, aed) формируют выравнивание во время декодирования: логиты зависят от возможных переходов по временной оси. это требует либо динамического программирования (в ctc), либо перебора всех допустимых путей (в rnn-t). в aligner-encoder модель учится решать, стоит ли выпускать токен на каждом аудиофрейме. в энкодер добавляют ff-слой (aligner head), обучаемый по меткам из ctc loss. принудительное выравнивание не требуется. токены добавляются только тогда, когда aligner говорит «да» — без использования blank-символов или графа выравнивания. модель не создаёт лишних гипотез, декодинг упрощается, сложность по памяти — существенно ниже: o(u×vocab) против o(u×t×vocab) у rnn-t. что касается архитектуры, энкодер состоит из 2d-свёрток и conformer-блоков (ffn, multi-head attention, 1d conv, residuals). вход — log-mel-спектрограммы (окно 32 мс, шаг 10 мс), токены — wordpiece, используется label smoothing (δ = 2/v), чтобы избежать смещения к коротким предсказаниям. начиная с 14-го слоя self-attention, первые текстовые токены начинают фокусироваться на соответствующих аудиофреймах — это можно проследить по диагональному паттерну. модель при этом «сдвигает» важные представления ближе к началу, сохраняя порядок токенов. в обычных энкодерах такого сдвига не происходит. модель обучалась на трёх англоязычных датасетах: — librispeech (960 часов), — voice search, — youtube (670 тысяч часов псевдозаписей длиной 5–15 секунд). для оценки на youtube выделили 30 часов 8-минутных аудиофрагментов (по 15 часов на валидацию и тест). модель показывает точность на уровне ctc-базлайна на librispeech и превосходит его на youtube. авторы также проверяют, можно ли использовать обученный aligner в других моделях. в одном из экспериментов инициализируют rnn-t слоями из выученного энкодера и получают улучшение по метрикам. это показывает, что aligner-head может использоваться как самостоятельный механизм выравнивания. несмотря на то, что в названии статьи сделан акцент на выравнивание, главная польза модели — в скорости и простоте. в сравнительном эксперименте все модели были одного размера (100 млн параметров). на обучении aligner оказался в 10 раз быстрее rnn-t (29 мс против 290 мс на шаг), главным образом за счёт отказа от сканирования по временной оси в join-сети. это также позволило снизить пиковое потребление памяти на 18 % (−1.4 гб). на инференсе модель тоже самая быстрая: каждый шаг декодера занимает 0,19 мс против 8,5 мс у aed. общая сложность — o(u), тогда как у rnn-t — o(u+t), где u — длина текста, t — длина аудио. переупорядочивание гипотез в beam почти не требуется. отдельно подчёркивается, что хоть aed и делает шаги почти так же быстро, как aligner, благодаря трансформерной природе он сходится за меньшее число итераций. илья новицкий ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-25T09:04:17+00:00" href="./posts/63.html">2025-07-25 09:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>Aligner-Encoders: Self-Attention Transformers Can Be Self-Transducers</strong><br><br>Можно ли выучить выравнивание между аудио и текстом без архитектур вроде RNN-T и без использования blank-токенов? Авторы этой <a href="https://arxiv.org/abs/2502.05232" rel="nofollow noopener noreferrer">статьи </a>считают, что можно. Достаточно self-attention-энкодера с отдельной головой, которая решает, на каких шагах нужно выпускать токены.<br><br>Классические ASR-модели (RNN-T, AED) формируют выравнивание во время декодирования: логиты зависят от возможных переходов по временной оси. Это требует либо динамического программирования (в CTC), либо перебора всех допустимых путей (в RNN-T). В Aligner-Encoder модель учится решать, стоит ли выпускать токен на каждом аудиофрейме. В энкодер добавляют FF-слой (aligner head), обучаемый по меткам из CTC loss. Принудительное выравнивание не требуется.<br><br>Токены добавляются только тогда, когда aligner говорит «да» — без использования blank-символов или графа выравнивания. Модель не создаёт лишних гипотез, декодинг упрощается, сложность по памяти — существенно ниже: O(U×Vocab) против O(U×T×Vocab) у RNN-T.<br><br>Что касается архитектуры, энкодер состоит из 2D-свёрток и Conformer-блоков (FFN, multi-head attention, 1D conv, residuals). Вход — log-mel-спектрограммы (окно 32 мс, шаг 10 мс), токены — WordPiece, используется label smoothing (δ = 2/V), чтобы избежать смещения к коротким предсказаниям.<br><br>Начиная с 14-го слоя self-attention, первые текстовые токены начинают фокусироваться на соответствующих аудиофреймах — это можно проследить по диагональному паттерну. Модель при этом «сдвигает» важные представления ближе к началу, сохраняя порядок токенов. В обычных энкодерах такого сдвига не происходит.<br><br>Модель обучалась на трёх англоязычных датасетах:<br>— LibriSpeech (960 часов),<br>— Voice Search,<br>— YouTube (670 тысяч часов псевдозаписей длиной 5–15 секунд).<br><br>Для оценки на YouTube выделили 30 часов 8-минутных аудиофрагментов (по 15 часов на валидацию и тест). Модель показывает точность на уровне CTC-базлайна на LibriSpeech и превосходит его на YouTube.<br><br>Авторы также проверяют, можно ли использовать обученный aligner в других моделях. В одном из экспериментов инициализируют RNN-T слоями из выученного энкодера  и получают улучшение по метрикам. Это показывает, что aligner-head может использоваться как самостоятельный механизм выравнивания.<br><br>Несмотря на то, что в названии статьи сделан акцент на выравнивание, главная польза модели — в скорости и простоте. В сравнительном эксперименте все модели были одного размера (100 млн параметров). На обучении Aligner оказался в 10 раз быстрее RNN-T (29 мс против 290 мс на шаг), главным образом за счёт отказа от сканирования по временной оси в join-сети. Это также позволило снизить пиковое потребление памяти на 18 % (−1.4 ГБ). На инференсе модель тоже самая быстрая: каждый шаг декодера занимает 0,19 мс против 8,5 мс у AED. Общая сложность — O(U), тогда как у RNN-T — O(U+T), где U — длина текста, T — длина аудио. Переупорядочивание гипотез в beam почти не требуется. Отдельно подчёркивается, что хоть AED и делает шаги почти так же быстро, как Aligner, благодаря трансформерной природе он сходится за меньшее число итераций.<br><br><em>Илья Новицкий </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/63_480.webp" srcset="../assets/media/thumbs/63_480.webp 480w, ../assets/media/63.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="63" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 073 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/63" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/63.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link disabled" href="#">←</a>
        <a class="page-link current" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a>
        <a class="nav-link" href="page-2.html">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 118, "media": [{"kind": "photo", "path": "../assets/media/118.jpg", "thumb": "../assets/media/thumbs/118_480.webp", "size": 49002, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/119.jpg", "thumb": "../assets/media/thumbs/119_480.webp", "size": 59788, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/120.jpg", "thumb": "../assets/media/thumbs/120_480.webp", "size": 60592, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/121.jpg", "thumb": "../assets/media/thumbs/121_480.webp", "size": 51191, "mime": "image/jpeg", "name": null}]}, {"id": 115, "media": [{"kind": "photo", "path": "../assets/media/115.jpg", "thumb": "../assets/media/thumbs/115_480.webp", "size": 91492, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/116.jpg", "thumb": "../assets/media/thumbs/116_480.webp", "size": 58690, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/117.jpg", "thumb": "../assets/media/thumbs/117_480.webp", "size": 71366, "mime": "image/jpeg", "name": null}]}, {"id": 112, "media": [{"kind": "photo", "path": "../assets/media/112.jpg", "thumb": "../assets/media/thumbs/112_480.webp", "size": 59449, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/113.jpg", "thumb": "../assets/media/thumbs/113_480.webp", "size": 77568, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/114.jpg", "thumb": "../assets/media/thumbs/114_480.webp", "size": 21734, "mime": "image/jpeg", "name": null}]}, {"id": 111, "media": []}, {"id": 110, "media": []}, {"id": 109, "media": []}, {"id": 106, "media": [{"kind": "photo", "path": "../assets/media/106.jpg", "thumb": "../assets/media/thumbs/106_480.webp", "size": 20153, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/107.jpg", "thumb": "../assets/media/thumbs/107_480.webp", "size": 34375, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/108.jpg", "thumb": "../assets/media/thumbs/108_480.webp", "size": 79415, "mime": "image/jpeg", "name": null}]}, {"id": 105, "media": [{"kind": "photo", "path": "../assets/media/105.jpg", "thumb": "../assets/media/thumbs/105_480.webp", "size": 62266, "mime": "image/jpeg", "name": null}]}, {"id": 104, "media": []}, {"id": 103, "media": [{"kind": "photo", "path": "../assets/media/103.jpg", "thumb": "../assets/media/thumbs/103_480.webp", "size": 140797, "mime": "image/jpeg", "name": null}]}, {"id": 100, "media": [{"kind": "photo", "path": "../assets/media/100.jpg", "thumb": "../assets/media/thumbs/100_480.webp", "size": 116831, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/101.jpg", "thumb": "../assets/media/thumbs/101_480.webp", "size": 50256, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/102.jpg", "thumb": "../assets/media/thumbs/102_480.webp", "size": 99872, "mime": "image/jpeg", "name": null}]}, {"id": 97, "media": [{"kind": "photo", "path": "../assets/media/97.jpg", "thumb": "../assets/media/thumbs/97_480.webp", "size": 67565, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/98.jpg", "thumb": "../assets/media/thumbs/98_480.webp", "size": 55934, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/99.jpg", "thumb": "../assets/media/thumbs/99_480.webp", "size": 68996, "mime": "image/jpeg", "name": null}]}, {"id": 95, "media": [{"kind": "photo", "path": "../assets/media/95.jpg", "thumb": "../assets/media/thumbs/95_480.webp", "size": 49640, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/96.jpg", "thumb": "../assets/media/thumbs/96_480.webp", "size": 54981, "mime": "image/jpeg", "name": null}]}, {"id": 94, "media": []}, {"id": 93, "media": [{"kind": "photo", "path": "../assets/media/93.jpg", "thumb": "../assets/media/thumbs/93_480.webp", "size": 68452, "mime": "image/jpeg", "name": null}]}, {"id": 90, "media": [{"kind": "photo", "path": "../assets/media/90.jpg", "thumb": "../assets/media/thumbs/90_480.webp", "size": 46283, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/91.jpg", "thumb": "../assets/media/thumbs/91_480.webp", "size": 57369, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/92.jpg", "thumb": "../assets/media/thumbs/92_480.webp", "size": 114546, "mime": "image/jpeg", "name": null}]}, {"id": 89, "media": []}, {"id": 87, "media": [{"kind": "photo", "path": "../assets/media/87.jpg", "thumb": "../assets/media/thumbs/87_480.webp", "size": 52427, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/88.jpg", "thumb": "../assets/media/thumbs/88_480.webp", "size": 86341, "mime": "image/jpeg", "name": null}]}, {"id": 78, "media": [{"kind": "photo", "path": "../assets/media/78.jpg", "thumb": "../assets/media/thumbs/78_480.webp", "size": 88284, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/79.jpg", "thumb": "../assets/media/thumbs/79_480.webp", "size": 168634, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/80.jpg", "thumb": "../assets/media/thumbs/80_480.webp", "size": 169529, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/81.jpg", "thumb": "../assets/media/thumbs/81_480.webp", "size": 174046, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/82.jpg", "thumb": "../assets/media/thumbs/82_480.webp", "size": 145828, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/83.jpg", "thumb": "../assets/media/thumbs/83_480.webp", "size": 170751, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/84.jpg", "thumb": "../assets/media/thumbs/84_480.webp", "size": 75071, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/85.jpg", "thumb": "../assets/media/thumbs/85_480.webp", "size": 93076, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/86.jpg", "thumb": "../assets/media/thumbs/86_480.webp", "size": 94042, "mime": "image/jpeg", "name": null}]}, {"id": 77, "media": [{"kind": "photo", "path": "../assets/media/77.jpg", "thumb": "../assets/media/thumbs/77_480.webp", "size": 93233, "mime": "image/jpeg", "name": null}]}, {"id": 76, "media": []}, {"id": 75, "media": [{"kind": "photo", "path": "../assets/media/75.jpg", "thumb": "../assets/media/thumbs/75_480.webp", "size": 83866, "mime": "image/jpeg", "name": null}]}, {"id": 74, "media": [{"kind": "photo", "path": "../assets/media/74.jpg", "thumb": "../assets/media/thumbs/74_480.webp", "size": 27785, "mime": "image/jpeg", "name": null}]}, {"id": 71, "media": [{"kind": "video", "path": "../assets/media/71_37820807.mp4", "thumb": null, "size": 10009739, "mime": "video/mp4", "name": "37820807.mp4"}, {"kind": "photo", "path": "../assets/media/72.jpg", "thumb": "../assets/media/thumbs/72_480.webp", "size": 192765, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/73.jpg", "thumb": "../assets/media/thumbs/73_480.webp", "size": 226607, "mime": "image/jpeg", "name": null}]}, {"id": 69, "media": [{"kind": "photo", "path": "../assets/media/69.jpg", "thumb": "../assets/media/thumbs/69_480.webp", "size": 130099, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/70.jpg", "thumb": "../assets/media/thumbs/70_480.webp", "size": 101676, "mime": "image/jpeg", "name": null}]}, {"id": 67, "media": [{"kind": "photo", "path": "../assets/media/67.jpg", "thumb": "../assets/media/thumbs/67_480.webp", "size": 136980, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/68.jpg", "thumb": "../assets/media/thumbs/68_480.webp", "size": 145104, "mime": "image/jpeg", "name": null}]}, {"id": 66, "media": [{"kind": "photo", "path": "../assets/media/66.jpg", "thumb": "../assets/media/thumbs/66_480.webp", "size": 50189, "mime": "image/jpeg", "name": null}]}, {"id": 65, "media": []}, {"id": 64, "media": []}, {"id": 63, "media": [{"kind": "photo", "path": "../assets/media/63.jpg", "thumb": "../assets/media/thumbs/63_480.webp", "size": 71086, "mime": "image/jpeg", "name": null}]}];
    window.__STATIC_META = {"title": "Speech Info", "username": "speechinfo", "channel": "speechinfo", "last_sync_utc": "2026-02-10T21:28:54Z", "posts_count": 57, "last_seen_message_id": 121, "stats": {"new": 60, "updated": 0, "media_downloaded": 60}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
