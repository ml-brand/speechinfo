<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Speech Info — статическая версия (стр. 2/2)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-02-03T23%3A59%3A03Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-02-03T23%3A59%3A03Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-02-03T23%3A59%3A03Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">Speech Info</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+0VCG1pUgLahlZGNi" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="index.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link current" href="page-2.html">2</a>
        <a class="nav-link disabled" href="#">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="50" data-search="как tortoisetts изменил правила игры в синтезе речи в 2017–2019 годах tts-сообщество начало учиться на заметно больших объёмах данных, чем до этого, и выбирало между двумя классами моделей, которые можно обучать. первый вариант — взять трансформер из nlp и научить его предсказывать аудиотокены. второй — использовать диффузии, как в генерации изображений, и генерировать спектрограммы. оба варианта давали частичный результат. диффузии хорошо передавали голос, но интонацию — слабо. трансформеры, наоборот, хорошо моделировали интонацию, но теряли сходство с голосом. в 2022 году инженер джеймс беткер, много лет проработавший в garmin, решил пойти другим путём. в одиночку он начал собирать свою модель дома! он купил восемь б/у rtx 3090, которые до этого майнили крипту, собрал из них кластер, и два года вёл блог, в котором подробно описывал каждую итерацию своего ресёрча. формат — не статьи с графиками, а markdown и картинки от руки. иногда — просто пост с одной мыслью и решением. спустя два года такой работы появился tortoisetts. гибрид трансформера и диффузии, вдохновленный архитектурой dall-e 1, только вместо изображений — аудио. что сделал беткер: 1. сначала — кодек. беткер обучил vq-vae, который сжимал аудио до 25 токенов в секунду. это довольно маленький битрейт для получения качественного восстановления, но такой цели и не стояло — кодек нужен был как интерфейс между аудио и трансформером. 2. потом — трансформер. модель по тексту и примеру голоса, который надо скопировать, предсказывала токены этого кодека. поскольку токенов было мало, он мог использовать большие батчи, что критично при ограниченных ресурсах. но важнее другое: трансформер давал не только токены, но и латенты, из которых эти токены предсказываются. (они понадобятся нам на следующем этапе.) именно этот кусок пайплайна отвечает за генерацию правильной похожести голоса и разумные интонации. 3. дальше — диффузия. по примеру голоса и сгенерированным gpt латентам она предсказывала спектрограмму. задача этой модельки в том, чтобы получившаяся запись была качественной и хорошо сохранилась похожесть голоса. 4. в конце — предобученный вокодер univnet, который предсказывает аудио по спектрограмме. в результате получился пайплайн: текст → латента (из gpt) → спектрограмма (из диффузии) → аудио. в модельке есть ещё несколько хаков, но мы не останавливаемся на них, чтобы упростить повествование. за счёт комбинации gpt и диффузии этому способу удалось усесться на оба стула: он хорошо моделировал интонацию и клонировал голос. и это обучилось на кластере из восьми видеокарт! после выхода tortoise в 2022 году появилось много похожих работ. например: — cosyvoice (alibaba): заменили vq-vae на кодек, обученный на asr-задаче, чтобы эмбеддинги хранили больше информации о речи. вместо диффузии применили flow matching — он даёт звук быстрее за меньшее число шагов. — seed-tts (bytedance): заменили токенизатор, отказались от clvp, дообучили модель на 5 голосах. — base tts (amazon): взяли большую gpt (1b параметров вместо 300m) и использовали токены из wavlm. получается, что tortoise стал не столько готовым решением, сколько универсальной схемой, которую подхватили сразу несколько крупных команд. ну а джеймса беткера наняли в openai сразу после релиза финальной версии модели. эта история — только малая часть интересного из speech сourse от яндекса и шад. все лекции, слайды, домашки и полезные ссылки выложены на github. курс охватывает биометрию, распознавание, синтез, шумоподавление. а на тему tts — целых четыре лекции. роман кайль ❣ специально для speech info как tortoisetts изменил правила игры в синтезе речи в 2017–2019 годах tts-сообщество начало учиться на заметно больших объёмах данных, чем до этого, и выбирало между двумя классами моделей, которые можно обучать. первый вариант — взять трансформер из nlp и научить его предсказывать аудиотокены. второй — использовать диффузии, как в генерации изображений, и генерировать спектрограммы. оба варианта давали частичный результат. диффузии хорошо передавали голос, но интонацию — слабо. трансформеры, наоборот, хорошо моделировали интонацию, но теряли сходство с голосом. в 2022 году инженер джеймс беткер, много лет проработавший в garmin, решил пойти другим путём. в одиночку он начал собирать свою модель дома! он купил восемь б/у rtx 3090, которые до этого майнили крипту, собрал из них кластер, и два года вёл блог , в котором подробно описывал каждую итерацию своего ресёрча. формат — не статьи с графиками, а markdown и картинки от руки. иногда — просто пост с одной мыслью и решением. спустя два года такой работы появился tortoisetts. гибрид трансформера и диффузии, вдохновленный архитектурой dall-e 1, только вместо изображений — аудио. что сделал беткер: 1. сначала — кодек. беткер обучил vq-vae, который сжимал аудио до 25 токенов в секунду. это довольно маленький битрейт для получения качественного восстановления, но такой цели и не стояло — кодек нужен был как интерфейс между аудио и трансформером. 2. потом — трансформер. модель по тексту и примеру голоса, который надо скопировать, предсказывала токены этого кодека. поскольку токенов было мало, он мог использовать большие батчи, что критично при ограниченных ресурсах. но важнее другое: трансформер давал не только токены, но и латенты, из которых эти токены предсказываются. (они понадобятся нам на следующем этапе.) именно этот кусок пайплайна отвечает за генерацию правильной похожести голоса и разумные интонации. 3. дальше — диффузия. по примеру голоса и сгенерированным gpt латентам она предсказывала спектрограмму. задача этой модельки в том, чтобы получившаяся запись была качественной и хорошо сохранилась похожесть голоса. 4. в конце — предобученный вокодер univnet, который предсказывает аудио по спектрограмме. в результате получился пайплайн: текст → латента (из gpt) → спектрограмма (из диффузии) → аудио. в модельке есть ещё несколько хаков, но мы не останавливаемся на них, чтобы упростить повествование. за счёт комбинации gpt и диффузии этому способу удалось усесться на оба стула: он хорошо моделировал интонацию и клонировал голос. и это обучилось на кластере из восьми видеокарт! после выхода tortoise в 2022 году появилось много похожих работ. например: — cosyvoice (alibaba): заменили vq-vae на кодек, обученный на asr-задаче, чтобы эмбеддинги хранили больше информации о речи. вместо диффузии применили flow matching — он даёт звук быстрее за меньшее число шагов. — seed-tts (bytedance): заменили токенизатор, отказались от clvp, дообучили модель на 5 голосах. — base tts (amazon): взяли большую gpt (1b параметров вместо 300m) и использовали токены из wavlm. получается, что tortoise стал не столько готовым решением, сколько универсальной схемой, которую подхватили сразу несколько крупных команд. ну а джеймса беткера наняли в openai сразу после релиза финальной версии модели. эта история — только малая часть интересного из speech сourse от яндекса и шад. все лекции, слайды, домашки и полезные ссылки выложены на github. курс охватывает биометрию, распознавание, синтез, шумоподавление. а на тему tts — целых четыре лекции. роман кайль ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-10T08:05:37+00:00" href="./posts/50.html">2025-07-10 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как TortoiseTTS изменил правила игры в синтезе речи</strong><br><br>В 2017–2019 годах TTS-сообщество начало учиться на заметно больших объёмах данных, чем до этого, и выбирало между двумя классами моделей, которые можно обучать. Первый вариант — взять трансформер из NLP и научить его предсказывать аудиотокены. Второй — использовать диффузии, как в генерации изображений, и генерировать спектрограммы. Оба варианта давали частичный результат. Диффузии хорошо передавали голос, но интонацию — слабо. Трансформеры, наоборот, хорошо моделировали интонацию, но теряли сходство с голосом.<br><br>В 2022 году инженер Джеймс Беткер, много лет <a href="https://www.linkedin.com/in/james-betker-4a051013/" rel="nofollow noopener noreferrer">проработавший</a> в Garmin, решил пойти другим путём. В одиночку он начал собирать свою модель дома! Он купил восемь б/у RTX 3090, которые до этого майнили крипту, собрал из них кластер, и два года вёл <a href="https://nonint.com/2022/page/2/" rel="nofollow noopener noreferrer">блог</a>, в котором подробно описывал каждую итерацию своего ресёрча. Формат — не статьи с графиками, а Markdown и картинки от руки. Иногда — просто пост с одной мыслью и решением.<br><br>Спустя два года такой работы появился TortoiseTTS. Гибрид трансформера и диффузии, вдохновленный архитектурой DALL-e 1, только вместо изображений — аудио.<br><br><strong>Что сделал Беткер:</strong><br><strong><br></strong>1. Сначала — кодек. Беткер обучил VQ-VAE, который сжимал аудио до 25 токенов в секунду. Это довольно маленький битрейт для получения качественного восстановления, но такой цели и не стояло — кодек нужен был как интерфейс между аудио и трансформером.<br><br>2. Потом — трансформер. Модель по тексту и примеру голоса, который надо скопировать, предсказывала токены этого кодека. Поскольку токенов было мало, он мог использовать большие батчи, что критично при ограниченных ресурсах. Но важнее другое: трансформер давал не только токены, но и латенты, из которых эти токены предсказываются. (Они понадобятся нам на следующем этапе.) Именно этот кусок пайплайна отвечает за генерацию правильной похожести голоса и разумные интонации.<br><br>3. Дальше — диффузия. По примеру голоса и сгенерированным GPT латентам она предсказывала спектрограмму. Задача этой модельки в том, чтобы получившаяся запись была качественной и хорошо сохранилась похожесть голоса.<br><br>4. В конце — предобученный вокодер UnivNet, который предсказывает аудио по спектрограмме.<br><br>В результате получился пайплайн: текст → латента (из GPT) → спектрограмма (из диффузии) → аудио. В модельке есть ещё несколько хаков, но мы не останавливаемся на них, чтобы упростить повествование. <br><br>За счёт комбинации GPT и диффузии этому способу удалось усесться на оба стула: он хорошо моделировал интонацию и клонировал голос. И это обучилось на кластере из восьми видеокарт!<br><br>После выхода Tortoise в 2022 году появилось много похожих работ. Например:<br><br>— <a href="https://arxiv.org/abs/2407.05407" rel="nofollow noopener noreferrer">CosyVoice</a> (Alibaba): заменили VQ-VAE на кодек, обученный на ASR-задаче, чтобы эмбеддинги хранили больше информации о речи. Вместо диффузии применили Flow Matching — он даёт звук быстрее за меньшее число шагов.<br> — <a href="https://arxiv.org/abs/2406.02430" rel="nofollow noopener noreferrer">Seed-TTS</a> (ByteDance): заменили токенизатор, отказались от CLVP, дообучили модель на 5 голосах.<br> — <a href="https://arxiv.org/abs/2402.08093" rel="nofollow noopener noreferrer">Base TTS</a> (Amazon): взяли большую GPT (1B параметров вместо 300M) и использовали токены из WavLM.<br><br>Получается, что Tortoise стал не столько готовым решением, сколько универсальной схемой, которую подхватили сразу несколько крупных команд. Ну а Джеймса Беткера наняли в OpenAI сразу после релиза финальной версии модели.<br><br>Эта история — только малая часть интересного из <a href="https://github.com/yandexdataschool/speech_course" rel="nofollow noopener noreferrer">Speech Сourse</a> от Яндекса и ШАД. Все лекции, слайды, домашки и полезные ссылки выложены на GitHub. Курс охватывает биометрию, распознавание, синтез, шумоподавление. А на тему TTS — целых четыре лекции. <br><br><em>Роман Кайль</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/50_480.webp" srcset="../assets/media/thumbs/50_480.webp 480w, ../assets/media/50.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="50" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/51_480.webp" srcset="../assets/media/thumbs/51_480.webp 480w, ../assets/media/51.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="50" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/52_480.webp" srcset="../assets/media/thumbs/52_480.webp 480w, ../assets/media/52.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="50" data-image-index="2" /></div></div>
      <div class="actions">
        <span>912 просмотров · 45 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/50" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/50.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="46" data-search="обзор статей с icassp 25. часть 3: llm для улучшения в asr две статьи от meta* с icassp 2025 показывают, как сократить число вызовов декодера в asr: в одной модель сразу предсказывает несколько токенов, в другой — принимает только те, у которых логиты выше порога. разбираем, как устроены эти методы и как они влияют на скорость и wer. efficient streaming llm for speech recognition статья о стриминговой asr-модели speechllm-xl. её архитектура состоит из двух компонентов: аудиоэнкодера и llm-декодера. на вход декодеру одновременно подаются выходы аудиоэнкодера (как в обычной asr) и токены, которые декодер уже успел предсказать. в обычном последовательном предсказании на вход сразу передаются звуковые токены, а затем модель догенерирует предсказание по одному токену — и каждый раз сгенерированный токен добавляется ко входу. таким образом модель работает в decoder-only-режиме. основная сложность со стримингом в том, что нужно попеременно передавать новый полученный звук и текстовые токены, которые предсказала модель. это делается следующим образом. во время инференса модель обрабатывает аудио по чанкам. после каждого декодер может сгенерировать не один токен, а сразу несколько — пока не встретит специальный маркер конца предсказания. это позволяет системе работать в стриминге и не откладывать вывод до самого конца. такой режим авторы реализуют через модифицированную схему тренировки, где модель учат предсказывать текст по частичному аудиоконтексту. особенность speechllm-xl — в устройстве генерации: в каждый момент llm-декодер видит и текущий аудиочанк, и собственные предыдущие предсказания. это позволяет ему лучше моделировать зависимость между звучанием и текстом, особенно в условиях ограниченного контекста. авторы сравнивают свою модель с другими стриминговыми asr-решениями. по качеству speechllm-xl обходит все перечисленные в работе бейзлайны на dev-наборах librispeech. например, на clean-части она показывает wer 2,5% против 2,9% у transducer и 2,7% у reallm при схожем размере чанка и lookahead — хотя сложно не заметить, что в целом скоры у бейзлайнов великоваты. faster speech-llama with multi-token prediction авторы пытаются ускорить llm-декодер в asr. идея в том, что вместо генерации одного токена за раз, как в обычной llm, они учат декодер предсказывать сразу несколько токенов. чтобы не вызывать llm отдельно для каждого из них, добавляют в декодер несколько «голов» — по числу токенов, которые нужно предсказать. эти головы работают параллельно: каждая предсказывает свой токен, зная предыдущие. получается схема из трёх шагов: 1) predict: модель сразу предсказывает k токенов. 2) verify: среди них ищем самую длинную префикс-последовательность, которую можно подтвердить более строгим one-step-декодером. 3) accept: принимаем только подтверждённые токены и продолжаем с новой гипотезой. это позволяет сократить число вызовов декодера без сильной потери качества. на графике видно, как число вызовов на слово (ось y) падает, особенно при 4–6 головах, а качество (wer по оси x) остаётся на уровне. лучший компромисс — 4 головы: ускорение ×2, при этом wer почти не растёт. верификацию авторы реализуют двумя способами: — по порогу вероятности; — по позиции гипотезы в top-n (например, если гипотеза оказалась в топ-5, то её можно принять). интересно, что при увеличении числа голов качество даже немного улучшалось. хотя авторы отмечают это только на librispeech, а на других датасетах наблюдается небольшая просадка. по сути, это доработка идеи deepseek: там тоже пробовали multi-token prediction, но здесь её применили именно в asr. алексей рак ❣ специально для speech info * компания meta признана экстремистской; её деятельность в россии запрещена. обзор статей с icassp 25. часть 3: llm для улучшения в asr две статьи от meta* с icassp 2025 показывают, как сократить число вызовов декодера в asr: в одной модель сразу предсказывает несколько токенов, в другой — принимает только те, у которых логиты выше порога. разбираем, как устроены эти методы и как они влияют на скорость и wer. efficient streaming llm for speech recognition статья о стриминговой asr-модели speechllm-xl. её архитектура состоит из двух компонентов: аудиоэнкодера и llm-декодера. на вход декодеру одновременно подаются выходы аудиоэнкодера (как в обычной asr) и токены, которые декодер уже успел предсказать. в обычном последовательном предсказании на вход сразу передаются звуковые токены, а затем модель догенерирует предсказание по одному токену — и каждый раз сгенерированный токен добавляется ко входу. таким образом модель работает в decoder-only-режиме. основная сложность со стримингом в том, что нужно попеременно передавать новый полученный звук и текстовые токены, которые предсказала модель. это делается следующим образом. во время инференса модель обрабатывает аудио по чанкам. после каждого декодер может сгенерировать не один токен, а сразу несколько — пока не встретит специальный маркер конца предсказания. это позволяет системе работать в стриминге и не откладывать вывод до самого конца. такой режим авторы реализуют через модифицированную схему тренировки, где модель учат предсказывать текст по частичному аудиоконтексту. особенность speechllm-xl — в устройстве генерации: в каждый момент llm-декодер видит и текущий аудиочанк, и собственные предыдущие предсказания. это позволяет ему лучше моделировать зависимость между звучанием и текстом, особенно в условиях ограниченного контекста. авторы сравнивают свою модель с другими стриминговыми asr-решениями. по качеству speechllm-xl обходит все перечисленные в работе бейзлайны на dev-наборах librispeech. например, на clean-части она показывает wer 2,5% против 2,9% у transducer и 2,7% у reallm при схожем размере чанка и lookahead — хотя сложно не заметить, что в целом скоры у бейзлайнов великоваты. faster speech-llama with multi-token prediction авторы пытаются ускорить llm-декодер в asr. идея в том, что вместо генерации одного токена за раз, как в обычной llm, они учат декодер предсказывать сразу несколько токенов. чтобы не вызывать llm отдельно для каждого из них, добавляют в декодер несколько «голов» — по числу токенов, которые нужно предсказать. эти головы работают параллельно: каждая предсказывает свой токен, зная предыдущие. получается схема из трёх шагов: 1) predict: модель сразу предсказывает k токенов. 2) verify: среди них ищем самую длинную префикс-последовательность, которую можно подтвердить более строгим one-step-декодером. 3) accept: принимаем только подтверждённые токены и продолжаем с новой гипотезой. это позволяет сократить число вызовов декодера без сильной потери качества. на графике видно, как число вызовов на слово (ось y) падает, особенно при 4–6 головах, а качество (wer по оси x) остаётся на уровне. лучший компромисс — 4 головы: ускорение ×2, при этом wer почти не растёт. верификацию авторы реализуют двумя способами: — по порогу вероятности; — по позиции гипотезы в top-n (например, если гипотеза оказалась в топ-5, то её можно принять). интересно, что при увеличении числа голов качество даже немного улучшалось. хотя авторы отмечают это только на librispeech, а на других датасетах наблюдается небольшая просадка. по сути, это доработка идеи deepseek: там тоже пробовали multi-token prediction, но здесь её применили именно в asr. алексей рак ❣ специально для speech info * компания meta признана экстремистской; её деятельность в россии запрещена.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-04T08:43:36+00:00" href="./posts/46.html">2025-07-04 08:43 UTC</a></div>
      </div>
      <div class="post-body"><strong>Обзор статей с ICASSP 25. Часть 3: LLM для улучшения в ASR</strong><br><br>Две статьи от Meta* с ICASSP 2025 показывают, как сократить число вызовов декодера в ASR: в одной модель сразу предсказывает несколько токенов, в другой — принимает только те, у которых логиты выше порога. Разбираем, как устроены эти методы и как они влияют на скорость и WER.<br><br><a href="https://arxiv.org/abs/2410.03752" rel="nofollow noopener noreferrer"><strong>Efficient Streaming LLM for Speech Recognition</strong></a><br><br>Статья о стриминговой ASR-модели SpeechLLM-XL. Её архитектура состоит из двух компонентов: аудиоэнкодера и LLM-декодера. На вход декодеру одновременно подаются выходы аудиоэнкодера (как в обычной ASR) и токены, которые декодер уже успел предсказать.<br><br>В обычном последовательном предсказании на вход сразу передаются звуковые токены, а затем модель догенерирует предсказание по одному токену — и каждый раз сгенерированный токен добавляется ко входу. Таким образом модель работает в decoder-only-режиме.<br><br>Основная сложность со стримингом в том, что нужно попеременно передавать новый полученный звук и текстовые токены, которые предсказала модель. Это делается следующим образом. Во время инференса модель обрабатывает аудио по чанкам. После каждого декодер может сгенерировать не один токен, а сразу несколько — пока не встретит специальный маркер конца предсказания. Это позволяет системе работать в стриминге и не откладывать вывод до самого конца. Такой режим авторы реализуют через модифицированную схему тренировки, где модель учат предсказывать текст по частичному аудиоконтексту.<br><br>Особенность SpeechLLM-XL — в устройстве генерации: в каждый момент LLM-декодер видит и текущий аудиочанк, и собственные предыдущие предсказания. Это позволяет ему лучше моделировать зависимость между звучанием и текстом, особенно в условиях ограниченного контекста.<br><br>Авторы сравнивают свою модель с другими стриминговыми ASR-решениями. По качеству SpeechLLM-XL обходит все перечисленные в работе бейзлайны на dev-наборах LibriSpeech. Например, на clean-части она показывает WER 2,5% против 2,9% у Transducer и 2,7% у ReaLLM при схожем размере чанка и lookahead — хотя сложно не заметить, что в целом скоры у бейзлайнов великоваты.<br><br><a href="https://arxiv.org/abs/2409.08148" rel="nofollow noopener noreferrer"><strong>Faster Speech-LLaMA with Multi-token Prediction</strong></a><br><br>Авторы пытаются ускорить LLM-декодер в ASR. Идея в том, что вместо генерации одного токена за раз, как в обычной LLM, они учат декодер предсказывать сразу несколько токенов. Чтобы не вызывать LLM отдельно для каждого из них, добавляют в декодер несколько «голов» — по числу токенов, которые нужно предсказать. Эти головы работают параллельно: каждая предсказывает свой токен, зная предыдущие.<br><br>Получается схема из трёх шагов:<br><br>1) Predict: модель сразу предсказывает K токенов.<br>2)  Verify: среди них ищем самую длинную префикс-последовательность, которую можно подтвердить более строгим one-step-декодером.<br>3) Accept: принимаем только подтверждённые токены и продолжаем с новой гипотезой.<br><br>Это позволяет сократить число вызовов декодера без сильной потери качества. На графике видно, как число вызовов на слово (ось Y) падает, особенно при 4–6 головах, а качество (WER по оси X) остаётся на уровне. Лучший компромисс — 4 головы: ускорение ×2, при этом WER почти не растёт.<br><br>Верификацию авторы реализуют двумя способами:<br><br>— по порогу вероятности;<br>— по позиции гипотезы в top-N (например, если гипотеза оказалась в топ-5, то её можно принять).<br><br>Интересно, что при увеличении числа голов качество даже немного улучшалось. Хотя авторы отмечают это только на LibriSpeech, а на других датасетах наблюдается небольшая просадка.<br><br>По сути, это доработка идеи DeepSeek: там тоже пробовали multi-token prediction, но здесь её применили именно в ASR.<br><br><em>Алексей Рак </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><em><br></em><br><em>* Компания Meta</em> <em>признана экстремистской; её деятельность в России запрещена.</em><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/46_480.webp" srcset="../assets/media/thumbs/46_480.webp 480w, ../assets/media/46.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="46" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/47_480.webp" srcset="../assets/media/thumbs/47_480.webp 480w, ../assets/media/47.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="46" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/48_480.webp" srcset="../assets/media/thumbs/48_480.webp 480w, ../assets/media/48.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="46" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/49_480.webp" srcset="../assets/media/thumbs/49_480.webp 480w, ../assets/media/49.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="46" data-image-index="3" /></div></div>
      <div class="actions">
        <span>834 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/46" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/46.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="45" data-search="dmdspeech: distilled diffusion model surpassing the teacher in zero-shot speech synthesis via direct metric optimization сегодня разберём статью о синтезе речи с помощью диффузионных моделей. авторы из adobe research утверждают, что им удалось избавиться от главного недостатка такого подхода — медленного итеративного инференса — при помощи оригинального метода дистилляции. называется новый метод direct metric optimization. именно он вкупе с уже известной (но не очень популярной) методикой дистилляции dmd и бейзлайн-архитектурой latent speech diffusion позволяет улучшить качество генераций. latent speech diffusion — диффузионный трансформер, оперирующий в пространстве непрерывных латентов (continious latents). латенты авторы получают из чистой wave-формы при помощи энкодера дискретного аудиокодека. потом добавляют к ним шум, u-net-подобной архитектурой итеративно расшумляют и декодируют с помощью стандартного диффузионного лосса. поскольку модель не авторегрессионная, размерность выхода необходимо знать заранее. её выясняют, перемножая количество фонем на speaking rate речевого промпта. метод dmd работает так: расшумлённый результат ученика в заданный момент времени приближают к аналогичному результату учителя от одного зашумлённого латента. для этого обучают специальную состязательную модель: - при помощи дискриминатора отличать результаты ученика и учителя; - сокращать за счёт ученика разницу между ним и учителем. предложенный метод генерации речи — end-to-end. это значит, что в обучение для direct metric optimization можно добавлять sv- и asr-лоссы. аудиосэмплы доступны на github. дмитрий попов ❣ специально для speech info dmdspeech: distilled diffusion model surpassing the teacher in zero-shot speech synthesis via direct metric optimization сегодня разберём статью о синтезе речи с помощью диффузионных моделей. авторы из adobe research утверждают, что им удалось избавиться от главного недостатка такого подхода — медленного итеративного инференса — при помощи оригинального метода дистилляции. называется новый метод direct metric optimization. именно он вкупе с уже известной (но не очень популярной) методикой дистилляции dmd и бейзлайн-архитектурой latent speech diffusion позволяет улучшить качество генераций. latent speech diffusion — диффузионный трансформер, оперирующий в пространстве непрерывных латентов (continious latents). латенты авторы получают из чистой wave-формы при помощи энкодера дискретного аудиокодека. потом добавляют к ним шум, u-net-подобной архитектурой итеративно расшумляют и декодируют с помощью стандартного диффузионного лосса. поскольку модель не авторегрессионная, размерность выхода необходимо знать заранее. её выясняют, перемножая количество фонем на speaking rate речевого промпта. метод dmd работает так: расшумлённый результат ученика в заданный момент времени приближают к аналогичному результату учителя от одного зашумлённого латента. для этого обучают специальную состязательную модель: - при помощи дискриминатора отличать результаты ученика и учителя; - сокращать за счёт ученика разницу между ним и учителем. предложенный метод генерации речи — end-to-end. это значит, что в обучение для direct metric optimization можно добавлять sv- и asr-лоссы. аудиосэмплы доступны на github . дмитрий попов ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-27T07:45:01+00:00" href="./posts/45.html">2025-06-27 07:45 UTC</a></div>
      </div>
      <div class="post-body"><strong>DMDSpeech: Distilled Diffusion Model Surpassing The Teacher in Zero-shot Speech Synthesis via Direct Metric Optimization</strong><br><br>Сегодня разберём <a href="https://arxiv.org/abs/2410.11097v1" rel="nofollow noopener noreferrer">статью</a> о синтезе речи с помощью диффузионных моделей. Авторы из Adobe Research утверждают, что им удалось избавиться от главного недостатка такого подхода — медленного итеративного инференса — при помощи оригинального метода дистилляции.<br><br>Называется новый метод Direct Metric Optimization. Именно он вкупе с уже известной (но не очень популярной) методикой дистилляции DMD и бейзлайн-архитектурой Latent Speech Diffusion позволяет улучшить качество генераций.<br><br>Latent Speech Diffusion — диффузионный трансформер, оперирующий в пространстве непрерывных латентов (continious latents). Латенты авторы получают из чистой wave-формы при помощи энкодера дискретного аудиокодека. Потом добавляют к ним шум, U-net-подобной архитектурой итеративно расшумляют и декодируют с помощью стандартного диффузионного лосса.<br><br>Поскольку модель не авторегрессионная, размерность выхода необходимо знать заранее. Её выясняют, перемножая количество фонем на speaking rate речевого промпта. <br><br>Метод DMD работает так: расшумлённый результат ученика в заданный момент времени приближают к аналогичному результату учителя от одного зашумлённого латента. Для этого обучают специальную состязательную модель: <br><br>- при помощи дискриминатора отличать результаты ученика и учителя;<br>- сокращать за счёт ученика разницу между ним и учителем.<br><br>Предложенный метод генерации речи — end-to-end. Это значит, что в обучение для Direct Metric Optimization можно добавлять SV- и ASR-лоссы.<br><br>Аудиосэмплы доступны <a href="https://dmdspeech.github.io/" rel="nofollow noopener noreferrer">на GitHub</a>.<br><br><em>Дмитрий Попов</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a></div>
      <div class="actions">
        <span>968 просмотров · 24 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/45" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/45.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="41" data-search="synthio, fugatto и mmau: интересное с аудио на iclr 2025 на iclr в этом году было не так много статей на тему аудио, но несколько интересных работ всё же встретились — продолжаем разбирать их в этом и следующих постах. сегодня расскажем, как синтетические данные помогают обучать аудиоклассификаторы (synthio), об универсальной модели, которая по тексту и аудио решает множество задач (fugatto), и о свежем бенчмарке на понимание сложных аудиозадач — от ритма до аккордов (mmau). synthio: augmenting small-scale audio classification datasets with synthetic data в статье предлагают пайплайн для генерации синтетических аудиоданных с помощью t2a-модели, которая по текстовому описанию создаёт аудио. её сначала выравнивают на основе предпочтений, используя ограниченное количество размеченных примеров. после этого модель генерирует синтетику, пригодную для задач классификации. на втором этапе добавляют фильтрацию: оценивают соответствие между текстом и аудио и отбирают качественные пары. дополнительно текст можно уточить с помощью llm. на выходе — расширенный синтетический датасет, который даёт прирост точности в разных аудиоклассификационных задачах. fugatto 1: foundational generative audio transformer opus 1 fugatto — универсальная аудиомодель, которая по текстовому описанию и/или аудиопримеру решает задачи синтеза речи (tts), преобразования голоса (vc), генерации аудио по тексту (t2a), шумоподавления и другие. всё в одной архитектуре. модель построена на flow matching — это позволяет отказаться от gan-дискриминаторов и легче масштабировать обучение. в качестве данных собирают максимально разнообразные открытые датасеты по всем типам задач. для генерации инструкций, которым должна следовать модель, используют llm: она пишет код на python, который вызывает нужный аудиоэффект (например, через библиотеку pedalboard). авторы показывают emergent-эффекты: модель способна выполнять необычные преобразования, которых явно не было в обучении — например, «лающий женский голос» или «мяукание саксофона». также они демонстрируют, как можно итеративно прогонять сэмплы между a2t- и t2a-моделью, уточняя выходы на каждом шаге. mmau: a massive multi-task audio understanding and reasoning benchmark авторы собрали бенчмарк из 10 000 аудиозаданий, вручную размеченных специалистами. каждое задание состоит из аудио, текстового вопроса и нескольких вариантов ответа. примеры задач — от понимания какой конкретный звук длится дольше всего на аудио до музыкального анализа: нужно определить аккорд-прогрессию, ритмический рисунок или эмоциональную окраску. бенчмарк сложный: требует не просто услышать звук, но и понять его структуру. некоторые модели (например, mulalama и salmonn) почти не теряют точности при замене аудио на шум — значит, не используют сам звук. а вот gemini 1.5 pro и qwen2 действительно извлекают аудиосигнал: при шуме качество падает. gemini 1.5 pro лучше всего справляется с задачами фонетического анализа. mmau подчёркивает важность реального аудиопонимания: на нём даже сильные мультимодели работают на уровне 59% точности. из аблейшна авторов следует, что основная доля ошибок приходится на perceptual errors. то есть моделям пока сложно понимать, что именно происходит на записи. влад батаев ❣ специально для speech info #yaiclr synthio, fugatto и mmau: интересное с аудио на iclr 2025 на iclr в этом году было не так много статей на тему аудио, но несколько интересных работ всё же встретились — продолжаем разбирать их в этом и следующих постах. сегодня расскажем, как синтетические данные помогают обучать аудиоклассификаторы (synthio), об универсальной модели, которая по тексту и аудио решает множество задач (fugatto), и о свежем бенчмарке на понимание сложных аудиозадач — от ритма до аккордов (mmau). synthio: augmenting small-scale audio classification datasets with synthetic data в статье предлагают пайплайн для генерации синтетических аудиоданных с помощью t2a-модели, которая по текстовому описанию создаёт аудио. её сначала выравнивают на основе предпочтений, используя ограниченное количество размеченных примеров. после этого модель генерирует синтетику, пригодную для задач классификации. на втором этапе добавляют фильтрацию: оценивают соответствие между текстом и аудио и отбирают качественные пары. дополнительно текст можно уточить с помощью llm. на выходе — расширенный синтетический датасет, который даёт прирост точности в разных аудиоклассификационных задачах. fugatto 1: foundational generative audio transformer opus 1 fugatto — универсальная аудиомодель, которая по текстовому описанию и/или аудиопримеру решает задачи синтеза речи (tts), преобразования голоса (vc), генерации аудио по тексту (t2a), шумоподавления и другие. всё в одной архитектуре. модель построена на flow matching — это позволяет отказаться от gan-дискриминаторов и легче масштабировать обучение. в качестве данных собирают максимально разнообразные открытые датасеты по всем типам задач. для генерации инструкций, которым должна следовать модель, используют llm: она пишет код на python, который вызывает нужный аудиоэффект (например, через библиотеку pedalboard). авторы показывают emergent-эффекты: модель способна выполнять необычные преобразования, которых явно не было в обучении — например, «лающий женский голос» или «мяукание саксофона». также они демонстрируют, как можно итеративно прогонять сэмплы между a2t- и t2a-моделью, уточняя выходы на каждом шаге. mmau: a massive multi-task audio understanding and reasoning benchmark авторы собрали бенчмарк из 10 000 аудиозаданий, вручную размеченных специалистами. каждое задание состоит из аудио, текстового вопроса и нескольких вариантов ответа. примеры задач — от понимания какой конкретный звук длится дольше всего на аудио до музыкального анализа: нужно определить аккорд-прогрессию, ритмический рисунок или эмоциональную окраску. бенчмарк сложный: требует не просто услышать звук, но и понять его структуру. некоторые модели (например, mulalama и salmonn) почти не теряют точности при замене аудио на шум — значит, не используют сам звук. а вот gemini 1.5 pro и qwen2 действительно извлекают аудиосигнал: при шуме качество падает. gemini 1.5 pro лучше всего справляется с задачами фонетического анализа. mmau подчёркивает важность реального аудиопонимания: на нём даже сильные мультимодели работают на уровне 59% точности. из аблейшна авторов следует, что основная доля ошибок приходится на perceptual errors. то есть моделям пока сложно понимать, что именно происходит на записи. влад батаев ❣ специально для speech info #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-20T12:36:41+00:00" href="./posts/41.html">2025-06-20 12:36 UTC</a></div>
      </div>
      <div class="post-body"><strong>Synthio, Fugatto и MMAU: интересное с аудио на ICLR 2025</strong><br><br>На ICLR в этом году было не так много статей на тему аудио, но несколько интересных работ всё же встретились — продолжаем разбирать их в этом и следующих постах. <br><br>Сегодня расскажем, как синтетические данные помогают обучать аудиоклассификаторы (Synthio), об универсальной модели, которая по тексту и аудио решает множество задач (Fugatto), и о свежем бенчмарке на понимание сложных аудиозадач — от ритма до аккордов (MMAU).<br><br><a href="https://openreview.net/forum?id=bR1J7SpzrD" rel="nofollow noopener noreferrer"><strong>Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data</strong></a> <br><br>В статье предлагают пайплайн для генерации синтетических аудиоданных с помощью T2A-модели, которая по текстовому описанию создаёт аудио. Её сначала выравнивают на основе предпочтений, используя ограниченное количество размеченных примеров. После этого модель генерирует синтетику, пригодную для задач классификации.<br><br>На втором этапе добавляют фильтрацию: оценивают соответствие между текстом и аудио и отбирают качественные пары. Дополнительно текст можно уточить с помощью LLM. На выходе — расширенный синтетический датасет, который даёт прирост точности в разных аудиоклассификационных задачах.<br><br><a href="https://openreview.net/forum?id=B2Fqu7Y2cd" rel="nofollow noopener noreferrer"><strong>Fugatto 1: Foundational Generative Audio Transformer Opus 1</strong></a><br><br>Fugatto — универсальная аудиомодель, которая по текстовому описанию и/или аудиопримеру решает задачи синтеза речи (TTS), преобразования голоса (VC), генерации аудио по тексту (T2A), шумоподавления и другие. Всё в одной архитектуре.<br><br>Модель построена на flow matching — это позволяет отказаться от GAN-дискриминаторов и легче масштабировать обучение. В качестве данных собирают максимально разнообразные открытые датасеты по всем типам задач. Для генерации инструкций, которым должна следовать модель, используют LLM: она пишет код на Python, который вызывает нужный аудиоэффект (например, через библиотеку Pedalboard).<br><br>Авторы показывают emergent-эффекты: модель способна выполнять необычные преобразования, которых явно не было в обучении — например, «лающий женский голос» или «мяукание саксофона». Также они демонстрируют, как можно итеративно прогонять сэмплы между A2T- и T2A-моделью, уточняя выходы на каждом шаге.<br><br><a href="https://arxiv.org/abs/2410.19168" rel="nofollow noopener noreferrer"><strong>MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark<br></strong></a><br>Авторы собрали бенчмарк из 10 000 аудиозаданий, вручную размеченных специалистами. Каждое задание состоит из аудио, текстового вопроса и нескольких вариантов ответа. Примеры задач — от понимания какой конкретный звук длится дольше всего на аудио до музыкального анализа: нужно определить аккорд-прогрессию, ритмический рисунок или эмоциональную окраску.<br><br>Бенчмарк сложный: требует не просто услышать звук, но и понять его структуру. Некоторые модели (например, MuLaLaMa и SALMONN) почти не теряют точности при замене аудио на шум — значит, не используют сам звук. А вот Gemini 1.5 Pro и Qwen2 действительно извлекают аудиосигнал: при шуме качество падает. Gemini 1.5 Pro лучше всего справляется с задачами фонетического анализа.<br><br>MMAU подчёркивает важность реального аудиопонимания: на нём даже сильные мультимодели работают на уровне 59% точности. Из аблейшна авторов следует, что основная доля ошибок приходится на perceptual errors. То есть моделям пока сложно понимать, что именно происходит на записи. <br><br><em>Влад Батаев </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/41_480.webp" srcset="../assets/media/thumbs/41_480.webp 480w, ../assets/media/41.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="41" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/42_480.webp" srcset="../assets/media/thumbs/42_480.webp 480w, ../assets/media/42.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="41" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/43_480.webp" srcset="../assets/media/thumbs/43_480.webp 480w, ../assets/media/43.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="41" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/44_480.webp" srcset="../assets/media/thumbs/44_480.webp 480w, ../assets/media/44.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="41" data-image-index="3" /></div></div>
      <div class="actions">
        <span>853 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/41" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/41.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="39" data-search="обзор статей с icassp 25. часть 2: голосовая активация в прошлой части рассказали о работах на тему шумоподавления в наушниках, теперь переходим к голосовой активации. сегодня кратко о двух статьях: одна — о визуальном споттере (активация по звуку и видео, но без активационной фразы), вторая — о кастомных keyword spotters. an efficient and streaming audio visual active speaker detection system статья от apple на тему детекции активного спикера в стриминговом видео с помощью аудио- и видеосигналов. такая задача уже решается в колонках amazon и google. архитектура построена на двух похоже устроенных энкодерах: аудио- и визуальном. в них используются каузальные свёртки, которые не «заглядывают» в будущее — это важно для стриминга. после извлечения признаков фреймы выравниваются и объединяются, чтобы для каждого момента времени были и аудио-, и визуальные фичи. поверх этого авторы обучают трансформер. трансформеру за счёт использования масок также ограничивали возможность «заглядывать» в будущее. ключевое исследование — о том, какие маски использовать в трансформере: сколько контекста из прошлого и будущего ему давать. контекст из будущего начинает помогать, только если в модель уже подаётся достаточно длинный контекст из прошлого — примерно от 15 фреймов. по ощущениям, решение довольно зрелое: авторы утверждают, что обучали модель не только на youtube, но и на внутренних данных. slick: exploiting subsequences for length-constrained keyword spotting ещё одна статья — о кастомных keyword-spotters: пользователь задаёт произвольную фразу, на которую должна реагировать модель. на эту тему в последнее время выходит довольно большое количество статей. для начала gpt-подобная модель превращает текст в последовательность фонем. архитектура включает fully connected-слой, который кодирует фонемы в эмбеддинги, и аудиопарсер, который учится по звуку восстанавливать ту же последовательность. главная особенность работы — продуманные лоссы, которые помогают модели устойчиво обучаться. во-первых, аудиомодель учат предсказывать последовательность фонем — это даёт хорошую привязку к произнесённому тексту. во-вторых, добавляют рантайм-механизм сопоставления между аудио и заданной последовательностью: с помощью cross-attention проверяют, совпадают ли векторы в нужных местах. и в-третьих, делают то же самое, но уже для всех префиксов заданной последовательности. если на нужной фонеме случается несовпадение — срабатывает сигнал об ошибке. если первые два пункта уже встречались ранее, то третий — нововведение этой статьи, которое, судя по результатам, приносит заметный прирост по качеству. основной вывод: такой подход с дополнительными лоссами и проверкой совпадений позволяет сильно улучшить точность детекции по сравнению с базовыми моделями. концепция не совсем новая, но реализация аккуратная и работающая. в следующей части обзора расскажем о двух llm для улучшения asr. алексей рак ❣ специально для speech info обзор статей с icassp 25. часть 2: голосовая активация в прошлой части рассказали о работах на тему шумоподавления в наушниках, теперь переходим к голосовой активации. сегодня кратко о двух статьях: одна — о визуальном споттере (активация по звуку и видео, но без активационной фразы), вторая — о кастомных keyword spotters. an efficient and streaming audio visual active speaker detection system статья от apple на тему детекции активного спикера в стриминговом видео с помощью аудио- и видеосигналов. такая задача уже решается в колонках amazon и google. архитектура построена на двух похоже устроенных энкодерах: аудио- и визуальном. в них используются каузальные свёртки, которые не «заглядывают» в будущее — это важно для стриминга. после извлечения признаков фреймы выравниваются и объединяются, чтобы для каждого момента времени были и аудио-, и визуальные фичи. поверх этого авторы обучают трансформер. трансформеру за счёт использования масок также ограничивали возможность «заглядывать» в будущее. ключевое исследование — о том, какие маски использовать в трансформере: сколько контекста из прошлого и будущего ему давать. контекст из будущего начинает помогать, только если в модель уже подаётся достаточно длинный контекст из прошлого — примерно от 15 фреймов. по ощущениям, решение довольно зрелое: авторы утверждают, что обучали модель не только на youtube, но и на внутренних данных. slick: exploiting subsequences for length-constrained keyword spotting ещё одна статья — о кастомных keyword-spotters: пользователь задаёт произвольную фразу, на которую должна реагировать модель. на эту тему в последнее время выходит довольно большое количество статей. для начала gpt-подобная модель превращает текст в последовательность фонем. архитектура включает fully connected-слой, который кодирует фонемы в эмбеддинги, и аудиопарсер, который учится по звуку восстанавливать ту же последовательность. главная особенность работы — продуманные лоссы, которые помогают модели устойчиво обучаться. во-первых, аудиомодель учат предсказывать последовательность фонем — это даёт хорошую привязку к произнесённому тексту. во-вторых, добавляют рантайм-механизм сопоставления между аудио и заданной последовательностью: с помощью cross-attention проверяют, совпадают ли векторы в нужных местах. и в-третьих, делают то же самое, но уже для всех префиксов заданной последовательности. если на нужной фонеме случается несовпадение — срабатывает сигнал об ошибке. если первые два пункта уже встречались ранее, то третий — нововведение этой статьи, которое, судя по результатам, приносит заметный прирост по качеству. основной вывод: такой подход с дополнительными лоссами и проверкой совпадений позволяет сильно улучшить точность детекции по сравнению с базовыми моделями. концепция не совсем новая, но реализация аккуратная и работающая. в следующей части обзора расскажем о двух llm для улучшения asr. алексей рак ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-11T10:05:31+00:00" href="./posts/39.html">2025-06-11 10:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Обзор статей с ICASSP 25. Часть 2: голосовая активация<br></strong><br>В <a href="https://t.me/speechinfo/38" rel="nofollow noopener noreferrer">прошлой части</a> рассказали о работах на тему шумоподавления в наушниках, теперь переходим к голосовой активации. Сегодня кратко о двух статьях: одна — о визуальном споттере (активация по звуку и видео, но без активационной фразы), вторая — о кастомных keyword spotters.<br><br><a href="https://arxiv.org/abs/2409.09018" rel="nofollow noopener noreferrer"><strong>An Efficient and Streaming Audio Visual Active Speaker Detection System</strong></a><br><br>Статья от Apple на тему детекции активного спикера в стриминговом видео с помощью аудио- и видеосигналов. Такая задача уже решается в колонках Amazon и Google. Архитектура построена на двух похоже устроенных энкодерах: аудио- и визуальном. В них используются каузальные свёртки, которые не «заглядывают» в будущее — это важно для стриминга.<br><br>После извлечения признаков фреймы выравниваются и объединяются, чтобы для каждого момента времени были и аудио-, и визуальные фичи. Поверх этого авторы обучают трансформер. Трансформеру за счёт использования масок также ограничивали возможность «заглядывать» в будущее.<br><br>Ключевое исследование — о том, какие маски использовать в трансформере: сколько контекста из прошлого и будущего ему давать. Контекст из будущего начинает помогать, только если в модель уже подаётся достаточно длинный контекст из прошлого — примерно от 15 фреймов.<br><br>По ощущениям, решение довольно зрелое: авторы утверждают, что обучали модель не только на YouTube, но и на внутренних данных. <br><br><a href="https://arxiv.org/html/2409.09067v1" rel="nofollow noopener noreferrer"><strong>SLiCK: Exploiting Subsequences for Length-Constrained Keyword Spotting</strong></a><br><br>Ещё одна статья — о кастомных keyword-spotters: пользователь задаёт произвольную фразу, на которую должна реагировать модель. На эту тему в последнее время выходит довольно большое количество статей. Для начала GPT-подобная модель превращает текст в последовательность фонем.<br><br>Архитектура включает fully connected-слой, который кодирует фонемы в эмбеддинги, и аудиопарсер, который учится по звуку восстанавливать ту же последовательность. Главная особенность работы — продуманные лоссы, которые помогают модели устойчиво обучаться.<br><br>Во-первых, аудиомодель учат предсказывать последовательность фонем — это даёт хорошую привязку к произнесённому тексту. Во-вторых, добавляют рантайм-механизм сопоставления между аудио и заданной последовательностью: с помощью cross-attention проверяют, совпадают ли векторы в нужных местах. И в-третьих, делают то же самое, но уже для всех префиксов заданной последовательности. Если на нужной фонеме случается несовпадение — срабатывает сигнал об ошибке.<br><br>Если первые два пункта уже встречались ранее, то третий — нововведение этой статьи, которое, судя по результатам, приносит заметный прирост по качеству.<br><br>Основной вывод: такой подход с дополнительными лоссами и проверкой совпадений позволяет сильно улучшить точность детекции по сравнению с базовыми моделями. Концепция не совсем новая, но реализация аккуратная и работающая.<br><br>В следующей части обзора расскажем о двух LLM для улучшения ASR.<br><br><em>Алексей</em> <em>Рак</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> <em>Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/39_480.webp" srcset="../assets/media/thumbs/39_480.webp 480w, ../assets/media/39.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="39" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/40_480.webp" srcset="../assets/media/thumbs/40_480.webp 480w, ../assets/media/40.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="39" data-image-index="1" /></div></div>
      <div class="actions">
        <span>832 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/39" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/39.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="38" data-search="обзор статей с icassp 25. часть 1: шумоподавление в наушниках в апреле в индии прошла конференция icassp 2025, на которой побывал руководитель группы встроенного голосового ввода алексей рак. в этом году обошлось без откровений, но кое-что любопытное всё же нашлось. пожалуй, главный тренд: заметно меньше статей о колонках — индустрия уходит в наушники и стриминг. в этом и следующем постах разберём интересные статьи с конференции — начнём двух работ о шумоподавлении в наушниках. towards sub-millisecond latency real-time speech enhancement models on hearables статья от google о шумоподавлении в режиме прозрачности для наушников. такая технология нужна, когда не хочешь снимать наушники, но хочешь слышать речь вокруг. при этом нужно, чтобы голоса усиливались, а шум — наоборот, глушился. но вся магия работает, только если нет сильной задержки. иначе речь дублируется, так как амбушюры не полностью поглощают речь, а наушники проигрывают очищенные разговор с задержкой. поэтому важно уложиться в лаг в пару миллисекунд. авторы сделали компактную нейросеть, которая предсказывает параметры fir-фильтра — по сути, какую свёртку применить к звуку. эта свёртка обновляется каждые 8 мс, но применяется сразу к новым аудиофрагментам, так что задержка остаётся минимальной — 2–3 мс (алгоритмическая задержка — 1 мс, а всё остальное — вычислительная). работает даже на слабом железе — тестировали на 600 mhz hifi4 audio dsp(cadence), и там всё летает. статья даёт понять, насколько вообще можно опустить задержку в таких системах, если аккуратно подойти к задаче и сделать лёгкую модель. bone conducted signal guided speech enhancement for voice assistanton earbuds ещё одна статья о шумоподавлении, но уже для очистки того, что пользователь наушников говорит, а не слышит. человек говорит в шумной обстановке (на улице, на ветру, в метро), и сам этого может даже не замечать. снаружи всё гудит, а внутри уха — тишина. только вот собеседнику слышно совсем не так. в современных наушниках уже есть не только внешние микрофоны, но и внутренние, которые улавливают сигнал через кость черепа. он искажён, но в нём почти нет внешнего шума. его и используют. вход у модели — два канала: грязный внешний микрофон и искажённый, но «чистый» костный сигнал. всё это подаётся в cleanformer — это их старая модель, адаптированная под такую задачу. cleanformer предсказывает маску, которую потом накладывают на внешний сигнал, чтобы его «почистить»: сохранить полезные частоты и убрать шум. похожую модель google использует уже довольно давно для очистки звука в умных колонках. цель — сделать так, чтобы в особо шумных условиях голос всё равно звучал понятно. для реального применения такой подход годится, если в устройстве есть костный канал — а он уже есть во многих моделях наушников. в следующей части разберём пару работ о голосовой активации. алексей рак ❣ специально для speech info обзор статей с icassp 25. часть 1: шумоподавление в наушниках в апреле в индии прошла конференция icassp 2025, на которой побывал руководитель группы встроенного голосового ввода алексей рак. в этом году обошлось без откровений, но кое-что любопытное всё же нашлось. пожалуй, главный тренд: заметно меньше статей о колонках — индустрия уходит в наушники и стриминг. в этом и следующем постах разберём интересные статьи с конференции — начнём двух работ о шумоподавлении в наушниках. towards sub-millisecond latency real-time speech enhancement models on hearables статья от google о шумоподавлении в режиме прозрачности для наушников. такая технология нужна, когда не хочешь снимать наушники, но хочешь слышать речь вокруг. при этом нужно, чтобы голоса усиливались, а шум — наоборот, глушился. но вся магия работает, только если нет сильной задержки. иначе речь дублируется, так как амбушюры не полностью поглощают речь, а наушники проигрывают очищенные разговор с задержкой. поэтому важно уложиться в лаг в пару миллисекунд. авторы сделали компактную нейросеть, которая предсказывает параметры fir-фильтра — по сути, какую свёртку применить к звуку. эта свёртка обновляется каждые 8 мс, но применяется сразу к новым аудиофрагментам, так что задержка остаётся минимальной — 2–3 мс (алгоритмическая задержка — 1 мс, а всё остальное — вычислительная). работает даже на слабом железе — тестировали на 600 mhz hifi4 audio dsp(cadence), и там всё летает. статья даёт понять, насколько вообще можно опустить задержку в таких системах, если аккуратно подойти к задаче и сделать лёгкую модель. bone conducted signal guided speech enhancement for voice assistanton earbuds ещё одна статья о шумоподавлении, но уже для очистки того, что пользователь наушников говорит, а не слышит. человек говорит в шумной обстановке (на улице, на ветру, в метро), и сам этого может даже не замечать. снаружи всё гудит, а внутри уха — тишина. только вот собеседнику слышно совсем не так. в современных наушниках уже есть не только внешние микрофоны, но и внутренние, которые улавливают сигнал через кость черепа. он искажён, но в нём почти нет внешнего шума. его и используют. вход у модели — два канала: грязный внешний микрофон и искажённый, но «чистый» костный сигнал. всё это подаётся в cleanformer — это их старая модель, адаптированная под такую задачу. cleanformer предсказывает маску, которую потом накладывают на внешний сигнал, чтобы его «почистить»: сохранить полезные частоты и убрать шум. похожую модель google использует уже довольно давно для очистки звука в умных колонках. цель — сделать так, чтобы в особо шумных условиях голос всё равно звучал понятно. для реального применения такой подход годится, если в устройстве есть костный канал — а он уже есть во многих моделях наушников. в следующей части разберём пару работ о голосовой активации. алексей рак ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-05T08:00:00+00:00" href="./posts/38.html">2025-06-05 08:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>Обзор статей с ICASSP 25. Часть 1: шумоподавление в наушниках</strong><br><br>В апреле в Индии прошла конференция ICASSP 2025, на которой побывал руководитель группы встроенного голосового ввода Алексей Рак. В этом году обошлось без откровений, но кое-что любопытное всё же нашлось. Пожалуй, главный тренд: заметно меньше статей о колонках — индустрия уходит в наушники и стриминг. В этом и следующем постах разберём интересные статьи с конференции — начнём двух работ о шумоподавлении в наушниках.<br><br><a href="https://arxiv.org/abs/2409.18239" rel="nofollow noopener noreferrer"><strong>Towards Sub-millisecond Latency Real-Time Speech Enhancement Models on Hearables</strong></a><br><br>Статья от Google о шумоподавлении в режиме прозрачности для наушников. Такая технология нужна, когда не хочешь снимать наушники, но хочешь слышать речь вокруг. При этом нужно, чтобы голоса усиливались, а шум — наоборот, глушился. Но вся магия работает, только если нет сильной задержки. Иначе речь дублируется, так как амбушюры не полностью поглощают речь, а наушники проигрывают очищенные разговор с задержкой. Поэтому важно уложиться в лаг в пару миллисекунд.<br><br>Авторы сделали компактную нейросеть, которая предсказывает параметры FIR-фильтра — по сути, какую свёртку применить к звуку. Эта свёртка обновляется каждые 8 мс, но применяется сразу к новым аудиофрагментам, так что задержка остаётся минимальной — 2–3 мс (алгоритмическая задержка — 1 мс, а всё остальное — вычислительная). Работает даже на слабом железе — тестировали на 600 MHz HiFi4 Audio DSP(Cadence), и там всё летает.<br><br>Статья даёт понять, насколько вообще можно опустить задержку в таких системах, если аккуратно подойти к задаче и сделать лёгкую модель.<br><br><a href="https://www.researchgate.net/publication/390537997_Bone_Conducted_Signal_Guided_Speech_Enhancement_For_Voice_Assistant_on_Earbuds" rel="nofollow noopener noreferrer"><strong>BONE CONDUCTED SIGNAL GUIDED SPEECH ENHANCEMENT FOR VOICE ASSISTANTON EARBUDS</strong></a><br><br>Ещё одна статья о шумоподавлении, но уже для очистки того, что пользователь наушников говорит, а не слышит. Человек говорит в шумной обстановке (на улице, на ветру, в метро), и сам этого может даже не замечать. Снаружи всё гудит, а внутри уха — тишина. Только вот собеседнику слышно совсем не так.<br><br>В современных наушниках уже есть не только внешние микрофоны, но и внутренние, которые улавливают сигнал через кость черепа. Он искажён, но в нём почти нет внешнего шума. Его и используют.<br><br>Вход у модели — два канала: грязный внешний микрофон и искажённый, но «чистый» костный сигнал. Всё это подаётся в Cleanformer — это их старая модель, адаптированная под такую задачу. Cleanformer предсказывает маску, которую потом накладывают на внешний сигнал, чтобы его «почистить»: сохранить полезные частоты и убрать шум. Похожую модель Google использует уже довольно давно для очистки звука в умных колонках.<br><br>Цель — сделать так, чтобы в особо шумных условиях голос всё равно звучал понятно. Для реального применения такой подход годится, если в устройстве есть костный канал — а он уже есть во многих моделях наушников.<br><br>В следующей части разберём пару работ о голосовой активации.<br><br><em>Алексей</em> <em>Рак</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> <em>Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/38_480.webp" srcset="../assets/media/thumbs/38_480.webp 480w, ../assets/media/38.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="38" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 347 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/38" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/38.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="37" data-search="moshi в деталях: новая архитектура диалоговой системы в реальном времени. часть 2/2 продолжаем разбирать moshi — диалоговую систему, которая совмещает распознавание, чат-бота и синтез речи в одной модели. в первой части речь шла о llm helium и аудиокодеке mimi. здесь — о том, как устроена сама moshi и как работает механизм переключения между «слушать» и «говорить». moshi на следующем этапе авторы хотят научить текстовую llm helium аудиомодальности, а конкретно — предсказывать токены от mimi. да ещё и так, чтобы модель не потеряла свои llm-ные знания. наша задача — предсказывать матрицу из токенов с размерностями времени на 8 кодеков. для этого модель moshi состоит из двух трансформеров: temporal transformer и depth transformer. temporal transformer — это большой трансформер, проинициализированный весами helium. он будет авторегрессивно идти по размерности времени и генерировать эмбеддинг для каждого тика времени. depth transformer инициализируется шумом. его задача — на каждом шаге temporal transformer-a закондишениться на сгенерированный эмбеддинг и развернуть его в 8 mimi-токенов. учиться вся эта конструкция будет в три этапа. тут опускаем много подробностей, но идея примерно такая: 1. учимся на огромном, шумном audio-only датасете. на этом этапе моделька познаёт аудиомодальность и пытается соотнести её с текстовой модальностью. 2. учимся на синтетических диалоговых данных. здесь модель учиться слушать и слышать одновременно, подстраивается под диалоговый формат (так называемый full-duplex-режим). 3. тюнимся на более качественном диалоговом датасете. модель обретает свой голос и выучивает более осознанные диалоговые ответы. важнейшая фича moshi — full-duplex: способность модели одновременно слушать и говорить. с ней диалог получается плавнее и человечнее, в нём могут быть одновременные реплики, перебивания и междометия. модель достигает режима full-duplex с помощью алгоритма inner monologue. inner-monologue для начала, мы хотим сделать так, чтобы одно и то же слово, представленное в виде текстовых токенов и в виде аудиотокенов, занимало одно и то же количество токенов. для этого авторы взяли датасет и модель whisperv3 и сделали алайнмент. то есть для каждого слова в тексте нашли время, когда оно начинает и заканчивает произноситься. после этого авторы взяли специальные паддинг-токены и в текстовой модальности добавили их после каждого слова — столько, чтобы по длине они совпадали с количеством токенов, которое занимает это слово в аудиомодальности. дальше мы хотим учить модельку на этих данных. тут полезно посмотреть на картинку. — мы хотим, чтобы в каждый момент времени наша моделька работала с тремя стримами информации: аудио, которое произносит пользователь (8 токенов), аудио, которое произносит moshi (8 токенов), и текст, разбавленный паддингами, который произносит moshi (1 токен). на картинке они показаны сверху вниз. — мы хотим все три стрима подавать на вход к модельке. соответственно для каждого стрима токенов будет своя матрица эмбеддингов, которые в итоге складываются. — на выход мы хотим получать только текст и аудио реплик. на картинке — это оранжево-жёлто-зелёные (каждый цвет — отдельное слово) токены. текст предсказывает линейная голова поверх temporal transformer, а для аудиотокенов есть depth transformer. — в такой парадигме моделька учится и инферится. moshi вышла 7 месяцев назад и, кажется, уже потихоньку устаревает. если попользоваться демкой, сначала она приводит в восхищение, но потом становятся заметны косяки: модель говорит глупости, неуместно перебивает, начинает отвечать с большой задержкой. она ощутимо слабее, чем, например, voicemode от openai. но у ребят подробная статья, много интересных выводов и экспериментов, а также выложенный в открытый доступ кодек. это довольно большой вклад в область. роман кайль ❣ специально для speech info moshi в деталях: новая архитектура диалоговой системы в реальном времени. часть 2/2 продолжаем разбирать moshi — диалоговую систему, которая совмещает распознавание, чат-бота и синтез речи в одной модели. в первой части речь шла о llm helium и аудиокодеке mimi. здесь — о том, как устроена сама moshi и как работает механизм переключения между «слушать» и «говорить». moshi на следующем этапе авторы хотят научить текстовую llm helium аудиомодальности, а конкретно — предсказывать токены от mimi. да ещё и так, чтобы модель не потеряла свои llm-ные знания. наша задача — предсказывать матрицу из токенов с размерностями времени на 8 кодеков. для этого модель moshi состоит из двух трансформеров: temporal transformer и depth transformer. temporal transformer — это большой трансформер, проинициализированный весами helium. он будет авторегрессивно идти по размерности времени и генерировать эмбеддинг для каждого тика времени. depth transformer инициализируется шумом. его задача — на каждом шаге temporal transformer-a закондишениться на сгенерированный эмбеддинг и развернуть его в 8 mimi-токенов. учиться вся эта конструкция будет в три этапа. тут опускаем много подробностей, но идея примерно такая: 1. учимся на огромном, шумном audio-only датасете. на этом этапе моделька познаёт аудиомодальность и пытается соотнести её с текстовой модальностью. 2. учимся на синтетических диалоговых данных. здесь модель учиться слушать и слышать одновременно, подстраивается под диалоговый формат (так называемый full-duplex-режим). 3. тюнимся на более качественном диалоговом датасете. модель обретает свой голос и выучивает более осознанные диалоговые ответы. важнейшая фича moshi — full-duplex: способность модели одновременно слушать и говорить. с ней диалог получается плавнее и человечнее, в нём могут быть одновременные реплики, перебивания и междометия. модель достигает режима full-duplex с помощью алгоритма inner monologue. inner-monologue для начала, мы хотим сделать так, чтобы одно и то же слово, представленное в виде текстовых токенов и в виде аудиотокенов, занимало одно и то же количество токенов. для этого авторы взяли датасет и модель whisperv3 и сделали алайнмент. то есть для каждого слова в тексте нашли время, когда оно начинает и заканчивает произноситься. после этого авторы взяли специальные паддинг-токены и в текстовой модальности добавили их после каждого слова — столько, чтобы по длине они совпадали с количеством токенов, которое занимает это слово в аудиомодальности. дальше мы хотим учить модельку на этих данных. тут полезно посмотреть на картинку. — мы хотим, чтобы в каждый момент времени наша моделька работала с тремя стримами информации: аудио, которое произносит пользователь (8 токенов), аудио, которое произносит moshi (8 токенов), и текст, разбавленный паддингами, который произносит moshi (1 токен). на картинке они показаны сверху вниз. — мы хотим все три стрима подавать на вход к модельке. соответственно для каждого стрима токенов будет своя матрица эмбеддингов, которые в итоге складываются. — на выход мы хотим получать только текст и аудио реплик. на картинке — это оранжево-жёлто-зелёные (каждый цвет — отдельное слово) токены. текст предсказывает линейная голова поверх temporal transformer, а для аудиотокенов есть depth transformer. — в такой парадигме моделька учится и инферится. moshi вышла 7 месяцев назад и, кажется, уже потихоньку устаревает. если попользоваться демкой, сначала она приводит в восхищение, но потом становятся заметны косяки: модель говорит глупости, неуместно перебивает, начинает отвечать с большой задержкой. она ощутимо слабее, чем, например, voicemode от openai. но у ребят подробная статья, много интересных выводов и экспериментов, а также выложенный в открытый доступ кодек. это довольно большой вклад в область. роман кайль ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-27T08:05:48+00:00" href="./posts/37.html">2025-05-27 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Moshi в деталях: новая архитектура диалоговой системы в реальном времени. Часть 2/2 </strong><br><br>Продолжаем разбирать <a href="https://arxiv.org/abs/2410.00037" rel="nofollow noopener noreferrer">Moshi </a>— диалоговую систему, которая совмещает распознавание, чат-бота и синтез речи в одной модели. В первой части речь шла о LLM Helium и аудиокодеке Mimi. Здесь — о том, как устроена сама Moshi и как работает механизм переключения между «слушать» и «говорить».<br><br><strong>Moshi </strong><br><br>На следующем этапе авторы хотят научить текстовую LLM Helium аудиомодальности, а конкретно — предсказывать токены от Mimi. Да ещё и так, чтобы модель не потеряла свои LLM-ные знания.<br><br>Наша задача — предсказывать матрицу из токенов с размерностями времени на 8 кодеков. Для этого модель Moshi состоит из двух трансформеров: Temporal Transformer и Depth Transformer.<br><br>Temporal Transformer — это большой трансформер, проинициализированный весами Helium. Он будет авторегрессивно идти по размерности времени и генерировать эмбеддинг для каждого тика времени. Depth Transformer инициализируется шумом. Его задача — на каждом шаге Temporal Transformer-a закондишениться на сгенерированный эмбеддинг и развернуть его в 8 Mimi-токенов.<br><br>Учиться вся эта конструкция будет в три этапа. Тут опускаем много подробностей, но идея примерно такая:<br><br>1. Учимся на огромном, шумном audio-only датасете. На этом этапе моделька познаёт аудиомодальность и пытается соотнести её с текстовой модальностью.<br><br>2. Учимся на синтетических диалоговых данных. Здесь модель учиться слушать и слышать одновременно, подстраивается под диалоговый формат (так называемый full-duplex-режим).<br><br>3. Тюнимся на более качественном диалоговом датасете. Модель обретает свой голос и выучивает более осознанные диалоговые ответы.<br><br>Важнейшая фича Moshi — full-duplex: способность модели одновременно слушать и говорить. С ней диалог получается плавнее и человечнее, в нём могут быть одновременные реплики, перебивания и междометия. Модель достигает режима full-duplex с помощью алгоритма Inner Monologue.<br><br><strong>Inner-monologue</strong><br><strong><br></strong>Для начала, мы хотим сделать так, чтобы одно и то же слово, представленное в виде текстовых токенов и в виде аудиотокенов, занимало одно и то же количество токенов. Для этого авторы взяли датасет и модель WhisperV3 и сделали алайнмент. То есть для каждого слова в тексте нашли время, когда оно начинает и заканчивает произноситься. После этого авторы взяли специальные паддинг-токены и в текстовой модальности добавили их после каждого слова — столько, чтобы по длине они совпадали с количеством токенов, которое занимает это слово в аудиомодальности.<br><br>Дальше мы хотим учить модельку на этих данных. Тут полезно посмотреть на картинку.<br><br>— Мы хотим, чтобы в каждый момент времени наша моделька работала с тремя стримами информации: аудио, которое произносит пользователь (8 токенов), аудио, которое произносит Moshi (8 токенов), и текст, разбавленный паддингами, который произносит Moshi (1 токен). На картинке они показаны сверху вниз.<br><br>— Мы хотим все три стрима подавать на вход к модельке. Соответственно для каждого стрима токенов будет своя матрица эмбеддингов, которые  в итоге складываются.<br><br>— На выход мы хотим получать только текст и аудио реплик. На картинке — это оранжево-жёлто-зелёные (каждый цвет — отдельное слово) токены. Текст предсказывает линейная голова поверх Temporal Transformer, а для аудиотокенов есть Depth Transformer.<br><br>— В такой парадигме моделька учится и инферится.<br><br>Moshi вышла 7 месяцев назад и, кажется, уже потихоньку устаревает. Если попользоваться демкой, сначала она приводит в восхищение, но потом становятся заметны косяки: модель говорит глупости, неуместно перебивает, начинает отвечать с большой задержкой. Она ощутимо слабее, чем, например, VoiceMode от OpenAI. Но у ребят подробная статья, много интересных выводов и экспериментов, а также выложенный в открытый доступ кодек. Это довольно большой вклад в область.<br><br><em>Роман Кайль</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> <em>Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/37_480.webp" srcset="../assets/media/thumbs/37_480.webp 480w, ../assets/media/37.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="37" data-image-index="0" /></div></div>
      <div class="actions">
        <span>820 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/37" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/37.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="36" data-search="moshi в деталях: новая архитектура диалоговой системы в реальном времени. часть 1/2 разбираем статью о real-time dialogue-модели moshi, которая, в отличие от предыдущих диалоговых систем, объединяет в одной архитектуре три компонента: asr (распознавание речи), llm (языковая модель) и tts (синтез речи). такая схема позволяет воспринимать речь и генерировать ответ одновременно. управление тем, когда говорить и когда слушать, реализовано через специальный управляющий токен. (даже я не всегда так умею — прим. автора). архитектура модели состоит из четырёх частей, и у всех звучные названия. в этом посте уместим разбор двух первых частей, а в следующем — ещё двух. helium простая текстовая модель, предсказывающая следующий токен. претрейн модели проводился на 2,1 трлн токенов. (для сравнения: llama 2 — 1,8 трлн, llama 3 — 15,6 трлн). данные собирали, фильтруя commoncrawl — огромный дамп интернета, где много мусора, но если хорошо почистить, получается неплохой датасет. в итоге датасет состоит из 87,5% commoncrawl и 12,5% wikipedia. после претрейна провели три дополнительных этапа обучения: пост-тренировку, файнтюнинг и инструкционное обучение — чтобы модель лучше справлялась с диалогами. по оценке авторов, helium сравнима с llama 2 и первым mistral, но не дотягивает до llama 3. mimi нейросетевой аудиокодек на основе residual vector quantization (rvq). архитектура — стандартный энкодер-декодер, но с интересными деталями: обычно в аудиокодеках используются только свёрточные слои, а тут добавили трансформеры — в конце энкодера и в начале декодера. это сделало кодек умнее (и тяжелее). ещё одно важное отличие mimi от остальных аудиокодеков — у него довольно маленькая герцовка. mimi нужно 12,5 векторов, чтобы закодировать секунду аудио. для сравнения у encodec — 75, wavtokenizer — 40. за счёт этого трансформер поверх такого кодека можно учить с бóльшим батчем (в секундах) и быстрее инферить. набор лоссов у moshi примерно такой же, как и у hifi-gan-a. единственное отличие — это то, что авторы убрали l1-loss между stft-спектрограммами, из-за плохой корреляции с человеческим восприятием. без него субъективные метрики получались лучше. главное ноу-хау mimi — семантическая дистилляция, которая позволяет получить акустические токены со свойствами семантических. акустические токены создаются кодек-моделями вроде mimi. из них можно хорошо и качественно восстановить аудиозапись, но они плохо кодируют смысл и плохо связаны между собой. из-за этого дальнейшей модели (в нашем случае — moshi) сложно их предсказывать. семантические токены делаются ssl-моделями — здесь это wavlm. эти токены хорошо связаны между собой, они кодируют смысл сказанного в аудиозаписи. но они не предназначены для того, чтобы восстанавливать из них аудиозапись. получается, что нужны акустические токены со свойствами семантических — это то, чего авторы пытались достичь семантической дистилляцией. решение — дистиллировать семантические эмбеддинги wavlm в акустические эмбеддинги mimi. для этого нужно посчитать косинусное расстояние между эмбеддингами wavlm и mimi и использовать это как дополнительную компоненту лосса. есть одна проблема — у моделей разные герцовки: у wavlm — 50, а у moshi — 12.5, в 4 раза реже. мы не можем просто посчитать косинусное расстояние между соответствующими эмбеддингами. чтобы справиться с этим, авторы применили averagepooling со stride-ом 4 к последовательности эмбеддингов из wavlm и привели обе последовательности к одной частоте — 12,5. в следующей части разберём главное об устройстве модели moshi и алгоритма inner-monologue. роман кайль ❣ специально для speech info moshi в деталях: новая архитектура диалоговой системы в реальном времени. часть 1/2 разбираем статью о real-time dialogue-модели moshi, которая, в отличие от предыдущих диалоговых систем, объединяет в одной архитектуре три компонента: asr (распознавание речи), llm (языковая модель) и tts (синтез речи). такая схема позволяет воспринимать речь и генерировать ответ одновременно. управление тем, когда говорить и когда слушать, реализовано через специальный управляющий токен. (даже я не всегда так умею — прим. автора ). архитектура модели состоит из четырёх частей, и у всех звучные названия. в этом посте уместим разбор двух первых частей, а в следующем — ещё двух. helium простая текстовая модель, предсказывающая следующий токен. претрейн модели проводился на 2,1 трлн токенов. (для сравнения: llama 2 — 1,8 трлн, llama 3 — 15,6 трлн). данные собирали, фильтруя commoncrawl — огромный дамп интернета, где много мусора, но если хорошо почистить, получается неплохой датасет. в итоге датасет состоит из 87,5% commoncrawl и 12,5% wikipedia. после претрейна провели три дополнительных этапа обучения: пост-тренировку, файнтюнинг и инструкционное обучение — чтобы модель лучше справлялась с диалогами. по оценке авторов, helium сравнима с llama 2 и первым mistral, но не дотягивает до llama 3. mimi нейросетевой аудиокодек на основе residual vector quantization (rvq). архитектура — стандартный энкодер-декодер, но с интересными деталями: обычно в аудиокодеках используются только свёрточные слои, а тут добавили трансформеры — в конце энкодера и в начале декодера. это сделало кодек умнее (и тяжелее). ещё одно важное отличие mimi от остальных аудиокодеков — у него довольно маленькая герцовка. mimi нужно 12,5 векторов, чтобы закодировать секунду аудио. для сравнения у encodec — 75, wavtokenizer — 40. за счёт этого трансформер поверх такого кодека можно учить с бóльшим батчем (в секундах) и быстрее инферить. набор лоссов у moshi примерно такой же, как и у hifi-gan-a. единственное отличие — это то, что авторы убрали l1-loss между stft-спектрограммами, из-за плохой корреляции с человеческим восприятием. без него субъективные метрики получались лучше. главное ноу-хау mimi — семантическая дистилляция, которая позволяет получить акустические токены со свойствами семантических. акустические токены создаются кодек-моделями вроде mimi. из них можно хорошо и качественно восстановить аудиозапись, но они плохо кодируют смысл и плохо связаны между собой. из-за этого дальнейшей модели (в нашем случае — moshi) сложно их предсказывать. семантические токены делаются ssl-моделями — здесь это wavlm. эти токены хорошо связаны между собой, они кодируют смысл сказанного в аудиозаписи. но они не предназначены для того, чтобы восстанавливать из них аудиозапись. получается, что нужны акустические токены со свойствами семантических — это то, чего авторы пытались достичь семантической дистилляцией. решение — дистиллировать семантические эмбеддинги wavlm в акустические эмбеддинги mimi. для этого нужно посчитать косинусное расстояние между эмбеддингами wavlm и mimi и использовать это как дополнительную компоненту лосса. есть одна проблема — у моделей разные герцовки: у wavlm — 50, а у moshi — 12.5, в 4 раза реже. мы не можем просто посчитать косинусное расстояние между соответствующими эмбеддингами. чтобы справиться с этим, авторы применили averagepooling со stride-ом 4 к последовательности эмбеддингов из wavlm и привели обе последовательности к одной частоте — 12,5. в следующей части разберём главное об устройстве модели moshi и алгоритма inner-monologue. роман кайль ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-21T08:04:46+00:00" href="./posts/36.html">2025-05-21 08:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>Moshi в деталях: новая архитектура диалоговой системы в реальном времени. Часть 1/2 </strong><br><br>Разбираем <a href="https://arxiv.org/abs/2410.00037" rel="nofollow noopener noreferrer">статью</a> о real-time dialogue-модели Moshi, которая, в отличие от предыдущих диалоговых систем, объединяет в одной архитектуре три компонента: ASR (распознавание речи), LLM (языковая модель) и TTS (синтез речи). Такая схема позволяет воспринимать речь и генерировать ответ одновременно. Управление тем, когда говорить и когда слушать, реализовано через специальный управляющий токен. (Даже я не всегда так умею <em>— прим. автора</em>). <br><br>Архитектура модели состоит из четырёх частей, и у всех звучные названия. В этом посте уместим разбор двух первых частей, а в следующем — ещё двух.<br><br><strong>Helium</strong><br><strong><br></strong>Простая текстовая модель, предсказывающая следующий токен. <br><br>Претрейн модели проводился на 2,1 трлн токенов. (Для сравнения: Llama 2 — 1,8 трлн, Llama 3 — 15,6 трлн). Данные собирали, фильтруя CommonCrawl — огромный дамп интернета, где много мусора, но если хорошо почистить, получается неплохой датасет. В итоге датасет состоит из 87,5% CommonCrawl и 12,5% Wikipedia.<br><br>После претрейна провели три дополнительных этапа обучения: пост-тренировку, файнтюнинг и инструкционное обучение — чтобы модель лучше справлялась с диалогами. По оценке авторов, Helium сравнима с Llama 2 и первым Mistral, но не дотягивает до Llama 3.<br><br><strong>Mimi</strong><br><br>Нейросетевой аудиокодек на основе Residual Vector Quantization (RVQ). Архитектура — стандартный энкодер-декодер, но с интересными деталями: обычно в аудиокодеках используются только свёрточные слои, а тут добавили трансформеры — в конце энкодера и в начале декодера. Это сделало кодек умнее (и тяжелее).<br><br>Ещё одно важное отличие Mimi от остальных аудиокодеков — у него довольно маленькая герцовка. Mimi нужно 12,5 векторов, чтобы закодировать секунду аудио. Для сравнения у EnCodec — 75, WavTokenizer — 40. За счёт этого трансформер поверх такого кодека можно учить с бóльшим батчем (в секундах) и быстрее инферить.<br><br>Набор лоссов у Moshi примерно такой же, как и у HiFi-GAN-a. Единственное отличие — это то, что авторы убрали L1-loss между STFT-спектрограммами, из-за плохой корреляции с человеческим восприятием. Без него субъективные метрики получались лучше.<br><br>Главное ноу-хау Mimi — семантическая дистилляция, которая позволяет получить акустические токены со свойствами семантических.<br><br>Акустические токены создаются кодек-моделями вроде Mimi. Из них можно хорошо и качественно восстановить аудиозапись, но они плохо кодируют смысл и плохо связаны между собой. Из-за этого дальнейшей модели (в нашем случае — Moshi) сложно их предсказывать. <br><br>Семантические токены делаются SSL-моделями — здесь это WavLM. Эти токены хорошо связаны между собой, они кодируют смысл сказанного в аудиозаписи. Но они не предназначены для того, чтобы восстанавливать из них аудиозапись.<br><br>Получается, что нужны акустические токены со свойствами семантических — это то, чего авторы пытались достичь семантической дистилляцией.<br><br>Решение — дистиллировать семантические эмбеддинги WavLM в акустические эмбеддинги Mimi. Для этого нужно посчитать косинусное расстояние между эмбеддингами WavLM и Mimi и использовать это как дополнительную компоненту лосса. Есть одна проблема — у моделей разные герцовки: у WavLM — 50, а у Moshi — 12.5, в 4 раза реже. Мы не можем просто посчитать косинусное расстояние между соответствующими эмбеддингами. Чтобы справиться с этим, авторы применили AveragePooling со stride-ом 4 к последовательности эмбеддингов из WavLM и привели обе последовательности к одной частоте — 12,5.<br><br>В следующей части разберём главное об устройстве модели Moshi и алгоритма Inner-monologue. <br><br><em>Роман Кайль</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> <em>Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/36_480.webp" srcset="../assets/media/thumbs/36_480.webp 480w, ../assets/media/36.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="36" data-image-index="0" /></div></div>
      <div class="actions">
        <span>822 просмотров · 28 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/36" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/36.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="34" data-search="wavchat: a survey of spoken dialogue models. часть 4/4 публикуем последнюю часть классификации и выводы из большого обзора современных разговорных ии. в предыдущих сериях: 1, 2, 3. классификация по подходу к обеспечению диалоговости: стриминг, симплекс, дюплекс, полудюплекс. стриминговая модель может обрабатывать запросы пользователя прямо в процессе их появления, не дожидаясь конца монолога. например, qwen, нестриминговая модель — она ждёт, пока вы договорите, и обрабатывает всё аудио, прежде чем ответить. в настоящем же диалоге всё по-другому: собеседник слушает и осмысляет вашу реплику непрерывно, и может начать отвечать после неё с минимальной задержкой. нестриминговые модели могут работать в симплекс-подходе, когда диалог больше похож на обмен голосовыми сообщениями. полудюплекс включает в себя автоматическое определение конца речи, после которого она обрабатывается и ответ возвращается в виде аудио — как это реализовано в alexa, алисе и других умных помощниках. в полноценной дюплекс-реализации модель каждый чанк времени обрабатывает входящие реплики и генерирует выходные. когда она слушает, она генерирует тишину или поддакивает, показывая что на связи. а если её перебить, сможет естественно остановиться, договорив слово до конца. так получаются наиболее живые диалоги, но попытку сделать полноценный дюплекс на момент написания статьи сделали только в moshi. ещë в статье была часть про бенчи, которая скорее расстроила: эффективно измерять качество именно диалоговых систем пока толком не на чем. есть замеры, которые сравнивают отдельные аспекты «умности» диалоговых систем/alm. при этом у выигрывающих по бенчам статей далеко не всегда самые впечатляющие демо. если пытаться идти вглубь и смотреть, как сравниваются исходные статьи, то можно найти противоречия даже в базовых замерах. например, в задержках. авторы проделали большую работу: собрали и систематизировали значимые статьи. но аспекты, по которым они проводили систематизацию, коррелируют между собой, поэтому обзор повторяется, переходя от одного к другому. например, парадигма тренировки на 90% определяется выбором представления звука. для себя я вынес следующее: 1. spiritllm, moshi, synclm — интересные реализации диалоговых систем. 2. победят акустические токены + длительная стадия пост-претрейна на большом объёме аудио + файнтюн на диалогах (даже синтезированных). но это не точно. 3. нормальных бенчей нет. 4. дюплекс — правильный путь, но работает пока только у moshi, и то плохо. спойлер, выходящий за пределы этого обзора: кажется, у sesame.com получилось. 5. статей становится всё больше, так что будущее — за e2e-диалоговыми системами. итоговый вывод: скам статья или не скам? на мой взгляд, статья — не скам. например, с неё будет полезно начать, чтобы понять, что вообще происходит в отрасли. никита рыжиков ❣ специально для speech info wavchat: a survey of spoken dialogue models. часть 4/4 публикуем последнюю часть классификации и выводы из большого обзора современных разговорных ии. в предыдущих сериях: 1 , 2 , 3 . классификация по подходу к обеспечению диалоговости: стриминг, симплекс, дюплекс, полудюплекс. стриминговая модель может обрабатывать запросы пользователя прямо в процессе их появления, не дожидаясь конца монолога. например, qwen, нестриминговая модель — она ждёт, пока вы договорите, и обрабатывает всё аудио, прежде чем ответить. в настоящем же диалоге всё по-другому: собеседник слушает и осмысляет вашу реплику непрерывно, и может начать отвечать после неё с минимальной задержкой. нестриминговые модели могут работать в симплекс-подходе, когда диалог больше похож на обмен голосовыми сообщениями. полудюплекс включает в себя автоматическое определение конца речи, после которого она обрабатывается и ответ возвращается в виде аудио — как это реализовано в alexa, алисе и других умных помощниках. в полноценной дюплекс-реализации модель каждый чанк времени обрабатывает входящие реплики и генерирует выходные. когда она слушает, она генерирует тишину или поддакивает, показывая что на связи. а если её перебить, сможет естественно остановиться, договорив слово до конца. так получаются наиболее живые диалоги, но попытку сделать полноценный дюплекс на момент написания статьи сделали только в moshi. ещë в статье была часть про бенчи, которая скорее расстроила: эффективно измерять качество именно диалоговых систем пока толком не на чем. есть замеры, которые сравнивают отдельные аспекты «умности» диалоговых систем/alm. при этом у выигрывающих по бенчам статей далеко не всегда самые впечатляющие демо. если пытаться идти вглубь и смотреть, как сравниваются исходные статьи, то можно найти противоречия даже в базовых замерах. например, в задержках. авторы проделали большую работу: собрали и систематизировали значимые статьи. но аспекты, по которым они проводили систематизацию, коррелируют между собой, поэтому обзор повторяется, переходя от одного к другому. например, парадигма тренировки на 90% определяется выбором представления звука. для себя я вынес следующее: 1. spiritllm, moshi, synclm — интересные реализации диалоговых систем. 2. победят акустические токены + длительная стадия пост-претрейна на большом объёме аудио + файнтюн на диалогах (даже синтезированных). но это не точно. 3. нормальных бенчей нет. 4. дюплекс — правильный путь, но работает пока только у moshi, и то плохо. спойлер, выходящий за пределы этого обзора: кажется, у sesame.com получилось. 5. статей становится всё больше, так что будущее — за e2e-диалоговыми системами. итоговый вывод: скам статья или не скам? на мой взгляд, статья — не скам. например, с неё будет полезно начать, чтобы понять, что вообще происходит в отрасли. никита рыжиков ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-13T08:05:41+00:00" href="./posts/34.html">2025-05-13 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>WavChat: A Survey of Spoken Dialogue Models. Часть 4/4</strong><br><br>Публикуем последнюю часть классификации и выводы из <a href="https://arxiv.org/abs/2411.13577" rel="nofollow noopener noreferrer">большого обзора</a> современных разговорных ИИ. <br><br><em>В предыдущих сериях: </em><a href="https://t.me/speechinfo/10" rel="nofollow noopener noreferrer"><em>1</em></a><em>, </em><a href="https://t.me/speechinfo/11" rel="nofollow noopener noreferrer"><em>2</em></a><em>, </em><a href="https://t.me/speechinfo/33" rel="nofollow noopener noreferrer"><em>3</em></a><em>.<br></em><br>Классификация по подходу к обеспечению диалоговости: стриминг, симплекс, дюплекс, полудюплекс. Стриминговая модель может обрабатывать запросы пользователя прямо в процессе их появления, не дожидаясь конца монолога. Например, Qwen, нестриминговая модель — она ждёт, пока вы договорите, и обрабатывает всё аудио, прежде чем ответить. В настоящем же диалоге всё по-другому: собеседник слушает и осмысляет вашу реплику непрерывно, и может начать отвечать после неё с минимальной задержкой. Нестриминговые модели могут работать в симплекс-подходе, когда диалог больше похож на обмен голосовыми сообщениями. <br><br>Полудюплекс включает в себя автоматическое определение конца речи, после которого она обрабатывается и ответ возвращается в виде аудио — как это реализовано в Alexa, Алисе и других умных помощниках.<br><br>В полноценной дюплекс-реализации модель каждый чанк времени обрабатывает входящие реплики и генерирует выходные. Когда она слушает, она генерирует тишину или поддакивает, показывая что на связи. А если её перебить, сможет естественно остановиться, договорив слово до конца. Так получаются наиболее живые диалоги, но попытку сделать полноценный дюплекс на момент написания статьи сделали только в Moshi.<br><br>Ещë в статье была часть про бенчи, которая скорее расстроила: эффективно измерять качество именно диалоговых систем пока толком не на чем. Есть замеры, которые сравнивают отдельные аспекты «умности» диалоговых систем/ALM. При этом у выигрывающих по бенчам статей далеко не всегда самые впечатляющие демо. <br><br>Если пытаться идти вглубь и смотреть, как сравниваются исходные статьи, то можно найти противоречия даже в базовых замерах. Например, в задержках. <br><br>Авторы проделали большую работу: собрали и систематизировали значимые статьи. Но аспекты, по которым они проводили систематизацию, коррелируют между собой, поэтому обзор повторяется, переходя от одного к другому. Например, парадигма тренировки на 90% определяется выбором представления звука.<br><br>Для себя я вынес следующее:<br><br>1. SpiritLLM, Moshi, SyncLM — интересные реализации диалоговых систем.<br><br>2. Победят акустические токены + длительная стадия пост-претрейна на большом объёме аудио + файнтюн на диалогах (даже синтезированных). Но это не точно.<br><br>3. Нормальных бенчей нет.<br><br>4. Дюплекс — правильный путь, но работает пока только у Moshi, и то плохо. Спойлер, выходящий за пределы этого обзора: кажется, у <a rel="nofollow noopener noreferrer">Sesame.com</a> получилось.<br><br>5. Статей становится всё больше, так что будущее — за e2e-диалоговыми системами.<br><br>Итоговый вывод: скам статья или не скам? На мой взгляд, статья — не скам. Например, с неё будет полезно начать, чтобы понять, что вообще происходит в отрасли.<br><br><em>Никита Рыжиков</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> <em>Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/34_480.webp" srcset="../assets/media/thumbs/34_480.webp 480w, ../assets/media/34.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="34" data-image-index="0" /></div></div>
      <div class="actions">
        <span>862 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/34" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/34.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="33" data-search="wavchat: a survey of spoken dialogue models. часть 3/4 продолжаем пошагово разбираться в классификации, которую предлагают в большом обзоре актуальных разговорных систем. в предыдущих сериях: 1, 2. классификация по парадигме тренировки: использовали ли постпретрейн, какие задачи решали. основной тейк этой части классификации довольно очевидный. текстовые модели добились значительных успехов, а вот остальные (в том числе акустические), пока не могут похвастаться ни размером обучающих корпусов, ни количеством электричества, которое тратят на их обучение. для выхода из этого тупика, нужно, чтобы при добавлении аудиомодальности тренировочная парадигма позволяла использовать мощности существующих текстовых моделей. нужно научиться хорошо конвертировать входные запросы в акустические токены, а потом также хорошо оперировать ими. то есть, в обучении должны быть задачи, решение которых требует обуславливаться на аудиоконтекст. например, задача asr. авторы рассматривают разные способы объединять текстовые и аудиоданные на этапе тренировки (как на картинке). но одними из самых многообещающих считают интерливинг и chain-of-modality. оба этих подхода позволяют учиться на больших корпусах частично структурированных данных, запоминая как структуру аудиоданных, так и взаимное обусловливание речи и текста друг на друга. а вот обучение адаптера в text-only-output-парадигме на большом наборе датасетов из разных задач (как было в salmonn и qwen-audio) авторы считают не очень жизнеспособным. полноценно обучить синтезу, в отрыве от других задач, нельзя — а значит, диалог с такой моделью проиграет в естественности другим подходам. из этой части лично я сделал вывод, что в диалоговых системах критично наличие постпретрейн-стадии для аудиомодальности. во-первых, хорошие диалоговые системы, представленные в статье, по большей части основаны на этой парадигме. во-вторых, интуиция подсказывает, что за счëт такой стадии можно выиграть в выразительности синтеза и использовать большие датасеты неструктурированных аудиоданных. продолжение следует. никита рыжиков ❣ специально для speech info wavchat: a survey of spoken dialogue models. часть 3/4 продолжаем пошагово разбираться в классификации, которую предлагают в большом обзоре актуальных разговорных систем. в предыдущих сериях: 1 , 2 . классификация по парадигме тренировки: использовали ли постпретрейн, какие задачи решали. основной тейк этой части классификации довольно очевидный. текстовые модели добились значительных успехов, а вот остальные (в том числе акустические), пока не могут похвастаться ни размером обучающих корпусов, ни количеством электричества, которое тратят на их обучение. для выхода из этого тупика, нужно, чтобы при добавлении аудиомодальности тренировочная парадигма позволяла использовать мощности существующих текстовых моделей. нужно научиться хорошо конвертировать входные запросы в акустические токены, а потом также хорошо оперировать ими. то есть, в обучении должны быть задачи, решение которых требует обуславливаться на аудиоконтекст. например, задача asr. авторы рассматривают разные способы объединять текстовые и аудиоданные на этапе тренировки (как на картинке). но одними из самых многообещающих считают интерливинг и chain-of-modality. оба этих подхода позволяют учиться на больших корпусах частично структурированных данных, запоминая как структуру аудиоданных, так и взаимное обусловливание речи и текста друг на друга. а вот обучение адаптера в text-only-output-парадигме на большом наборе датасетов из разных задач (как было в salmonn и qwen-audio) авторы считают не очень жизнеспособным. полноценно обучить синтезу, в отрыве от других задач, нельзя — а значит, диалог с такой моделью проиграет в естественности другим подходам. из этой части лично я сделал вывод, что в диалоговых системах критично наличие постпретрейн-стадии для аудиомодальности. во-первых, хорошие диалоговые системы, представленные в статье, по большей части основаны на этой парадигме. во-вторых, интуиция подсказывает, что за счëт такой стадии можно выиграть в выразительности синтеза и использовать большие датасеты неструктурированных аудиоданных. продолжение следует. никита рыжиков ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-05T08:41:15+00:00" href="./posts/33.html">2025-05-05 08:41 UTC</a></div>
      </div>
      <div class="post-body"><strong>WavChat: A Survey of Spoken Dialogue Models. Часть 3/4</strong><br><br>Продолжаем пошагово разбираться в классификации, которую предлагают в <a href="https://arxiv.org/abs/2411.13577" rel="nofollow noopener noreferrer">большом обзоре</a> актуальных разговорных систем. <br><br><em>В предыдущих сериях: </em><a href="https://t.me/speechinfo/10" rel="nofollow noopener noreferrer"><em>1</em></a><em>, </em><a href="https://t.me/speechinfo/11" rel="nofollow noopener noreferrer"><em>2</em></a><em>.<br></em><br>Классификация по парадигме тренировки: использовали ли постпретрейн, какие задачи решали. Основной тейк этой части классификации довольно очевидный. Текстовые модели добились значительных успехов, а вот остальные (в том числе акустические), пока не могут похвастаться ни размером обучающих корпусов, ни количеством электричества, которое тратят на их обучение. Для выхода из этого тупика, нужно, чтобы при добавлении аудиомодальности тренировочная парадигма позволяла использовать мощности существующих текстовых моделей. Нужно научиться хорошо конвертировать входные запросы в акустические токены, а потом также хорошо оперировать ими. То есть, в обучении должны быть задачи, решение которых требует обуславливаться на аудиоконтекст. Например, задача ASR.<br><br>Авторы рассматривают разные способы объединять текстовые и аудиоданные на этапе тренировки (как на картинке). Но одними из самых многообещающих считают интерливинг и chain-of-modality.<br><br>Оба этих подхода позволяют учиться на больших корпусах частично структурированных данных, запоминая как структуру аудиоданных, так и взаимное обусловливание речи и текста друг на друга. А вот обучение адаптера в text-only-output-парадигме на большом наборе датасетов из разных задач (как было в SALMONN и Qwen-Audio) авторы считают не очень жизнеспособным. Полноценно обучить синтезу, в отрыве от других задач, нельзя — а значит, диалог с такой моделью проиграет в естественности другим подходам.<br><br>Из этой части лично я сделал вывод, что в диалоговых системах критично наличие постпретрейн-стадии для аудиомодальности. Во-первых, хорошие диалоговые системы, представленные в статье, по большей части основаны на этой парадигме. Во-вторых, интуиция подсказывает, что за счëт такой стадии можно выиграть в выразительности синтеза и использовать большие датасеты неструктурированных аудиоданных.<br><br>Продолжение следует.<br><br><em>Никита Рыжиков</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> <em>Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/33_480.webp" srcset="../assets/media/thumbs/33_480.webp 480w, ../assets/media/33.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="33" data-image-index="0" /></div></div>
      <div class="actions">
        <span>995 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/33" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/33.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="27" data-search="кто о чём, а мы — продолжаем делиться постерами с iclr! несём ещё несколько любопытных работ, которые заметили и прокомментировали наши засланные казачки. restructuring vector quantization with the rotation trick чтобы лучше реконструировать входные данные в vq-vae, нужен всего лишь простой советский... rotation trick! суть идеи в том, что за счёт поворота центры кластеров сдвигаются в разные стороны, что заставляет лучше использовать векторы в кодбуке. добавляются три строчки кода без замедления обучения — и получается качество реконструкции на уровне vq-gan. есть код — и, учитывая простоту имплементации, решение можно будет легко попробовать на практике. balrog: benchmarking agentic llm and vlm reasoning on games vision-language model dialog games for self-improvement две независимые друг от друга работы. в первой — сделали несколько процедурно генерируемых игр. стэйты игр скармливали разным llm в текстовом и визуальном формате и замеряли качество работы — лучшие результаты у gemini. во второй статье отметили, что gemini «из коробки» неплохо справляется с процедурно генерируемыми играми разной семантики. (есть предположение, что google использует текстовое описание партий игр, как источник данных на претрейне). speech robust bench: a robustness benchmark for speech recognition авторы предложили бенчмарк для сравнения моделей распознавания речи по разным аспектам: устойчивости к шуму, far-field и акценту. правда, в их формуле расчёта упоминается не самая надёжная метрика pesq, а обсудить этот выбор на месте, к сожалению, было не с кем: авторы не приехали. aria-midi: a dataset of piano midi files for symbolic music modeling собрали датасет с youtube — выбрали записи с чистой игрой на пианино. на основе них с помощью пайплайна maestro подготовили датасет, который можно использовать для генеративной музыки. также обучили модель, которая делает midi-транскрипцию аудиофайлов. бонус: на постере есть красивая визуализация со сравнением композиторов (в самой статье её нет). никита рыжиков, степан комков и влад батаев ❣ специально для speech info #yaiclr кто о чём, а мы — продолжаем делиться постерами с iclr! несём ещё несколько любопытных работ, которые заметили и прокомментировали наши засланные казачки. restructuring vector quantization with the rotation trick чтобы лучше реконструировать входные данные в vq-vae, нужен всего лишь простой советский... rotation trick! суть идеи в том, что за счёт поворота центры кластеров сдвигаются в разные стороны, что заставляет лучше использовать векторы в кодбуке. добавляются три строчки кода без замедления обучения — и получается качество реконструкции на уровне vq-gan. есть код — и, учитывая простоту имплементации, решение можно будет легко попробовать на практике. balrog: benchmarking agentic llm and vlm reasoning on games vision-language model dialog games for self-improvement две независимые друг от друга работы. в первой — сделали несколько процедурно генерируемых игр. стэйты игр скармливали разным llm в текстовом и визуальном формате и замеряли качество работы — лучшие результаты у gemini. во второй статье отметили, что gemini «из коробки» неплохо справляется с процедурно генерируемыми играми разной семантики. (есть предположение, что google использует текстовое описание партий игр, как источник данных на претрейне). speech robust bench: a robustness benchmark for speech recognition авторы предложили бенчмарк для сравнения моделей распознавания речи по разным аспектам: устойчивости к шуму, far-field и акценту. правда, в их формуле расчёта упоминается не самая надёжная метрика pesq, а обсудить этот выбор на месте, к сожалению, было не с кем: авторы не приехали. aria-midi: a dataset of piano midi files for symbolic music modeling собрали датасет с youtube — выбрали записи с чистой игрой на пианино. на основе них с помощью пайплайна maestro подготовили датасет, который можно использовать для генеративной музыки. также обучили модель, которая делает midi-транскрипцию аудиофайлов. бонус: на постере есть красивая визуализация со сравнением композиторов (в самой статье её нет). никита рыжиков, степан комков и влад батаев ❣ специально для speech info #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-28T14:35:47+00:00" href="./posts/27.html">2025-04-28 14:35 UTC</a></div>
      </div>
      <div class="post-body"><strong>Кто о чём, а мы — продолжаем делиться постерами с ICLR!</strong><br><br>Несём ещё несколько любопытных работ, которые заметили и прокомментировали наши засланные казачки.<br><br><a href="https://arxiv.org/abs/2410.06424" rel="nofollow noopener noreferrer"><strong>Restructuring Vector Quantization with the Rotation Trick</strong></a><br><br>Чтобы лучше реконструировать входные данные в VQ-VAE, нужен всего лишь простой советский... Rotation trick! Суть идеи в том, что за счёт поворота центры кластеров сдвигаются в разные стороны, что заставляет лучше использовать векторы в кодбуке. Добавляются три строчки кода без замедления обучения — и получается качество реконструкции на уровне VQ-GAN. Есть код — и, учитывая простоту имплементации, решение можно будет легко попробовать на практике.<br><br><a href="https://arxiv.org/abs/2411.13543" rel="nofollow noopener noreferrer"><strong>BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games</strong></a><strong><br></strong><br><a href="https://arxiv.org/abs/2502.02740" rel="nofollow noopener noreferrer"><strong>Vision-Language Model Dialog Games for Self-Improvement</strong></a><br><br>Две независимые друг от друга работы. В первой — сделали несколько процедурно генерируемых игр. Стэйты игр скармливали разным LLM в текстовом и визуальном формате и замеряли качество работы — лучшие результаты у Gemini. Во второй статье отметили, что Gemini «из коробки» неплохо справляется с процедурно генерируемыми играми разной семантики. (Есть предположение, что Google использует текстовое описание партий игр, как источник данных на претрейне).<br><br><a href="https://arxiv.org/html/2403.07937v2" rel="nofollow noopener noreferrer"><strong>Speech Robust Bench: A Robustness Benchmark For Speech Recognition</strong></a><br><br>Авторы предложили бенчмарк для сравнения моделей распознавания речи по разным аспектам: устойчивости к шуму, far-field и акценту. Правда, в их формуле расчёта упоминается не самая надёжная метрика PESQ, а обсудить этот выбор на месте, к сожалению, было не с кем: авторы не приехали.<br><br><a href="https://arxiv.org/abs/2504.15071" rel="nofollow noopener noreferrer"><strong>Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling</strong></a><br><br>Собрали датасет с YouTube — выбрали записи с чистой игрой на пианино. На основе них с помощью пайплайна MAESTRO подготовили датасет, который можно использовать для генеративной музыки. Также обучили модель, которая делает MIDI-транскрипцию аудиофайлов. Бонус: на постере есть красивая визуализация со сравнением композиторов (в самой статье её нет).<br><br><em>Никита Рыжиков, Степан Комков и Влад Батаев </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><em> </em><br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/27_480.webp" srcset="../assets/media/thumbs/27_480.webp 480w, ../assets/media/27.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="27" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/28_480.webp" srcset="../assets/media/thumbs/28_480.webp 480w, ../assets/media/28.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="27" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/29_480.webp" srcset="../assets/media/thumbs/29_480.webp 480w, ../assets/media/29.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="27" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/30_480.webp" srcset="../assets/media/thumbs/30_480.webp 480w, ../assets/media/30.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="27" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/31_480.webp" srcset="../assets/media/thumbs/31_480.webp 480w, ../assets/media/31.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="27" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/32_480.webp" srcset="../assets/media/thumbs/32_480.webp 480w, ../assets/media/32.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="27" data-image-index="5" /></div></div>
      <div class="actions">
        <span>841 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/27" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/27.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="26" data-search="как яндекс браузер переводит видео с сохранением оригинальных голосов перевод видео в яндекс браузере появился ещё в 2021 году. сегодня компания представляет новую версию этой технологии, способную сохранять тембр и интонации оригинального голоса. а сам перевод стал точнее благодаря yandexgpt. в статье на хабре вы можете почитать все подробности о том, как устроен инструмент, а здесь расскажем коротко. в основе технологии синтеза речи лежит модифицированная опенсорс-модель tortoise-tts. сама по себе она выдаёт результаты хорошего качества, почти неотличимые от человеческой речи. однако есть несколько проблем, которые не позволяют использовать модель в продакшене. одна из них связана с качеством zero-shot-синтеза, то есть генерации аудио тем же голосом, что и в аудиопромпте. результат может быть не похожим на исходник, а при переносе тембра с английского на русский появляется акцент. чтобы исправить это, в яндексе использовали фонемное представление текста и создали общий алфавит для английских и русских фонем. благодаря этому произношение модели стало более правильным. для моделирования тембра голоса внедрили биометрические эмбеддинги и контролировали качество речи с помощью метрики utmos. а проблему акцента при переводе с английского на русский решили с помощью синтетического датасета, где голос одного и того же человека представлен на двух языках. ещё один недостаток tortoise-tts — низкая скорость инференса, из-за которой модель и получила своё название. в яндексе оптимизировали её архитектуру, уменьшили количество итераций в диффузионной модели и применили технику дистилляции знаний. благодаря этому, генерация ответа происходит в реальном времени. sbs-тестирование показало, что новый перевод видео в яндекс браузере значительно превосходит решение elevenlabs: 62% побед против 34%. что касается исключительно озвучивания, то есть превращения текста в речь, то здесь система яндекса также впереди: 46% против 42%. speech info как яндекс браузер переводит видео с сохранением оригинальных голосов перевод видео в яндекс браузере появился ещё в 2021 году. сегодня компания представляет новую версию этой технологии, способную сохранять тембр и интонации оригинального голоса. а сам перевод стал точнее благодаря yandexgpt. в статье на хабре вы можете почитать все подробности о том, как устроен инструмент, а здесь расскажем коротко. в основе технологии синтеза речи лежит модифицированная опенсорс-модель tortoise-tts. сама по себе она выдаёт результаты хорошего качества, почти неотличимые от человеческой речи. однако есть несколько проблем, которые не позволяют использовать модель в продакшене. одна из них связана с качеством zero-shot-синтеза, то есть генерации аудио тем же голосом, что и в аудиопромпте. результат может быть не похожим на исходник, а при переносе тембра с английского на русский появляется акцент. чтобы исправить это, в яндексе использовали фонемное представление текста и создали общий алфавит для английских и русских фонем. благодаря этому произношение модели стало более правильным. для моделирования тембра голоса внедрили биометрические эмбеддинги и контролировали качество речи с помощью метрики utmos. а проблему акцента при переводе с английского на русский решили с помощью синтетического датасета, где голос одного и того же человека представлен на двух языках. ещё один недостаток tortoise-tts — низкая скорость инференса, из-за которой модель и получила своё название. в яндексе оптимизировали её архитектуру, уменьшили количество итераций в диффузионной модели и применили технику дистилляции знаний. благодаря этому, генерация ответа происходит в реальном времени. sbs-тестирование показало, что новый перевод видео в яндекс браузере значительно превосходит решение elevenlabs: 62% побед против 34%. что касается исключительно озвучивания, то есть превращения текста в речь, то здесь система яндекса также впереди: 46% против 42%. speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-28T09:59:57+00:00" href="./posts/26.html">2025-04-28 09:59 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как Яндекс Браузер переводит видео с сохранением оригинальных голосов</strong><br><br>Перевод видео в Яндекс Браузере появился ещё в 2021 году. Сегодня компания представляет новую версию этой технологии, способную сохранять тембр и интонации оригинального голоса. А сам перевод стал точнее благодаря YandexGPT. <a href="https://habr.com/ru/companies/yandex/articles/902086/" rel="nofollow noopener noreferrer">В статье на Хабре</a> вы можете почитать все подробности о том, как устроен инструмент, а здесь расскажем коротко.<br><br>В основе технологии синтеза речи лежит модифицированная опенсорс-модель Tortoise-TTS. Сама по себе она выдаёт результаты хорошего качества, почти неотличимые от человеческой речи. Однако есть несколько проблем, которые не позволяют использовать модель в продакшене. <br><br>Одна из них связана с качеством zero-shot-синтеза, то есть генерации аудио тем же голосом, что и в аудиопромпте. Результат может быть не похожим на исходник, а при переносе тембра с английского на русский появляется акцент. <br><br>Чтобы исправить это, в Яндексе использовали фонемное представление текста и создали общий алфавит для английских и русских фонем. Благодаря этому произношение модели стало более правильным. Для моделирования тембра голоса внедрили биометрические эмбеддинги и контролировали качество речи с помощью метрики UTMOS. А проблему акцента при переводе с английского на русский решили с помощью синтетического датасета, где голос одного и того же человека представлен на двух языках.<br><br>Ещё один недостаток Tortoise-TTS — низкая скорость инференса, из-за которой модель и получила своё название. В Яндексе оптимизировали её архитектуру, уменьшили количество итераций в диффузионной модели и применили технику дистилляции знаний. Благодаря этому, генерация ответа происходит в реальном времени. <br><br>SBS-тестирование показало, что новый перевод видео в Яндекс Браузере значительно превосходит решение ElevenLabs:  62% побед против 34%. Что касается исключительно озвучивания, то есть превращения текста в речь, то здесь система Яндекса также впереди:  46% против 42%. <br><br><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a></div>
      <div class="actions">
        <span>3 389 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/26" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/26.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="24" data-search="iclr 2025 в сингапуре близится к завершению в этом году конференция приняла 3704 статьи (из 11 672 поданных). для сравнения: в прошлом году их было 2 260. мы старались освещать наиболее интересные работы, а в этом посте собрали все наши материалы о iclr 2025: - прямое включение из сингапура - подборка интересных статей первого дня конференции - атмосферные фото и видео с места событий - обзор статьи “scaling transformers for low-bitrate high-quality speech coding” а ещё к нам в руки попали любопытные фото: так выглядят четыре дня на другой крупной конференции (сверху) и два дня на iclr (снизу) — интенсивность программы видна невооружённым глазом! больше разборов, интересных постеров, фото и видео с iclr вы найдёте в наших других каналах: @timeforcv, @recsyschannel, @mlunderhood, @stuffynlp. speech info #yaiclr iclr 2025 в сингапуре близится к завершению в этом году конференция приняла 3704 статьи (из 11 672 поданных). для сравнения: в прошлом году их было 2 260. мы старались освещать наиболее интересные работы, а в этом посте собрали все наши материалы о iclr 2025: - прямое включение из сингапура - подборка интересных статей первого дня конференции - атмосферные фото и видео с места событий - обзор статьи “scaling transformers for low-bitrate high-quality speech coding” а ещё к нам в руки попали любопытные фото: так выглядят четыре дня на другой крупной конференции (сверху) и два дня на iclr (снизу) — интенсивность программы видна невооружённым глазом! больше разборов, интересных постеров, фото и видео с iclr вы найдёте в наших других каналах: @timeforcv , @recsyschannel , @mlunderhood , @stuffynlp . speech info #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-27T08:04:14+00:00" href="./posts/24.html">2025-04-27 08:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>ICLR 2025 в Сингапуре близится к завершению<br></strong><br>В этом году конференция приняла 3704 статьи (из 11 672 поданных). Для сравнения: в прошлом году их было 2 260. Мы старались освещать наиболее интересные работы, а в этом посте собрали все наши материалы о ICLR 2025:<br><br>- <a href="https://t.me/speechinfo/13" rel="nofollow noopener noreferrer">Прямое включение из Сингапура</a><br>- <a href="https://t.me/speechinfo/14" rel="nofollow noopener noreferrer">Подборка интересных статей первого дня конференции</a><br>- <a href="https://t.me/speechinfo/18" rel="nofollow noopener noreferrer">Атмосферные фото и видео с места событий</a><br>- <a href="https://t.me/speechinfo/23" rel="nofollow noopener noreferrer">Обзор статьи “Scaling Transformers for Low-Bitrate High-Quality Speech Coding”</a><br><br>А ещё к нам в руки попали любопытные фото: так выглядят четыре дня на другой крупной конференции (сверху) и два дня на ICLR (снизу) — интенсивность программы видна невооружённым глазом!<br><br><em>Больше разборов, интересных постеров, фото и видео с ICLR вы найдёте в наших других каналах: </em><em>@timeforcv</em><em>, </em><em>@RecSysChannel</em><em>, </em><em>@MLunderhood</em><em>, </em><em>@stuffyNLP</em><em>.</em><br><br><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/24_480.webp" srcset="../assets/media/thumbs/24_480.webp 480w, ../assets/media/24.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="24" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/25_480.webp" srcset="../assets/media/thumbs/25_480.webp 480w, ../assets/media/25.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="24" data-image-index="1" /></div></div>
      <div class="actions">
        <span>972 просмотров · 7 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/24" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/24.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="23" data-search="scaling transformers for low-bitrate high-quality speech coding мечта многих — заменить олдскул в аудиокодеках на нормальный трансформер. у авторов это, наконец, получилось, хоть и не сразу. они обнаружили, что «трансформер в лоб» не работает — и пошли разбираться почему. на постере причину не написали, но мы спросили — ведь ml-аудитории важны не только архитектура и таблички с метриками. так вот. если обучить трансформерный кодек и проверить его через adversarial-атаку на дискриминатор, можно увидеть: искажения группируются по границам stft-патчей, которые мы снимаем с wave-формы. это значит, что трансформеры умеют подстраиваться под дискриминатор, ломая wave-форму в правильных узких областях. а эти узкие места всегда повторяются из-за регулярности периодичности. можно заметить, что сейчас мода на дискриминаторы с периодикой по простым числам. авторы действуют нестандартно: используют дискриминаторы не по числам, а по золотому сечению. говорят, что просто перебирали разные периодичности и пришли к этому решению. (ссылку на работу луки пачоли дать забыли). ещё одна находка: моменты тишины убивают layernorm-статистики, поэтому сеть учится их игнорировать. выход простой: значительно увеличивать эпсилон в формуле. в остальном — авторы честно признались, что использовали fsq, потому что это модно, а размер кодбука был спущен сверху продуктово. в целом это всё напоминает статью zoom с interspeech, где использовали достаточно большой дискриминатор — и без просадок в инференсе получили буст качества. степан комков ❣ специально для speech info #yaiclr scaling transformers for low-bitrate high-quality speech coding мечта многих — заменить олдскул в аудиокодеках на нормальный трансформер. у авторов это, наконец, получилось, хоть и не сразу. они обнаружили, что «трансформер в лоб» не работает — и пошли разбираться почему. на постере причину не написали, но мы спросили — ведь ml-аудитории важны не только архитектура и таблички с метриками. так вот. если обучить трансформерный кодек и проверить его через adversarial-атаку на дискриминатор, можно увидеть: искажения группируются по границам stft-патчей, которые мы снимаем с wave-формы. это значит, что трансформеры умеют подстраиваться под дискриминатор, ломая wave-форму в правильных узких областях. а эти узкие места всегда повторяются из-за регулярности периодичности. можно заметить, что сейчас мода на дискриминаторы с периодикой по простым числам. авторы действуют нестандартно: используют дискриминаторы не по числам, а по золотому сечению. говорят, что просто перебирали разные периодичности и пришли к этому решению. (ссылку на работу луки пачоли дать забыли). ещё одна находка: моменты тишины убивают layernorm-статистики, поэтому сеть учится их игнорировать. выход простой: значительно увеличивать эпсилон в формуле. в остальном — авторы честно признались, что использовали fsq, потому что это модно, а размер кодбука был спущен сверху продуктово. в целом это всё напоминает статью zoom с interspeech, где использовали достаточно большой дискриминатор — и без просадок в инференсе получили буст качества. степан комков ❣ специально для speech info #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-25T13:22:18+00:00" href="./posts/23.html">2025-04-25 13:22 UTC</a></div>
      </div>
      <div class="post-body"><a href="https://arxiv.org/abs/2411.19842" rel="nofollow noopener noreferrer"><strong>Scaling Transformers for Low-Bitrate High-Quality Speech Coding</strong></a><br><br>Мечта многих — заменить олдскул в аудиокодеках на нормальный трансформер. У авторов это, наконец, получилось, хоть и не сразу. Они обнаружили, что «трансформер в лоб» не работает — и пошли разбираться почему. На постере причину не написали, но мы спросили — ведь ML-аудитории важны не только архитектура и таблички с метриками.<br><br>Так вот. Если обучить трансформерный кодек и проверить его через adversarial-атаку на дискриминатор, можно увидеть: искажения группируются по границам STFT-патчей, которые мы снимаем с wave-формы. Это значит, что трансформеры умеют подстраиваться под дискриминатор, ломая wave-форму в правильных узких областях. А эти узкие места всегда повторяются из-за регулярности периодичности.<br><br>Можно заметить, что сейчас мода на дискриминаторы с периодикой по простым числам. Авторы действуют нестандартно: используют дискриминаторы не по числам, а  по золотому сечению. Говорят, что просто перебирали разные периодичности и пришли к этому решению. (Ссылку на <a href="https://onartandaesthetics.com/2016/03/02/de-divina-proportione/?utm_source=chatgpt.com" rel="nofollow noopener noreferrer">работу Луки Пачоли</a> дать забыли).<br><br>Ещё одна находка: моменты тишины убивают LayerNorm-статистики, поэтому сеть учится их игнорировать. Выход простой: значительно увеличивать эпсилон в формуле.<br><br>В остальном — авторы честно признались, что использовали FSQ, потому что это модно, а размер кодбука был спущен сверху продуктово.<br><br>В целом это всё напоминает <a href="https://arxiv.org/abs/2408.15916" rel="nofollow noopener noreferrer">статью Zoom</a> с Interspeech, где использовали достаточно большой дискриминатор — и без просадок в инференсе получили буст качества.<br><br><em>Степан Комков </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/23_480.webp" srcset="../assets/media/thumbs/23_480.webp 480w, ../assets/media/23.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="23" data-image-index="0" /></div></div>
      <div class="actions">
        <span>884 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/23" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/23.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="18" data-search="немного кадров с места событий: масштабы холлов, атмосфера докладов, фантастические виды и яркие сингапурские цветы (нейро говорит, что это ​​муссенда филиппинская). speech info #yaiclr немного кадров с места событий: масштабы холлов, атмосфера докладов, фантастические виды и яркие сингапурские цветы (нейро говорит, что это ​​муссенда филиппинская). speech info #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-25T11:06:17+00:00" href="./posts/18.html">2025-04-25 11:06 UTC</a></div>
      </div>
      <div class="post-body">Немного кадров с места событий: масштабы холлов, атмосфера докладов, фантастические виды и яркие сингапурские цветы (Нейро говорит, что это ​​Муссенда филиппинская).<br><br><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/18_480.webp" srcset="../assets/media/thumbs/18_480.webp 480w, ../assets/media/18.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="18" data-image-index="0" /><video controls preload="metadata" src="../assets/media/19_________.mp4"></video><video controls preload="metadata" src="../assets/media/20______.mp4"></video><video controls preload="metadata" src="../assets/media/21_____.mp4"></video><video controls preload="metadata" src="../assets/media/22______.mp4"></video></div></div>
      <div class="actions">
        <span>629 просмотров · 5 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/18" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/18.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="14" data-search="первый день iclr 2025: интересные статьи и один грустный тренд конференция iclr 2025 идёт полным ходом. статей на тему аудио пока не слишком много, но уже есть кое-что любопытное. не стесняйтесь писать в комментариях — о чём ещё стоит рассказать. dynamic-superb phase-2: a collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks статья про бенчмарк для spoken language моделей. представляет собой набор из множества задач по описанию разных аспектов для моделей, принимающих аудио на вход. авторы мало касаются диалогов или voice-to-voice-сценариев — фокус смещён на задачи распознавания, понимания и другие аспекты обработки аудио. число заданий увеличили с 70 до 180, и собираются сделать ещё больше. оценка построена на иерархии задач, и внутри неё значения скоров осредняются без взвешивания. была первая фаза бенчмарка, сейчас идёт вторая, а в третьей говорят о диалоговых замерах. syllablelm: learning coarse semantic units for speech language models syllablelm — дистилляция hubert с целью уменьшить число токенов и сделать их ближе к слогам. синтез речи в статье не оценивался, и авторы отметили, что рецензенты тоже грустили по этому поводу. примеры генерации из полученных токенов есть на сайте, но, судя по всему, не очень хорошие. flow matching achieves almost minimax optimal convergence рубрика «если долго сидеть на берегу реки, можно увидеть, как проплывает труп твоего врага»игнорировать ml-тренд (а именно flow matching), можно увидеть статью, его обличающую». авторы показывают, что гарантии сходимости по времени для flow matching и обычного диффузионного процесса имеют одинаковый порядок. но сравнивать эти подходы напрямую сложно: один оценивает сходимость через kl-дивергенцию, другой — через расстояние вассерштейна. и напоследок — немного печальный тренд кажется, квантовые фонды (hudson river trading, jane street, jump trading и прочие) пытаются доминировать: в выставочном центре очень много их стендов. не теряем надежды увидеть больше научных работ! никита рыжиков и степан комков ❣ специально для speech info #yaiclr первый день iclr 2025: интересные статьи и один грустный тренд конференция iclr 2025 идёт полным ходом. статей на тему аудио пока не слишком много, но уже есть кое-что любопытное. не стесняйтесь писать в комментариях — о чём ещё стоит рассказать. dynamic-superb phase-2: a collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks статья про бенчмарк для spoken language моделей. представляет собой набор из множества задач по описанию разных аспектов для моделей, принимающих аудио на вход. авторы мало касаются диалогов или voice-to-voice-сценариев — фокус смещён на задачи распознавания, понимания и другие аспекты обработки аудио. число заданий увеличили с 70 до 180, и собираются сделать ещё больше. оценка построена на иерархии задач, и внутри неё значения скоров осредняются без взвешивания. была первая фаза бенчмарка, сейчас идёт вторая, а в третьей говорят о диалоговых замерах. syllablelm: learning coarse semantic units for speech language models syllablelm — дистилляция hubert с целью уменьшить число токенов и сделать их ближе к слогам. синтез речи в статье не оценивался, и авторы отметили, что рецензенты тоже грустили по этому поводу. примеры генерации из полученных токенов есть на сайте, но, судя по всему, не очень хорошие. flow matching achieves almost minimax optimal convergence рубрика «если долго сидеть на берегу реки, можно увидеть, как проплывает труп твоего врага» игнорировать ml-тренд (а именно flow matching), можно увидеть статью, его обличающую». авторы показывают, что гарантии сходимости по времени для flow matching и обычного диффузионного процесса имеют одинаковый порядок. но сравнивать эти подходы напрямую сложно: один оценивает сходимость через kl-дивергенцию, другой — через расстояние вассерштейна. и напоследок — немного печальный тренд кажется, квантовые фонды (hudson river trading, jane street, jump trading и прочие) пытаются доминировать: в выставочном центре очень много их стендов. не теряем надежды увидеть больше научных работ! никита рыжиков и степан комков ❣ специально для speech info #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-24T12:34:15+00:00" href="./posts/14.html">2025-04-24 12:34 UTC</a></div>
      </div>
      <div class="post-body"><strong>Первый день ICLR 2025: интересные статьи и один грустный тренд</strong><br><br>Конференция ICLR 2025 идёт полным ходом. Статей на тему аудио пока не слишком много, но уже есть кое-что любопытное. Не стесняйтесь писать в комментариях — о чём ещё стоит рассказать.<br><br><a href="https://arxiv.org/abs/2411.05361" rel="nofollow noopener noreferrer"><strong>Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks</strong></a><br><br>Статья про бенчмарк для spoken language моделей. Представляет собой набор из множества задач по описанию разных аспектов для моделей, принимающих аудио на вход.<br><br>Авторы мало касаются диалогов или voice-to-voice-сценариев — фокус смещён на задачи распознавания, понимания и другие аспекты обработки аудио.<br><br>Число заданий увеличили с 70 до 180, и собираются сделать ещё больше. Оценка построена на иерархии задач, и внутри неё значения скоров осредняются без взвешивания. <br><br>Была первая фаза бенчмарка, сейчас идёт вторая, а в третьей говорят о диалоговых замерах.<br><br><a href="https://arxiv.org/abs/2410.04029" rel="nofollow noopener noreferrer"><strong>SyllableLM: Learning Coarse Semantic Units for Speech Language Models</strong></a><br><br>SyllableLM — дистилляция HuBERT с целью уменьшить число токенов и сделать их ближе к слогам. Синтез речи в статье не оценивался, и авторы отметили, что рецензенты тоже грустили по этому поводу. Примеры генерации из полученных токенов есть на сайте, но, судя по всему, не очень хорошие.<br><br><a href="https://arxiv.org/abs/2405.20879" rel="nofollow noopener noreferrer"><strong>Flow Matching Achieves Almost Minimax Optimal Convergence</strong></a><br><br>Рубрика «Если долго <del>сидеть на берегу реки, можно увидеть, как проплывает труп твоего врага»</del>игнорировать ML-тренд (а именно Flow Matching), можно увидеть статью, его обличающую». Авторы показывают, что гарантии сходимости по времени для Flow Matching и обычного диффузионного процесса имеют одинаковый порядок. Но сравнивать эти подходы напрямую сложно: один оценивает сходимость через KL-дивергенцию, другой — через расстояние Вассерштейна. <br><br><strong>И напоследок — немного печальный тренд</strong><br><br>Кажется, квантовые фонды (Hudson River Trading, Jane Street, Jump Trading и прочие) пытаются доминировать: в выставочном центре очень много их стендов. Не теряем надежды увидеть больше научных работ!<br><br><em>Никита Рыжиков и Степан Комков </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/14_480.webp" srcset="../assets/media/thumbs/14_480.webp 480w, ../assets/media/14.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="14" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/15_480.webp" srcset="../assets/media/thumbs/15_480.webp 480w, ../assets/media/15.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="14" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/16_480.webp" srcset="../assets/media/thumbs/16_480.webp 480w, ../assets/media/16.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="14" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/17_480.webp" srcset="../assets/media/thumbs/17_480.webp 480w, ../assets/media/17.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="14" data-image-index="3" /></div></div>
      <div class="actions">
        <span>761 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/14" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/14.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="13" data-search="у нас прямое включение из сингапура, где ml-инженеры из яндекса готовятся к началу iclr`25! а пока предлагаем полюбоваться огнями вечерней столицы. у нас прямое включение из сингапура, где ml-инженеры из яндекса готовятся к началу iclr`25! а пока предлагаем полюбоваться огнями вечерней столицы.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-23T12:58:48+00:00" href="./posts/13.html">2025-04-23 12:58 UTC</a></div>
      </div>
      <div class="post-body">У нас прямое включение из Сингапура, где ML-инженеры из Яндекса готовятся к началу ICLR`25! А пока предлагаем полюбоваться огнями вечерней столицы.</div>
      <div class="actions">
        <span>724 просмотров · 29 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/13" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/13.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="12" data-search="">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-23T12:58:44+00:00" href="./posts/12.html">2025-04-23 12:58 UTC</a></div>
      </div>
      <div class="post-body"><p class="muted">[без текста]</p><div class="media"><video controls preload="metadata" src="../assets/media/12.mp4"></video></div></div>
      <div class="actions">
        <span>751 просмотров · 1 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/12" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/12.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="11" data-search="wavchat: a survey of spoken dialogue models. часть 2/4 попробуем пошагово проследить классификацию, которую предлагают в большом обзоре разговорных ии. предыдущая часть классификация моделей по архитектуре: каскадность или сквозная реализация (end2end). с каскадными моделями всё стандартно. asr конвертирует пользовательский запрос в текст и передаёт в llm, а llm отправляет ответ в tts. так работает большинство голосовых помощников прошлого поколения: они очень слабы в выражении эмоций, потому что, переводя входящий запрос в текст, теряют информацию о тоне голоса, эмоциях и интонациях. в итоге ответные реплики нейросети выглядят неестественно. кроме того, при передаче данных из системы в систему добавляются паузы, что приводит к дополнительным задержками. end2end-модели должны победить эту проблему: вместо того чтобы пошагово преобразовывать каждую фразу пользователя в текст, они работают сразу с аудио и учитывают невербальный контекст беседы. авторы выделяют модели вроде salmonn или qwen-audio, которые объединяют не весь стек, а лишь его части: asr+llm и отдельная модель для tts. классификация по способу представления звука: токенизация или энкодер. перед созданием alm нужно ответить на вопрос, каким образом модель будет обрабатывать звук. есть два основных подхода. энкодеры превращают аудио в непрерывное представление эмбеддов, а токенизаторы, наоборот, в дискретную последовательность — набор токенов, с которыми можно работать аналогично текстовым. в случае с токенайзерами есть ещё одна точка ветвления — они могут быть семантическими или акустическими. семантические (например, wav2vec, best-rq, hubert и wavlm) восстанавливают участки аудио по контексту. а акустические (encodec, mimi, speechtokenizer и т.д.) сжимают и разжимают аудио до ограниченного набора токенов. авторы отмечают, что токенайзеры, особенно акустические, сегодня доминируют в публикациях и позволяют реализовать next-token prediction — ключевой механизм для alm. но в итоге склоняются к тому, что будущее — за более сложными токейнайзерами, которые объединят в себе плюсы работы с семантическими и акустическими токенами. продолжение следует. никита рыжиков ❣ специально для speech info wavchat: a survey of spoken dialogue models . часть 2/4 попробуем пошагово проследить классификацию, которую предлагают в большом обзоре разговорных ии. предыдущая часть классификация моделей по архитектуре: каскадность или сквозная реализация (end2end). с каскадными моделями всё стандартно. asr конвертирует пользовательский запрос в текст и передаёт в llm, а llm отправляет ответ в tts. так работает большинство голосовых помощников прошлого поколения: они очень слабы в выражении эмоций, потому что, переводя входящий запрос в текст, теряют информацию о тоне голоса, эмоциях и интонациях. в итоге ответные реплики нейросети выглядят неестественно. кроме того, при передаче данных из системы в систему добавляются паузы, что приводит к дополнительным задержками. end2end-модели должны победить эту проблему: вместо того чтобы пошагово преобразовывать каждую фразу пользователя в текст, они работают сразу с аудио и учитывают невербальный контекст беседы. авторы выделяют модели вроде salmonn или qwen-audio, которые объединяют не весь стек, а лишь его части: asr+llm и отдельная модель для tts. классификация по способу представления звука: токенизация или энкодер. перед созданием alm нужно ответить на вопрос, каким образом модель будет обрабатывать звук. есть два основных подхода. энкодеры превращают аудио в непрерывное представление эмбеддов, а токенизаторы, наоборот, в дискретную последовательность — набор токенов, с которыми можно работать аналогично текстовым. в случае с токенайзерами есть ещё одна точка ветвления — они могут быть семантическими или акустическими. семантические (например, wav2vec, best-rq, hubert и wavlm) восстанавливают участки аудио по контексту. а акустические (encodec, mimi, speechtokenizer и т.д.) сжимают и разжимают аудио до ограниченного набора токенов. авторы отмечают, что токенайзеры, особенно акустические, сегодня доминируют в публикациях и позволяют реализовать next-token prediction — ключевой механизм для alm. но в итоге склоняются к тому, что будущее — за более сложными токейнайзерами, которые объединят в себе плюсы работы с семантическими и акустическими токенами. продолжение следует. никита рыжиков ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-21T08:34:01+00:00" href="./posts/11.html">2025-04-21 08:34 UTC</a></div>
      </div>
      <div class="post-body"><strong>WavChat: A Survey of Spoken Dialogue Models</strong>.<strong> Часть 2/4</strong><br><br>Попробуем пошагово проследить классификацию, которую предлагают в <a href="https://arxiv.org/abs/2411.13577" rel="nofollow noopener noreferrer">большом обзоре</a> разговорных ИИ. <br><br><a href="https://t.me/speechinfo/10" rel="nofollow noopener noreferrer"><em>Предыдущая часть<br></em></a><br><strong>Классификация моделей по архитектуре: каскадность или сквозная реализация (end2end).</strong> С каскадными моделями всё стандартно. ASR конвертирует пользовательский запрос в текст и передаёт в LLM, а LLM отправляет ответ в TTS. Так работает большинство голосовых помощников прошлого поколения: они очень слабы в выражении эмоций, потому что, переводя входящий запрос в текст, теряют информацию о тоне голоса, эмоциях и интонациях. В итоге ответные реплики нейросети выглядят неестественно. Кроме того, при передаче данных из системы в систему добавляются паузы, что приводит к дополнительным задержками.<br><br>End2end-модели должны победить эту проблему: вместо того чтобы пошагово преобразовывать каждую фразу пользователя в текст, они работают сразу с аудио и учитывают невербальный контекст беседы.<br><br>Авторы выделяют модели вроде SALMONN или Qwen-Audio, которые объединяют не весь стек, а лишь его части: ASR+LLM и отдельная модель для TTS.<br><br><strong>Классификация по способу представления звука: токенизация или энкодер. </strong>Перед созданием ALM нужно ответить на вопрос, каким образом модель будет обрабатывать звук. Есть два основных подхода. Энкодеры превращают аудио в непрерывное представление эмбеддов, а токенизаторы, наоборот, в дискретную последовательность — набор токенов, с которыми можно работать аналогично текстовым.<br><br>В случае с токенайзерами есть ещё одна точка ветвления — они могут быть семантическими или акустическими. Семантические (например, wav2vec, BEST-RQ, HuBERT и WavLM) восстанавливают участки аудио по контексту. А акустические (EnCodec, Mimi, SpeechTokenizer и т.д.) сжимают и разжимают аудио до ограниченного набора токенов.<br><br>Авторы отмечают, что токенайзеры, особенно акустические, сегодня доминируют в публикациях и позволяют реализовать next-token prediction — ключевой механизм для ALM. Но в итоге склоняются к тому, что будущее — за более сложными токейнайзерами, которые объединят в себе плюсы работы с семантическими и акустическими токенами. <br><br>Продолжение следует.<br><br><em>Никита Рыжиков</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> <em>Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a></div>
      <div class="actions">
        <span>853 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/11" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/11.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="10" data-search="wavchat: a survey of spoken dialogue models. часть 1/4 сегодня поделимся суммаризацией главным из большого обзора разговорных ии. сначала он кажется неплохой попыткой систематизировать происходящее в мире alm: авторы анализируют тренды и на основе существующих публикаций пытаются понять, куда всë идёт и как было бы лучше. но в какой-то момент статья начинает повторять саму себя. тем не менее, лучшей попытки осознать происходящее мы не нашли. давайте разбираться. идея объединить аудиомодальность с llm давно будоражит умы академии и индустрии. но долгое время никто толком не мог понять, для чего это нужно. первой значимой попыткой можно назвать whisper, который заставил seq2seq-модель предсказывать не только asr, но и перевод. на диаграмме легко заметить, какой именно момент развития alm стал переломным и сделал очевидным, что нужно двигаться к разговорным моделям: когда коммьюнити узнало о gpt-4o. openai показали, как аудиомодальность может сделать диалог с llm естественным, почти бесшовным, решая между делом не только задачи распознавания синтеза, но и, например, классификацию скорости дыхания. авторы считают, что всё нужно свести к voice-to-voice диалоговому стеку. его можно собрать из последовательной работы моделей (asr-llm-tts), сделать end2end или составить из частичных фьюзов отдельных компонент. трёхстадийный каскад asr-llm-tts при этом предлагается считать бейслайном, о который нужно калиброваться. и побеждать его — учиться понимать особенности речи, воспринимать звуки, уместно отвечать или, наоборот, пропускать реплики. в статье выделяют девять навыков, которыми должны обладать диалоговые модели: - text intelligence; - speech intelligence; - audio and music generation; - audio and music understanding; - multilingual capability; - context learning; - interaction capability; - streaming latency; - multimodal capability. всё, что опубликовано по теме диалоговых систем за последний год, авторы предлагают классифицировать по разным признакам: - архитектура: end2end- и каскадные модели. - способ представления звука: токенизация или энкодер. - парадигма тренировки: использовали ли пост-претрейн, какие задачи решали. - подход к обеспечению диалоговости: стриминг, симплекс, дюплекс, полудюплекс. дальше попробуем пошагово проследить эту классификацию. продолжение следует. никита рыжиков ❣ специально для speech info wavchat: a survey of spoken dialogue models. часть 1/4 сегодня поделимся суммаризацией главным из большого обзора разговорных ии. сначала он кажется неплохой попыткой систематизировать происходящее в мире alm: авторы анализируют тренды и на основе существующих публикаций пытаются понять, куда всë идёт и как было бы лучше. но в какой-то момент статья начинает повторять саму себя. тем не менее, лучшей попытки осознать происходящее мы не нашли. давайте разбираться. идея объединить аудиомодальность с llm давно будоражит умы академии и индустрии. но долгое время никто толком не мог понять, для чего это нужно. первой значимой попыткой можно назвать whisper, который заставил seq2seq-модель предсказывать не только asr, но и перевод. на диаграмме легко заметить, какой именно момент развития alm стал переломным и сделал очевидным, что нужно двигаться к разговорным моделям: когда коммьюнити узнало о gpt-4o. openai показали, как аудиомодальность может сделать диалог с llm естественным, почти бесшовным, решая между делом не только задачи распознавания синтеза, но и, например, классификацию скорости дыхания. авторы считают, что всё нужно свести к voice-to-voice диалоговому стеку. его можно собрать из последовательной работы моделей (asr-llm-tts), сделать end2end или составить из частичных фьюзов отдельных компонент. трёхстадийный каскад asr-llm-tts при этом предлагается считать бейслайном, о который нужно калиброваться. и побеждать его — учиться понимать особенности речи, воспринимать звуки, уместно отвечать или, наоборот, пропускать реплики. в статье выделяют девять навыков, которыми должны обладать диалоговые модели: - text intelligence; - speech intelligence; - audio and music generation; - audio and music understanding; - multilingual capability; - context learning; - interaction capability; - streaming latency; - multimodal capability. всё, что опубликовано по теме диалоговых систем за последний год, авторы предлагают классифицировать по разным признакам: - архитектура: end2end- и каскадные модели. - способ представления звука: токенизация или энкодер. - парадигма тренировки: использовали ли пост-претрейн, какие задачи решали. - подход к обеспечению диалоговости: стриминг, симплекс, дюплекс, полудюплекс. дальше попробуем пошагово проследить эту классификацию. продолжение следует. никита рыжиков ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-18T08:02:45+00:00" href="./posts/10.html">2025-04-18 08:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>WavChat: A Survey of Spoken Dialogue Models. Часть 1/4</strong><br><br>Сегодня поделимся <del>суммаризацией</del> главным из <a href="https://arxiv.org/abs/2411.13577" rel="nofollow noopener noreferrer">большого обзора</a> разговорных ИИ. Сначала он кажется неплохой попыткой систематизировать происходящее в мире ALM: авторы анализируют тренды и на основе существующих публикаций пытаются понять, куда всë идёт и как было бы лучше. Но в какой-то момент статья начинает повторять саму себя. Тем не менее, лучшей попытки осознать происходящее мы не нашли. Давайте разбираться. <br><br>Идея объединить аудиомодальность с LLM давно будоражит умы академии и индустрии. Но долгое время никто толком не мог понять, для чего это нужно. Первой значимой попыткой можно назвать Whisper, который заставил seq2seq-модель предсказывать не только ASR, но и перевод. <br><br>На диаграмме легко заметить, какой именно момент развития ALM стал переломным и сделал очевидным, что нужно двигаться к разговорным моделям: когда коммьюнити узнало о GPT-4o. OpenAI показали, как аудиомодальность может сделать диалог с LLM естественным, почти бесшовным, решая между делом не только задачи распознавания синтеза, но и, например, классификацию скорости дыхания. <br><br>Авторы считают, что всё нужно свести к voice-to-voice диалоговому стеку. Его можно собрать из последовательной работы моделей (ASR-LLM-TTS), сделать end2end или составить из частичных фьюзов отдельных компонент. Трёхстадийный каскад ASR-LLM-TTS при этом предлагается считать бейслайном, о который нужно калиброваться. И побеждать его — учиться понимать особенности речи, воспринимать звуки, уместно отвечать или, наоборот, пропускать реплики. <br><br>В статье выделяют девять навыков, которыми должны обладать диалоговые модели:<br><br>- Text Intelligence;<br>- Speech Intelligence;<br>- Audio and Music Generation;<br>- Audio and Music Understanding;<br>- Multilingual Capability;<br>- Context Learning;<br>- Interaction Capability;<br>- Streaming Latency;<br>- Multimodal Capability.<br><br>Всё, что опубликовано по теме диалоговых систем за последний год, авторы предлагают классифицировать по разным признакам: <br><br>- Архитектура: end2end- и каскадные модели.<br>- Способ представления звука: токенизация или энкодер.<br>- Парадигма тренировки: использовали ли пост-претрейн, какие задачи решали.<br>- Подход к обеспечению диалоговости: стриминг, симплекс, дюплекс, полудюплекс.<br><br>Дальше попробуем пошагово проследить эту классификацию.<br><br>Продолжение следует.<br><br><em>Никита Рыжиков</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> <em>Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/10_480.webp" srcset="../assets/media/thumbs/10_480.webp 480w, ../assets/media/10.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="10" data-image-index="0" /></div></div>
      <div class="actions">
        <span>4 121 просмотров · 28 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/10" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/10.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="9" data-search="билингвальный asr — уже в станциях и чате с алисой мы с хорошими новостями — теперь алиса знает два языка: русский и английский! при этом распознавание русского не пострадало, а стало даже лучше. обновлённая алиса и поддержит диалог с носителем, и поможет улучшить навыки новичка. мы ликуем, пользователи в восторге, а вот репетиторы и всем известная сова немного грустят. евгений ганкович, руководитель группы asr, рассказал, с какими вызовами столкнулась команда: — необходимо было обучить модель, которая способна работать с новым языком, при этом критически важно было не просадить качество на русском. — домен английского для русскоговорящих пользователей специфичен и не решается с помощью открытых данных. — end-of-utterance (eou) по многим причинам работает у англоговорящих пользователей иначе. разберём, почему нужно было создавать билингвальную модель, а не обучать две отдельные. сложность решения в том, что заранее неизвестно, на каком языке поступит запрос: пользователь может начать на русском, а продолжить на английском или наоборот. в теории можно использовать классификатор: задан запрос, система определяет язык и направляет его в соответствующую модель. но чтобы точно определить язык, придётся подождать несколько секунд. к тому же такая система сложнее в поддержке и плохо справляется со смешанными языками (см. «смотря какой fabric, смотря сколько details»). выходит, что разумный путь — развивать текущий русскоязычный стек до двуязычного и использовать одну модель, которая инкапсулирует логику выбора языка. однако и здесь есть подводные камни. даже незначительное ухудшение распознавания на русском негативно скажется на пользовательском опыте. поэтому новую логику в модель нужно добавлять осторожно. причём улучшения вносятся в две ключевые части голосового стека: - end-of-utterance (eou) — модель на основе аудио и паршального распознавания, которая определяет, когда пользователь закончил говорить. - e2e seq2seq на базе трансформеров — модель распознаёт завершённый фрагмент речи на русском или английском языках. чтобы улучшить эти две компоненты, нужны данные. начать можно с открытых — но это другой домен: и акустика, и пользователи отличаются. поэтому мы привлекли отдельных людей для создания более подходящих нам данных. так собрали рабочее решение, но не сразу получили нужное качество. следующим шагом провели тесты на сотрудниках яндекса, которые использовали колонку с раскатанной технологии. на этой стадии смогли собрать ошибки, необходимые для улучшения модели. группы, на которые раскатывали технологию, росли по мере улучшения модели, а мы всё тоньше настраивали модель. по мере появления данных мы проводили эксперименты с обеими моделями, подбирая датамиксы и гиперпараметры тренировок. и в какой-то момент достигли качества для полноценного распознавания целевых запросов на английском. интересно, что в этих экспериментах получилось немного улучшить качество русского, так что исходную задачу даже перевыполнили. оставалось разобраться с eou. здесь были сложности из-за режима, в котором можно вести диалог с алисой. пользователи сценария могут делать паузы, растягивать слова, и в таких случаях обычная модель может преждевременно обрезать речь. дослушивать мы тоже не можем — это может повлиять на другие компоненты и ответы алисы сильно замедлятся. решение крылось в добавлении в пайплайн eou более робастной и стабильной модели, способной учитывать паузы и длительность речи. хотелось бы рассказать о технологии подробнее, но для этого потребуется описать весь пайплайн распознавания — если вам интересно, дайте знать в комментариях. в итоге мы получили результат, который стал важной частью большого релиза: — голосовой набор сообщений на английском языке в чате и колонке; — сценарий диалогового тренажёра на колонке: пользователи могут вести диалог с алисой, получать фидбек и переводить текст голосом. зовём протестировать, что у нас получилось: попробуйте поговорить с алисой на английском или скажите: «алиса, давай практиковать английский». евгений ганкович ❣ специально для speech info билингвальный asr — уже в станциях и чате с алисой мы с хорошими новостями — теперь алиса знает два языка: русский и английский! при этом распознавание русского не пострадало, а стало даже лучше. обновлённая алиса и поддержит диалог с носителем, и поможет улучшить навыки новичка. мы ликуем, пользователи в восторге, а вот репетиторы и всем известная сова немного грустят. евгений ганкович, руководитель группы asr, рассказал, с какими вызовами столкнулась команда: — необходимо было обучить модель, которая способна работать с новым языком, при этом критически важно было не просадить качество на русском. — домен английского для русскоговорящих пользователей специфичен и не решается с помощью открытых данных. — end-of-utterance (eou) по многим причинам работает у англоговорящих пользователей иначе. разберём, почему нужно было создавать билингвальную модель, а не обучать две отдельные. сложность решения в том, что заранее неизвестно, на каком языке поступит запрос: пользователь может начать на русском, а продолжить на английском или наоборот. в теории можно использовать классификатор: задан запрос, система определяет язык и направляет его в соответствующую модель. но чтобы точно определить язык, придётся подождать несколько секунд. к тому же такая система сложнее в поддержке и плохо справляется со смешанными языками (см. «смотря какой fabric, смотря сколько details»). выходит, что разумный путь — развивать текущий русскоязычный стек до двуязычного и использовать одну модель, которая инкапсулирует логику выбора языка. однако и здесь есть подводные камни. даже незначительное ухудшение распознавания на русском негативно скажется на пользовательском опыте. поэтому новую логику в модель нужно добавлять осторожно. причём улучшения вносятся в две ключевые части голосового стека: - end-of-utterance (eou) — модель на основе аудио и паршального распознавания, которая определяет, когда пользователь закончил говорить. - e2e seq2seq на базе трансформеров — модель распознаёт завершённый фрагмент речи на русском или английском языках. чтобы улучшить эти две компоненты, нужны данные. начать можно с открытых — но это другой домен: и акустика, и пользователи отличаются. поэтому мы привлекли отдельных людей для создания более подходящих нам данных. так собрали рабочее решение, но не сразу получили нужное качество. следующим шагом провели тесты на сотрудниках яндекса, которые использовали колонку с раскатанной технологии. на этой стадии смогли собрать ошибки, необходимые для улучшения модели. группы, на которые раскатывали технологию, росли по мере улучшения модели, а мы всё тоньше настраивали модель. по мере появления данных мы проводили эксперименты с обеими моделями, подбирая датамиксы и гиперпараметры тренировок. и в какой-то момент достигли качества для полноценного распознавания целевых запросов на английском. интересно, что в этих экспериментах получилось немного улучшить качество русского, так что исходную задачу даже перевыполнили. оставалось разобраться с eou. здесь были сложности из-за режима, в котором можно вести диалог с алисой. пользователи сценария могут делать паузы, растягивать слова, и в таких случаях обычная модель может преждевременно обрезать речь. дослушивать мы тоже не можем — это может повлиять на другие компоненты и ответы алисы сильно замедлятся. решение крылось в добавлении в пайплайн eou более робастной и стабильной модели, способной учитывать паузы и длительность речи. хотелось бы рассказать о технологии подробнее, но для этого потребуется описать весь пайплайн распознавания — если вам интересно, дайте знать в комментариях. в итоге мы получили результат, который стал важной частью большого релиза: — голосовой набор сообщений на английском языке в чате и колонке; — сценарий диалогового тренажёра на колонке: пользователи могут вести диалог с алисой, получать фидбек и переводить текст голосом. зовём протестировать, что у нас получилось: попробуйте поговорить с алисой на английском или скажите: «алиса, давай практиковать английский». евгений ганкович ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-15T12:19:55+00:00" href="./posts/9.html">2025-04-15 12:19 UTC</a></div>
      </div>
      <div class="post-body"><strong>Билингвальный ASR — уже в станциях и чате с Алисой<br></strong><br>Мы с хорошими новостями — теперь Алиса знает два языка: русский и английский! При этом распознавание русского не пострадало, а стало даже лучше. Обновлённая Алиса и поддержит диалог с носителем, и поможет улучшить навыки новичка. Мы ликуем, пользователи в восторге, а вот репетиторы и всем известная сова немного грустят.<br><br>Евгений Ганкович, руководитель группы ASR, рассказал, с какими вызовами столкнулась команда:<br><br>— Необходимо было обучить модель, которая способна работать с новым языком, при этом критически важно было не просадить качество на русском. <br>— Домен английского для русскоговорящих пользователей специфичен и не решается с помощью открытых данных. <br>— End-of-utterance (EOU) по многим причинам работает у англоговорящих пользователей иначе.<br>Разберём, почему нужно было создавать билингвальную модель, а не обучать две отдельные. <br><br>Сложность решения в том, что заранее неизвестно, на каком языке поступит запрос: пользователь может начать на русском, а продолжить на английском или наоборот.<br><br>В теории можно использовать классификатор: задан запрос, система определяет язык и направляет его в соответствующую модель. Но чтобы точно определить язык, придётся подождать несколько секунд. К тому же такая система сложнее в поддержке и плохо справляется со смешанными языками (см. «смотря какой fabric, смотря сколько details»).<br><br>Выходит, что разумный путь — развивать текущий русскоязычный стек до двуязычного и использовать одну модель, которая инкапсулирует логику выбора языка.<br><br>Однако и здесь есть подводные камни. Даже незначительное ухудшение распознавания на русском негативно скажется на пользовательском опыте. Поэтому новую логику в модель нужно добавлять осторожно. Причём улучшения вносятся в две ключевые части голосового стека:<br><br>- End-of-utterance (EOU) — модель на основе аудио и паршального распознавания, которая определяет, когда пользователь закончил говорить. <br>- E2E Seq2Seq на базе трансформеров — модель распознаёт завершённый фрагмент речи на русском или английском языках.<br><br>Чтобы улучшить эти две компоненты, нужны данные. Начать можно с открытых — но это другой домен: и акустика, и пользователи отличаются. Поэтому мы привлекли отдельных людей для создания более подходящих нам данных. Так собрали рабочее решение, но не сразу получили нужное качество.<br><br>Следующим шагом провели тесты на сотрудниках Яндекса, которые использовали колонку с раскатанной технологии. На этой стадии смогли собрать ошибки, необходимые для улучшения модели. Группы, на которые раскатывали технологию, росли по мере улучшения модели, а мы всё тоньше настраивали модель. <br><br>По мере появления данных мы проводили эксперименты с обеими моделями, подбирая датамиксы и гиперпараметры тренировок. И в какой-то момент достигли качества для полноценного распознавания целевых запросов на английском. Интересно, что в этих экспериментах получилось немного улучшить качество русского, так что исходную задачу даже перевыполнили. <br><br>Оставалось разобраться с EOU. Здесь были сложности из-за режима, в котором можно вести диалог с Алисой. Пользователи сценария могут делать паузы, растягивать слова, и в таких случаях обычная модель может преждевременно обрезать речь. Дослушивать мы тоже не можем — это может повлиять на другие компоненты и ответы Алисы сильно замедлятся.<br><br>Решение крылось в добавлении в пайплайн EoU более робастной и стабильной модели, способной учитывать паузы и длительность речи. Хотелось бы рассказать о технологии подробнее, но для этого потребуется описать весь пайплайн распознавания — если вам интересно, дайте знать в комментариях.<br><br>В итоге мы получили результат, который стал важной частью большого релиза:<br><br>— Голосовой набор сообщений на английском языке в чате и колонке; <br>— Сценарий диалогового тренажёра на колонке: пользователи могут вести диалог с Алисой, получать фидбек и переводить текст голосом. <br><br>Зовём протестировать, что у нас получилось: попробуйте поговорить с Алисой на английском или скажите: «Алиса, давай практиковать английский».<br><br><em>Евгений Ганкович </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/9_480.webp" srcset="../assets/media/thumbs/9_480.webp 480w, ../assets/media/9.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="9" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 541 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/9" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/9.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="6" data-search="mamba-модели в задачах speech enhancement заключительный пост трилогии о mamba. впервые эту архитектуру упомянули в контексте задач speech enhancement в статье «an investigation of incorporating mamba for speech enhancement». в этой работе модель устроена довольно просто: waveform domain → short-time fourier transform (stft) для перехода time-frequency domain → encoder → tf-mamba → decoder → inverse stft → waveform domain. авторы сравнивают mamba с трансформерами и показывают, что достигают того же качества, но с меньшим числом flops-операций и количеством параметров. использование mamba-блоков продолжили развивать в другой статье: «mamba-seunet: mamba unet for monaural speech enhancement», где их добавляют в u-net на этапе обработки скрытых представлений для улавливания как локальных, так и глобальных зависимостей. каждый mamba-блок — двунаправленный, что позволяет использовать информацию о будущем и прошлом. архитектура модели стандартная для u-net: состоит из нескольких downsample- и затем upsample-блоков со skip-connection между ними, как показано на картинке. рассмотрим mamba-блоки (ts-mamba) подробнее. как сказано ранее, они двунаправленные: входное представление параллельно обрабатывается блоками forward mamba и backward mamba. постпроцессинг (rmsnorm) применяется к выходам обоих блоков, затем результаты конкатенируются и прогоняются через линейный слой. формально каждый mamba-блок (forwardи backward) такой же, как и в предыдущих работах. отметим, что авторы используют mamba-блоки и по времени, и по частотам, чтобы учитывать и временные, и частотные зависимости. для экспериментов выбирают четыре варианта модели с разным количеством параметров (зависит от размерности c1 и количества ts-mamba-блоков n): — mamba-seunet (xs) — 0.99m параметров; — mamba-seunet (s) — 1.88m параметров; — mamba-seunet (m) — 3.78m параметров; — mamba-seunet (l) — 6.28m параметров. их сравнивают c такими sota-моделями, как mp-senet и semamba (упомянута в начале поста) на датасете vctk+demand. согласно замерам маленькая модель mamba-seunet (xs) показывает сопоставимое качество по метрикам csig (4.75), cbak (3.95) и covl (4.23), имея вдвое меньше параметров и в разы меньше flops-операций. для сравнения mamba-блоков с conformer- и transformer-блоками авторы используют текущий u-net, в котором заменяют ts-mamba на conformer и transformer соответственно. замеры показывают, что mamba-seunet сравним по качеству с u-net’ами, у которых conformer или transformer вместо mamba-блоков. но mamba-seunet имеет меньше flops-операций, а по количеству параметров меньше или сравнимо с u-net с conformer и transformer. код модели выложен в открытый доступ. екатерина кузина ❣ специально для speech info mamba-модели в задачах speech enhancement заключительный пост трилогии о mamba. впервые эту архитектуру упомянули в контексте задач speech enhancement в статье « an investigation of incorporating mamba for speech enhancement ». в этой работе модель устроена довольно просто: waveform domain → short-time fourier transform (stft) для перехода time-frequency domain → encoder → tf-mamba → decoder → inverse stft → waveform domain. авторы сравнивают mamba с трансформерами и показывают, что достигают того же качества, но с меньшим числом flops-операций и количеством параметров. использование mamba-блоков продолжили развивать в другой статье: « mamba-seunet: mamba unet for monaural speech enhancement », где их добавляют в u-net на этапе обработки скрытых представлений для улавливания как локальных, так и глобальных зависимостей. каждый mamba-блок — двунаправленный, что позволяет использовать информацию о будущем и прошлом. архитектура модели стандартная для u-net: состоит из нескольких downsample- и затем upsample-блоков со skip-connection между ними, как показано на картинке. рассмотрим mamba-блоки (ts-mamba) подробнее. как сказано ранее, они двунаправленные: входное представление параллельно обрабатывается блоками forward mamba и backward mamba. постпроцессинг (rmsnorm) применяется к выходам обоих блоков, затем результаты конкатенируются и прогоняются через линейный слой. формально каждый mamba-блок (forwardи backward) такой же, как и в предыдущих работах. отметим, что авторы используют mamba-блоки и по времени, и по частотам, чтобы учитывать и временные, и частотные зависимости. для экспериментов выбирают четыре варианта модели с разным количеством параметров (зависит от размерности c1 и количества ts-mamba-блоков n): — mamba-seunet (xs) — 0.99m параметров; — mamba-seunet (s) — 1.88m параметров; — mamba-seunet (m) — 3.78m параметров; — mamba-seunet (l) — 6.28m параметров. их сравнивают c такими sota-моделями, как mp-senet и semamba (упомянута в начале поста) на датасете vctk+demand. согласно замерам маленькая модель mamba-seunet (xs) показывает сопоставимое качество по метрикам csig (4.75), cbak (3.95) и covl (4.23), имея вдвое меньше параметров и в разы меньше flops-операций. для сравнения mamba-блоков с conformer- и transformer-блоками авторы используют текущий u-net, в котором заменяют ts-mamba на conformer и transformer соответственно. замеры показывают, что mamba-seunet сравним по качеству с u-net’ами, у которых conformer или transformer вместо mamba-блоков. но mamba-seunet имеет меньше flops-операций, а по количеству параметров меньше или сравнимо с u-net с conformer и transformer. код модели выложен в открытый доступ. екатерина кузина ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-31T08:06:22+00:00" href="./posts/6.html">2025-03-31 08:06 UTC</a></div>
      </div>
      <div class="post-body"><strong>Mamba-модели в задачах Speech Enhancement</strong><br><br>Заключительный пост трилогии о Mamba. Впервые эту архитектуру упомянули в контексте задач Speech Enhancement в статье «<a href="https://arxiv.org/html/2405.06573v1" rel="nofollow noopener noreferrer">An Investigation of Incorporating Mamba for Speech Enhancement</a>». <br><br>В этой работе модель устроена довольно просто: waveform domain → Short-Time Fourier Transform (STFT) для перехода Time-Frequency domain → Encoder → TF-Mamba → Decoder → Inverse STFT → waveform domain. Авторы сравнивают Mamba с трансформерами и показывают, что достигают того же качества, но с меньшим числом FLOPs-операций и количеством параметров.<br><br>Использование Mamba-блоков продолжили развивать в другой статье: «<a href="https://arxiv.org/html/2412.16626v2" rel="nofollow noopener noreferrer">Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement</a>», где их добавляют в U-Net на этапе обработки скрытых представлений для улавливания как локальных, так и глобальных зависимостей. Каждый Mamba-блок — двунаправленный, что позволяет использовать информацию о будущем и прошлом. Архитектура модели стандартная для U-Net: состоит из нескольких downsample- и затем upsample-блоков со skip-connection между ними, как показано на картинке.<br><br>Рассмотрим Mamba-блоки (TS-Mamba) подробнее. Как сказано ранее, они двунаправленные: входное представление параллельно обрабатывается блоками Forward Mamba и Backward Mamba. Постпроцессинг (RMSNorm) применяется к выходам обоих блоков, затем результаты конкатенируются и прогоняются через линейный слой. Формально каждый Mamba-блок (forwardи backward) такой же, как и в предыдущих работах. Отметим, что авторы используют Mamba-блоки и по времени, и по частотам, чтобы учитывать и временные, и частотные зависимости.<br><br>Для экспериментов выбирают четыре варианта модели с разным количеством параметров (зависит от размерности C1 и количества TS-Mamba-блоков N):<br><br>— Mamba-SEUNet (XS) — 0.99M параметров;<br>— Mamba-SEUNet (S) — 1.88M параметров;<br>— Mamba-SEUNet (M) — 3.78M параметров;<br>— Mamba-SEUNet (L) — 6.28M параметров.<br><br>Их сравнивают c такими SOTA-моделями, как MP-SENet и SEMamba (упомянута в начале поста) на датасете VCTK+DEMAND. Согласно замерам маленькая модель Mamba-SEUNet (XS) показывает сопоставимое качество по метрикам CSIG (4.75), CBAK (3.95) и COVL (4.23), имея вдвое меньше параметров и в разы меньше FLOPs-операций. <br><br>Для сравнения Mamba-блоков с conformer- и transformer-блоками авторы используют текущий U-Net, в котором заменяют TS-Mamba на conformer и transformer соответственно. Замеры показывают, что Mamba-SEUNet сравним по качеству с U-Net’ами, у которых conformer или transformer вместо Mamba-блоков. Но Mamba-SEUNet имеет меньше FLOPS-операций, а по количеству параметров меньше или сравнимо с U-Net с conformer и transformer. Код модели <a href="https://github.com/MyParadise21/Mamba-SEUNet" rel="nofollow noopener noreferrer">выложен</a> в открытый доступ.<br><br><em>Екатерина Кузина </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/6_480.webp" srcset="../assets/media/thumbs/6_480.webp 480w, ../assets/media/6.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="6" data-image-index="0" /></div></div>
      <div class="actions">
        <span>10 593 просмотров · 10 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/6" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/6.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="5" data-search="mamba-like архитектуры. часть 2/2: samba и samba-asr samba: simple hybrid state space models for efficient unlimited context language modeling samba комбинирует mamba-блоки со sliding window attention из longformer и mlp-блоками. классический samba-блок — это mamba + mlp + swa + mlp. за длинный контекст отвечают mamba-блоки, а за краткосрочные зависимости — attention, который обрабатывает данные внутри окна. оказалось, что этот подход работает,обходя llama 3 по бенчмаркам на reasoning, понимание языка и генерацию кода. авторы также измерили perplexity на наборе данных slimpajama, сравнив результаты с llama 2, другими attention-based и mamba-based моделями. по скорости обучения все примерно равны, но samba и mamba успешно работают с длинными контекстами, а у llama 2 качество резко ухудшается уже на контексте в 32k токенов. в другом эксперименте качество (perplexity) samba 1.7b, mamba 1.8b и llama 3 1.6b замеряют на proofpile и оказывается, что samba и mamba способны поддерживать контекст длиной до 1m без просадки по качеству, в то время как качество llama 3 проседает уже на контексте длиной в несколько тысяч токенов. samba-asr: state-of-the-art speech recognition leveraging structured state-space models наконец, мы дошли до samba-asr. сразу скажем, что она не имеет никакой связи с предыдущей моделью samba, а основана на стандартных mamba-блоках. модель состоит из аудиоэнкодера, собранного из mamba-блоков, и декодера, который принимает эмбеддинги из энкодера через механизм mamba-cross-connection. к ним добавляют learned positional эмбеддинги для токенов уже транскрибированного текста, всё объединяется, и предсказывается следующий текстовый токен. авторы заявляют, что это новая sota — они валидируются не на всех датасетах, но на librispeech и gigaspeech модель лидирует (в сравнении с crisperwhisper, canary и parakeet). однако модель не open-source — ну нас нет ни кода модели, ни кода обучения, поэтому сложно сказать что-то о достоверности результатов. тем не менее не исключено, что это новый игрок на asr-рынке, о котором мы ещё услышим. екатерина козлова ❣ специально для speech info mamba-like архитектуры. часть 2/2: samba и samba-asr samba: simple hybrid state space models for efficient unlimited context language modeling samba комбинирует mamba-блоки со sliding window attention из longformer и mlp-блоками. классический samba-блок — это mamba + mlp + swa + mlp. за длинный контекст отвечают mamba-блоки, а за краткосрочные зависимости — attention, который обрабатывает данные внутри окна. оказалось, что этот подход работает,обходя llama 3 по бенчмаркам на reasoning, понимание языка и генерацию кода. авторы также измерили perplexity на наборе данных slimpajama, сравнив результаты с llama 2, другими attention-based и mamba-based моделями. по скорости обучения все примерно равны, но samba и mamba успешно работают с длинными контекстами, а у llama 2 качество резко ухудшается уже на контексте в 32k токенов. в другом эксперименте качество (perplexity) samba 1.7b, mamba 1.8b и llama 3 1.6b замеряют на proofpile и оказывается, что samba и mamba способны поддерживать контекст длиной до 1m без просадки по качеству, в то время как качество llama 3 проседает уже на контексте длиной в несколько тысяч токенов. samba-asr: state-of-the-art speech recognition leveraging structured state-space models наконец, мы дошли до samba-asr. сразу скажем, что она не имеет никакой связи с предыдущей моделью samba, а основана на стандартных mamba-блоках. модель состоит из аудиоэнкодера, собранного из mamba-блоков, и декодера, который принимает эмбеддинги из энкодера через механизм mamba-cross-connection. к ним добавляют learned positional эмбеддинги для токенов уже транскрибированного текста, всё объединяется, и предсказывается следующий текстовый токен. авторы заявляют, что это новая sota — они валидируются не на всех датасетах, но на librispeech и gigaspeech модель лидирует (в сравнении с crisperwhisper, canary и parakeet). однако модель не open-source — ну нас нет ни кода модели, ни кода обучения, поэтому сложно сказать что-то о достоверности результатов. тем не менее не исключено, что это новый игрок на asr-рынке, о котором мы ещё услышим. екатерина козлова ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-24T08:07:59+00:00" href="./posts/5.html">2025-03-24 08:07 UTC</a></div>
      </div>
      <div class="post-body"><strong>Mamba-like архитектуры. Часть 2/2: Samba и Samba-ASR</strong><br><a href="https://arxiv.org/abs/2406.07522" rel="nofollow noopener noreferrer"><br><strong>Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling</strong></a><br><br>Samba комбинирует Mamba-блоки со sliding window attention из Longformer и MLP-блоками. Классический Samba-блок — это Mamba + MLP + SWA + MLP. За длинный контекст отвечают Mamba-блоки, а за краткосрочные зависимости — attention, который обрабатывает данные внутри окна.<br><br>Оказалось, что этот подход работает,обходя Llama 3 по бенчмаркам на reasoning, понимание языка и генерацию кода. Авторы также измерили perplexity на наборе данных SlimPajama, сравнив результаты с Llama 2, другими attention-based и Mamba-based моделями. По скорости обучения все примерно равны, но Samba и Mamba успешно работают с длинными контекстами, а у Llama 2 качество резко ухудшается уже на контексте в 32k токенов. <br><br>В другом эксперименте качество (perplexity) Samba 1.7B, Mamba 1.8B и Llama 3 1.6B замеряют на ProofPile и оказывается, что Samba и Mamba способны поддерживать контекст длиной до 1M без просадки по качеству, в то время как качество Llama 3 проседает уже на контексте длиной в несколько тысяч токенов.<br><br><a href="https://arxiv.org/abs/2501.02832" rel="nofollow noopener noreferrer"><strong>Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured State-Space Models</strong></a><strong><br></strong><br>Наконец, мы дошли до Samba-ASR. Сразу скажем, что она не имеет никакой связи с предыдущей моделью Samba, а основана на стандартных Mamba-блоках.<br><br>Модель состоит из аудиоэнкодера, собранного из Mamba-блоков, и декодера, который принимает эмбеддинги из энкодера через механизм Mamba-cross-connection. К ним добавляют learned positional эмбеддинги для токенов уже транскрибированного текста, всё объединяется, и предсказывается следующий текстовый токен.<br><br>Авторы заявляют, что это новая SOTA — они валидируются не на всех датасетах, но на LibriSpeech и GigaSpeech модель лидирует (в сравнении с CrisperWhisper, Canary и Parakeet). Однако модель не open-source — ну нас нет ни кода модели, ни кода обучения, поэтому сложно сказать что-то о достоверности результатов. Тем не менее не исключено, что это новый игрок на ASR-рынке, о котором мы ещё услышим.<br><br><em>Екатерина Козлова</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> <em>Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a></div>
      <div class="actions">
        <span>797 просмотров · 10 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/5" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/5.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="4" data-search="mamba-like архитектуры. часть 1/2: mamba и jamba сегодня разберём четыре архитектуры, которые основаны на идее state space models (ssm). одна их них используется в задаче asr. mamba: linear-time sequence modeling with selective state spaces в этой статье авторы развивают идею ssm, дополняя классическую архитектуру «механизмом выбора» (selection mechanism). анализируя предыдущие работы с ssm-like-архитектурами, авторы приходят к выводу, что именно возможность mamba отбирать наиболее важные входы (selection in an input-dependent manner) позволяет ей достигать уровня трансформера на задачах моделирования, при этом сохраняя свою линейную сложность. при анализе современных моделей, работающих с длинным контекстом, авторы делят их на efficient и effective. первые — быстрые благодаря небольшому state, вторые — с крупным state, способные хранить больше информации. авторы стремятся найти баланс — сделать обработку быстрой, но при этом сохранить важные детали. именно для этого и используется selection mechanism. в базовых ssm матрицы состояний (b и c) имели размер d × n, где d — размерность эмбеддингов, а n — размерность скрытого состояния. теперь их «развернули во времени» — в новые матрицы состояний добавили новую временную размерность, следовательно, их новый размер — b × l × n. это дало модели некоторое понимание временного контекста. в стандартном ssm-подходе свёрточная и рекуррентная модели эквивалентны. здесь же — свёрточное представление теряется из-за появления input dependency, и возникает сканирование (scan) — матрицы состояния теперь меняются в зависимости от времени. mamba-блок получается в результате микса старых и новых идей. берётся h3-блок — это первый блок в ssm-моделях старого (не input-dependent) подхода, в него добавляется selection mechanism; модифицированный h3-блок миксуют с gated mlp. полученные mamba-блоки впоследствии либо совмещают друг с другом (классическая mamba), либо смешивают с attention’ом в разных пропорциях. эти эксперименты описаны в следующих статьях. jamba: a hybrid transformer-mamba language model jamba — попытка смешать mamba-блоки с attention, получить хорошее качество и большое количество токенов в секунду на гигантском контексте. в основе — комбинация слоёв: трансформерного, mamba-слоя и смеси экспертов (moe). их стакают в разных пропорциях, лучшим оказывается соотношение 1:7 (на каждый блок трансформера приходится 7 mamba-блоков); при этом каждый второй из mamba-блоков — это mamba-moe с 16 экспертами. у mamba без attention возникали сложности с задачами, где важен жёсткий формат ответа, а также с in-context learning. jamba решает эти проблемы: — mamba-слои и эксперты позволяют работать с длинным контекстом; — attention-слой помогает справляться с in-context learning и строгими форматами ответов. по бенчмаркам, связанным с качеством, jamba оказывается на уровне mistral 8x7b, при этом побеждая llama 2 13b и llama 2 70b; при этом по пропускной способности jamba побеждает всех конкурентов с большим перевесом, обеспечивая пропускную способность в 1500 токенов в секунду на контексте 128k. даёт jamba-подход и прирост на бенчмарках на следование формату. в imdb модель должна отвечать одной из двух категорий: positive или negative. классическая mamba не всегда следовала формату и периодически давала случайные ответы, например, «3 из 10». но при смешивании mamba с attention эта проблема исчезала — оценка на этих бенчмарках выравнивалась. екатерина козлова ❣ специально для speech info mamba-like архитектуры. часть 1/2: mamba и jamba сегодня разберём четыре архитектуры, которые основаны на идее state space models (ssm). одна их них используется в задаче asr. mamba: linear-time sequence modeling with selective state spaces в этой статье авторы развивают идею ssm, дополняя классическую архитектуру «механизмом выбора» (selection mechanism). анализируя предыдущие работы с ssm-like-архитектурами, авторы приходят к выводу, что именно возможность mamba отбирать наиболее важные входы (selection in an input-dependent manner) позволяет ей достигать уровня трансформера на задачах моделирования, при этом сохраняя свою линейную сложность. при анализе современных моделей, работающих с длинным контекстом, авторы делят их на efficient и effective. первые — быстрые благодаря небольшому state, вторые — с крупным state, способные хранить больше информации. авторы стремятся найти баланс — сделать обработку быстрой, но при этом сохранить важные детали. именно для этого и используется selection mechanism. в базовых ssm матрицы состояний (b и c) имели размер d × n, где d — размерность эмбеддингов, а n — размерность скрытого состояния. теперь их «развернули во времени» — в новые матрицы состояний добавили новую временную размерность, следовательно, их новый размер — b × l × n. это дало модели некоторое понимание временного контекста. в стандартном ssm-подходе свёрточная и рекуррентная модели эквивалентны. здесь же — свёрточное представление теряется из-за появления input dependency, и возникает сканирование (scan) — матрицы состояния теперь меняются в зависимости от времени. mamba-блок получается в результате микса старых и новых идей. берётся h3-блок — это первый блок в ssm-моделях старого (не input-dependent) подхода, в него добавляется selection mechanism; модифицированный h3-блок миксуют с gated mlp. полученные mamba-блоки впоследствии либо совмещают друг с другом (классическая mamba), либо смешивают с attention’ом в разных пропорциях. эти эксперименты описаны в следующих статьях. jamba: a hybrid transformer-mamba language model jamba — попытка смешать mamba-блоки с attention, получить хорошее качество и большое количество токенов в секунду на гигантском контексте. в основе — комбинация слоёв: трансформерного, mamba-слоя и смеси экспертов (moe). их стакают в разных пропорциях, лучшим оказывается соотношение 1:7 (на каждый блок трансформера приходится 7 mamba-блоков); при этом каждый второй из mamba-блоков — это mamba-moe с 16 экспертами. у mamba без attention возникали сложности с задачами, где важен жёсткий формат ответа, а также с in-context learning. jamba решает эти проблемы: — mamba-слои и эксперты позволяют работать с длинным контекстом; — attention-слой помогает справляться с in-context learning и строгими форматами ответов. по бенчмаркам, связанным с качеством, jamba оказывается на уровне mistral 8x7b, при этом побеждая llama 2 13b и llama 2 70b; при этом по пропускной способности jamba побеждает всех конкурентов с большим перевесом, обеспечивая пропускную способность в 1500 токенов в секунду на контексте 128k. даёт jamba-подход и прирост на бенчмарках на следование формату. в imdb модель должна отвечать одной из двух категорий: positive или negative. классическая mamba не всегда следовала формату и периодически давала случайные ответы, например, «3 из 10». но при смешивании mamba с attention эта проблема исчезала — оценка на этих бенчмарках выравнивалась. екатерина козлова ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-24T08:07:59+00:00" href="./posts/4.html">2025-03-24 08:07 UTC</a></div>
      </div>
      <div class="post-body"><strong>Mamba-like архитектуры. Часть 1/2: Mamba и Jamba</strong><br><br>Сегодня разберём четыре архитектуры, которые основаны на идее State Space Models (SSM). Одна их них используется в задаче ASR. <br><br><a href="https://arxiv.org/abs/2312.00752" rel="nofollow noopener noreferrer"><strong>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</strong></a><strong><br></strong><br>В этой статье авторы развивают идею SSM, дополняя классическую архитектуру «механизмом выбора» (selection mechanism). Анализируя предыдущие работы с SSM-like-архитектурами, авторы приходят к выводу, что именно возможность Mamba отбирать наиболее важные входы (selection in an input-dependent manner) позволяет ей достигать уровня трансформера на задачах моделирования, при этом сохраняя свою линейную сложность. <br><br>При анализе современных моделей, работающих с длинным контекстом, авторы делят их на efficient и effective. Первые — быстрые благодаря небольшому state, вторые — с крупным state, способные хранить больше информации. Авторы стремятся найти баланс — сделать обработку быстрой, но при этом сохранить важные детали. Именно для этого и используется selection mechanism.<br><br>В базовых SSM матрицы состояний (B и C) имели размер D × N, где D — размерность эмбеддингов, а N — размерность скрытого состояния. Теперь их «развернули во времени» — в новые матрицы состояний добавили новую временную размерность, следовательно, их новый размер — B × L × N. Это дало модели некоторое понимание временного контекста.<br><br>В стандартном SSM-подходе свёрточная и рекуррентная модели эквивалентны. Здесь же — свёрточное представление теряется из-за появления input dependency, и возникает сканирование (scan) — матрицы состояния теперь меняются в зависимости от времени.<br><br>Mamba-блок получается в результате микса старых и новых идей. Берётся H3-блок — это первый блок в SSM-моделях старого (не input-dependent) подхода, в него добавляется selection mechanism; модифицированный H3-блок миксуют с Gated MLP. Полученные Mamba-блоки впоследствии либо совмещают друг с другом (классическая Mamba), либо смешивают с attention’ом в разных пропорциях. Эти эксперименты описаны в следующих статьях.<br><br><a href="https://arxiv.org/abs/2403.19887" rel="nofollow noopener noreferrer"><strong>Jamba: A Hybrid Transformer-Mamba Language Model</strong></a><br><br>Jamba — попытка смешать Mamba-блоки с attention, получить хорошее качество и большое количество токенов в секунду на гигантском контексте.<br><br>В основе — комбинация слоёв: трансформерного, Mamba-слоя и смеси экспертов (MoE). Их стакают в разных пропорциях, лучшим оказывается соотношение 1:7 (на каждый блок трансформера приходится 7 Mamba-блоков); при этом каждый второй из Mamba-блоков — это Mamba-MoE с 16 экспертами.<br><br>У Mamba без attention возникали сложности с задачами, где важен жёсткий формат ответа, а также с in-context learning. Jamba решает эти проблемы:<br>— Mamba-слои и эксперты позволяют работать с длинным контекстом;<br>— Attention-слой помогает справляться с in-context learning и строгими форматами ответов.<br><br>По бенчмаркам, связанным с качеством, Jamba оказывается на уровне Mistral 8x7B, при этом побеждая Llama 2 13B и Llama 2 70B; при этом по пропускной способности Jamba побеждает всех конкурентов с большим перевесом, обеспечивая пропускную способность в 1500 токенов в секунду на контексте 128k.<br><br>Даёт Jamba-подход и прирост на бенчмарках на следование формату. В IMDB модель должна отвечать одной из двух категорий: positive или negative. Классическая Mamba не всегда следовала формату и периодически давала случайные ответы, например, «3 из 10». Но при смешивании Mamba с attention эта проблема исчезала — оценка на этих бенчмарках выравнивалась.<br><br><em>Екатерина Козлова</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> <em>Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/4_480.webp" srcset="../assets/media/thumbs/4_480.webp 480w, ../assets/media/4.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="4" data-image-index="0" /></div></div>
      <div class="actions">
        <span>744 просмотров · 15 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/4" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/4.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="3" data-search="устройство state space models начинаем делиться полезным в этом канале с обзорного рассказа о state space models (ssm). предлагаем стартовать с погружения в их устройство, а в следующем посте — посмотрим на разные архитектуры и то, как одну из них применяют в asr. классическая state space model — это непрерывная динамическая модель, имеющая вид дифференциального уравнения. входы модели u проходят через матрицу b и вносят свой вклад в скрытые представления модели x; выходы модели при этом — смесь входов и скрытых представлений: x’ = ax + bu y = cx + du поскольку в реальном мире данные дискретные, исходную непрерывную модель нужно тоже сделать дискретной: для этого входной сигнал рассматривают как последовательность, а дифференциальное уравнение — как рекуррентное уравнение, которое позволит пошагово преобразовывать входы в выходы. при этом от необходимости дифференцировать избавляются через метод трапеций: в итоге матрицы состояний дискретной системы выражаются через матрицы состояний непрерывной системы. когда получено дискретное представление, по сути, мы имеем обычную рекуррентную нейросеть. скрытое состояние на каждом шаге преобразуется, используя предыдущее состояние, а входы при этом рассматриваются последовательно. рекуррентное представление полезно из-за эффективного инференса: не нужно пересчитывать всё заново, потому что state модели обновляется шаг за шагом. если же мы хотим распараллелить вычисления во время обучения модели, рекуррентное представление нам не подойдёт — именно поэтому бывает полезно представлять ssm как свёрточную сеть. если «развернуть» рекуррентное уравнение и расписать все слагаемые, входящие в последний выход модели, окажется, что этот выход можно представить как сумму всех входов, умноженных на некоторый набор матриц. его и можно считать ядром свёртки. тем самым мы получили возможность работать со всеми входами сразу, а значит, эффективно тренировать модель. подробности о ядре свёртки можно почитать тут, ​​а о том, как должна выглядеть матрица а, — здесь. в следующем посте мы рассмотрим ряд моделей, которые используют именно такой подход к дискретизации, а также разберём применение подобной архитектуры в asr. екатерина козлова ❣ специально для speech info устройство state space models начинаем делиться полезным в этом канале с обзорного рассказа о state space models (ssm). предлагаем стартовать с погружения в их устройство, а в следующем посте — посмотрим на разные архитектуры и то, как одну из них применяют в asr. классическая state space model — это непрерывная динамическая модель, имеющая вид дифференциального уравнения. входы модели u проходят через матрицу b и вносят свой вклад в скрытые представления модели x; выходы модели при этом — смесь входов и скрытых представлений: x’ = ax + bu y = cx + du {} поскольку в реальном мире данные дискретные, исходную непрерывную модель нужно тоже сделать дискретной: для этого входной сигнал рассматривают как последовательность, а дифференциальное уравнение — как рекуррентное уравнение, которое позволит пошагово преобразовывать входы в выходы. при этом от необходимости дифференцировать избавляются через метод трапеций: в итоге матрицы состояний дискретной системы выражаются через матрицы состояний непрерывной системы. когда получено дискретное представление, по сути, мы имеем обычную рекуррентную нейросеть. скрытое состояние на каждом шаге преобразуется, используя предыдущее состояние, а входы при этом рассматриваются последовательно. рекуррентное представление полезно из-за эффективного инференса: не нужно пересчитывать всё заново, потому что state модели обновляется шаг за шагом. если же мы хотим распараллелить вычисления во время обучения модели, рекуррентное представление нам не подойдёт — именно поэтому бывает полезно представлять ssm как свёрточную сеть. если «развернуть» рекуррентное уравнение и расписать все слагаемые, входящие в последний выход модели, окажется, что этот выход можно представить как сумму всех входов, умноженных на некоторый набор матриц. его и можно считать ядром свёртки. тем самым мы получили возможность работать со всеми входами сразу, а значит, эффективно тренировать модель. подробности о ядре свёртки можно почитать тут , ​​а о том, как должна выглядеть матрица а, — здесь . в следующем посте мы рассмотрим ряд моделей, которые используют именно такой подход к дискретизации, а также разберём применение подобной архитектуры в asr. екатерина козлова ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-21T08:04:07+00:00" href="./posts/3.html">2025-03-21 08:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>Устройство State Space Models<br></strong><br>Начинаем делиться полезным в этом канале с обзорного рассказа о State Space Models (SSM). Предлагаем стартовать с погружения в их устройство, а в следующем посте — посмотрим на разные архитектуры и то, как одну из них применяют в ASR.<br><br>Классическая State Space Model — это непрерывная динамическая модель, имеющая вид дифференциального уравнения. Входы модели u проходят через матрицу B и вносят свой вклад в скрытые представления модели x; выходы модели при этом — смесь входов и скрытых представлений:<br><br><pre><br>    <code class="language-"><br>        <br>x’ = Ax + Bu<br>y = Cx + Du<br>{}<br>    </code><br></pre><br><br>Поскольку в реальном мире данные дискретные, исходную непрерывную модель нужно тоже сделать дискретной: для этого входной сигнал рассматривают как последовательность, а дифференциальное уравнение — как рекуррентное уравнение, которое позволит пошагово преобразовывать входы в выходы. При этом от необходимости дифференцировать избавляются через метод трапеций: в итоге матрицы состояний дискретной системы выражаются через матрицы состояний непрерывной системы. <br><br>Когда получено дискретное представление, по сути, мы имеем обычную рекуррентную нейросеть. Скрытое состояние на каждом шаге преобразуется, используя предыдущее состояние, а входы при этом рассматриваются последовательно. Рекуррентное представление полезно из-за эффективного инференса: не нужно пересчитывать всё заново, потому что state модели обновляется шаг за шагом. <br><br>Если же мы хотим распараллелить вычисления во время обучения модели, рекуррентное представление нам не подойдёт — именно поэтому бывает полезно представлять SSM как свёрточную сеть. Если «развернуть» рекуррентное уравнение и расписать все слагаемые, входящие в последний выход модели, окажется, что этот выход можно представить как сумму всех входов, умноженных на некоторый набор матриц. Его и можно считать ядром свёртки. Тем самым мы получили возможность работать со всеми входами сразу, а значит, эффективно тренировать модель. Подробности о ядре свёртки можно почитать <a href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-3" rel="nofollow noopener noreferrer">тут</a>, ​​а о том, как должна выглядеть матрица А, — <a href="https://huggingface.co/blog/lbourdois/get-on-the-ssm-train#learning-matrices" rel="nofollow noopener noreferrer">здесь</a>.<br><br>В следующем посте мы рассмотрим ряд моделей, которые используют именно такой подход к дискретизации, а также разберём применение подобной архитектуры в ASR.<br><br><em>Екатерина Козлова</em> <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em> <em>Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/3_480.webp" srcset="../assets/media/thumbs/3_480.webp 480w, ../assets/media/3.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="3" data-image-index="0" /></div></div>
      <div class="actions">
        <span>724 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/3" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/3.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="2" data-search="пш-пш... как слышно? добро пожаловать в @speechinfo — главный таблоид о распознавании речи, генеративном аудио и других голосовых технологиях. мы, инженеры из яндекса, не просто читаем свежие статьи по ml и аудио, а ещё разбираем их, ищем подводные камни и иногда выносим вердикт: полезна ли публикация. подписывайтесь, если хотите держать руку на пульсе и просто душевно болтать об аудио в хорошей компании. пш-пш... как слышно? добро пожаловать в @speechinfo — главный таблоид о распознавании речи, генеративном аудио и других голосовых технологиях. мы, инженеры из яндекса, не просто читаем свежие статьи по ml и аудио, а ещё разбираем их, ищем подводные камни и иногда выносим вердикт: полезна ли публикация. подписывайтесь, если хотите держать руку на пульсе и просто душевно болтать об аудио в хорошей компании.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-20T08:49:38+00:00" href="./posts/2.html">2025-03-20 08:49 UTC</a></div>
      </div>
      <div class="post-body">ПШ-ПШ... Как слышно? Добро пожаловать в @speechinfo — главный таблоид о распознавании речи, генеративном аудио и других голосовых технологиях. Мы, инженеры из Яндекса, не просто читаем свежие статьи по ML и аудио, а ещё разбираем их, ищем подводные камни и иногда выносим вердикт: полезна ли публикация.<br><br>Подписывайтесь, если хотите держать руку на пульсе и просто душевно болтать об аудио в хорошей компании.</div>
      <div class="actions">
        <span>761 просмотров · 8 реакций</span>
        <span class="action-links"><a href="https://t.me/speechinfo/2" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/2.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="index.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link current" href="page-2.html">2</a>
        <a class="nav-link disabled" href="#">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 50, "media": [{"kind": "photo", "path": "../assets/media/50.jpg", "thumb": "../assets/media/thumbs/50_480.webp", "size": 38967, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/51.jpg", "thumb": "../assets/media/thumbs/51_480.webp", "size": 40708, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/52.jpg", "thumb": "../assets/media/thumbs/52_480.webp", "size": 172121, "mime": "image/jpeg", "name": null}]}, {"id": 46, "media": [{"kind": "photo", "path": "../assets/media/46.jpg", "thumb": "../assets/media/thumbs/46_480.webp", "size": 83717, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/47.jpg", "thumb": "../assets/media/thumbs/47_480.webp", "size": 56348, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/48.jpg", "thumb": "../assets/media/thumbs/48_480.webp", "size": 50256, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/49.jpg", "thumb": "../assets/media/thumbs/49_480.webp", "size": 74346, "mime": "image/jpeg", "name": null}]}, {"id": 45, "media": []}, {"id": 41, "media": [{"kind": "photo", "path": "../assets/media/41.jpg", "thumb": "../assets/media/thumbs/41_480.webp", "size": 24265, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/42.jpg", "thumb": "../assets/media/thumbs/42_480.webp", "size": 168376, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/43.jpg", "thumb": "../assets/media/thumbs/43_480.webp", "size": 98936, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/44.jpg", "thumb": "../assets/media/thumbs/44_480.webp", "size": 104181, "mime": "image/jpeg", "name": null}]}, {"id": 39, "media": [{"kind": "photo", "path": "../assets/media/39.jpg", "thumb": "../assets/media/thumbs/39_480.webp", "size": 69370, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/40.jpg", "thumb": "../assets/media/thumbs/40_480.webp", "size": 81250, "mime": "image/jpeg", "name": null}]}, {"id": 38, "media": [{"kind": "photo", "path": "../assets/media/38.jpg", "thumb": "../assets/media/thumbs/38_480.webp", "size": 35543, "mime": "image/jpeg", "name": null}]}, {"id": 37, "media": [{"kind": "photo", "path": "../assets/media/37.jpg", "thumb": "../assets/media/thumbs/37_480.webp", "size": 113182, "mime": "image/jpeg", "name": null}]}, {"id": 36, "media": [{"kind": "photo", "path": "../assets/media/36.jpg", "thumb": "../assets/media/thumbs/36_480.webp", "size": 57444, "mime": "image/jpeg", "name": null}]}, {"id": 34, "media": [{"kind": "photo", "path": "../assets/media/34.jpg", "thumb": "../assets/media/thumbs/34_480.webp", "size": 44443, "mime": "image/jpeg", "name": null}]}, {"id": 33, "media": [{"kind": "photo", "path": "../assets/media/33.jpg", "thumb": "../assets/media/thumbs/33_480.webp", "size": 62906, "mime": "image/jpeg", "name": null}]}, {"id": 27, "media": [{"kind": "photo", "path": "../assets/media/27.jpg", "thumb": "../assets/media/thumbs/27_480.webp", "size": 177590, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/28.jpg", "thumb": "../assets/media/thumbs/28_480.webp", "size": 206157, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/29.jpg", "thumb": "../assets/media/thumbs/29_480.webp", "size": 277554, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/30.jpg", "thumb": "../assets/media/thumbs/30_480.webp", "size": 200610, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/31.jpg", "thumb": "../assets/media/thumbs/31_480.webp", "size": 124264, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/32.jpg", "thumb": "../assets/media/thumbs/32_480.webp", "size": 148504, "mime": "image/jpeg", "name": null}]}, {"id": 26, "media": []}, {"id": 24, "media": [{"kind": "photo", "path": "../assets/media/24.jpg", "thumb": "../assets/media/thumbs/24_480.webp", "size": 188646, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/25.jpg", "thumb": "../assets/media/thumbs/25_480.webp", "size": 118288, "mime": "image/jpeg", "name": null}]}, {"id": 23, "media": [{"kind": "photo", "path": "../assets/media/23.jpg", "thumb": "../assets/media/thumbs/23_480.webp", "size": 247421, "mime": "image/jpeg", "name": null}]}, {"id": 18, "media": [{"kind": "photo", "path": "../assets/media/18.jpg", "thumb": "../assets/media/thumbs/18_480.webp", "size": 152411, "mime": "image/jpeg", "name": null}, {"kind": "video", "path": "../assets/media/19_________.mp4", "thumb": null, "size": 2738830, "mime": "video/mp4", "name": "________.mp4"}, {"kind": "video", "path": "../assets/media/20______.mp4", "thumb": null, "size": 1826876, "mime": "video/mp4", "name": "_____.mp4"}, {"kind": "video", "path": "../assets/media/21_____.mp4", "thumb": null, "size": 2990800, "mime": "video/mp4", "name": "____.mp4"}, {"kind": "video", "path": "../assets/media/22______.mp4", "thumb": null, "size": 2936431, "mime": "video/mp4", "name": "_____.mp4"}]}, {"id": 14, "media": [{"kind": "photo", "path": "../assets/media/14.jpg", "thumb": "../assets/media/thumbs/14_480.webp", "size": 189839, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/15.jpg", "thumb": "../assets/media/thumbs/15_480.webp", "size": 172276, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/16.jpg", "thumb": "../assets/media/thumbs/16_480.webp", "size": 203322, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/17.jpg", "thumb": "../assets/media/thumbs/17_480.webp", "size": 198101, "mime": "image/jpeg", "name": null}]}, {"id": 13, "media": []}, {"id": 12, "media": [{"kind": "video", "path": "../assets/media/12.mp4", "thumb": null, "size": 4230173, "mime": "video/mp4", "name": null}]}, {"id": 11, "media": []}, {"id": 10, "media": [{"kind": "photo", "path": "../assets/media/10.jpg", "thumb": "../assets/media/thumbs/10_480.webp", "size": 76054, "mime": "image/jpeg", "name": null}]}, {"id": 9, "media": [{"kind": "photo", "path": "../assets/media/9.jpg", "thumb": "../assets/media/thumbs/9_480.webp", "size": 39543, "mime": "image/jpeg", "name": null}]}, {"id": 6, "media": [{"kind": "photo", "path": "../assets/media/6.jpg", "thumb": "../assets/media/thumbs/6_480.webp", "size": 108737, "mime": "image/jpeg", "name": null}]}, {"id": 5, "media": []}, {"id": 4, "media": [{"kind": "photo", "path": "../assets/media/4.jpg", "thumb": "../assets/media/thumbs/4_480.webp", "size": 96825, "mime": "image/jpeg", "name": null}]}, {"id": 3, "media": [{"kind": "photo", "path": "../assets/media/3.jpg", "thumb": "../assets/media/thumbs/3_480.webp", "size": 48408, "mime": "image/jpeg", "name": null}]}, {"id": 2, "media": []}];
    window.__STATIC_META = {"title": "Speech Info", "username": "speechinfo", "channel": "speechinfo", "last_sync_utc": "2026-02-03T23:59:03Z", "posts_count": 56, "last_seen_message_id": 117, "stats": {"new": 57, "updated": 0, "media_downloaded": 57}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
